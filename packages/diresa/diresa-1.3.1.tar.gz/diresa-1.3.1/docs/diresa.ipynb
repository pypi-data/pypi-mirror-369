{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43a1495a-017d-412f-bfb8-76ad9a9e5875",
   "metadata": {},
   "source": [
    "# Lorenz '63 DIRESA tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f159961e-b306-4428-be35-c0c124c64b9e",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/gdepaepe/diresa/blob/main/diresa.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1aba31-5e47-4c4c-b2bb-7610e9d6c7b4",
   "metadata": {},
   "source": [
    "### 1. Install packages\n",
    "The `diresa` package depends on the `tensorflow` package. This tutorial also uses `numpy` and `matplotlib`. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Install needed packages\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install tensorflow\n",
    "!pip install diresa"
   ],
   "id": "96dfb427d3ba906c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Load the dataset\n",
    "In this tutorial, we are going to compress the 3D lorenz '63 butterfly into a 2D latent space. The `lorenz.csv` contains a list of butterfly points, with three colums for the X, Y and Z coordinate. The DIRESA model has 2 inputs: the original dataset and a shuffled version of this dataset for the twin encoder."
   ],
   "id": "ad1a6e98391d1eea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!wget https://gitlab.com/etrovub/ai4wcm/public/diresa/-/raw/master/docs/lorenz.csv",
   "id": "cadfdeba2bb4c735"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "data_file = \"lorenz.csv\"\n",
    "data = np.loadtxt(data_file, delimiter=\",\")\n",
    "print(\"Shape\", data_file, \":\", data.shape)\n",
    "train = data[:30000]\n",
    "val = data[30000:]\n",
    "id_train = np.argsort((np.random.random(train.shape[0])))\n",
    "id_val = np.argsort((np.random.random(val.shape[0])))\n",
    "train_twin = train[id_train]\n",
    "val_twin = val[id_val]"
   ],
   "id": "7d7aacfc6ded83ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Build the DIRESA model\n",
    "We can build a DIRESA model with convolutional, attention and/or dense layers with the `build_diresa` function. We can also build a DIRESA model based on a custom encoder and decoder with the `diresa_model` function (see below). We build here a model with an input shape of `(3,)` for the 3D butterfly points. Our encoder model has 3 dense layers with 40, 20 and 2 units (the latter is the dimension of the latent space). The decoder is a reflection of the encoder. The DIRESA model has 3 loss functions, the reconstruction loss (usually the MSE is used here), the covariance loss and a distance loss (here the MSE distance loss is used). Also the weights for the diffenent loss functions are specified."
   ],
   "id": "ae018a9faf12a749"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from diresa.models import build_diresa\n",
    "from diresa.loss import mse_dist_loss, LatentCovLoss\n",
    "\n",
    "diresa = build_diresa(input_shape=(3,), dense_units=(40, 20, 2))\n",
    "\n",
    "diresa.compile(loss=['MSE', LatentCovLoss(), mse_dist_loss], loss_weights=[1., 3., 1.5], optimizer=\"adam\")"
   ],
   "id": "f0537ae57585aebb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In order to lower the loss weight tuning effort, we will use annealing for the covariance loss. In this case, the covariance weight starts from an initial value (here the keras backend variable `cov_weight` is initialized to 0.) and is increased until the covariance loss reaches a certain target.",
   "id": "25de45fe22429f9c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from diresa.callback import LossWeightAnnealing\n",
    "\n",
    "cov_weight = K.variable(0.)\n",
    "diresa.compile(loss=['MSE', LatentCovLoss(cov_weight), mse_dist_loss], loss_weights=[1., 1., 1.], optimizer=\"adam\")\n",
    "diresa.summary(expand_nested=True)"
   ],
   "id": "815611188fc7e0d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Train the DIRESA model\n",
    "We train the DIRESA model in a standard way. The model is fit with 2 inputs: the original dataset and the shuffled dataset. There are 3 outputs: the original dataset for the reconstruction loss; the 2 last outputs are not used, but are needed in Keras 3. The batch size should be large enough for the calculation of the covariance loss, which calculates the covariance matrix of the latent space components over the batch. In the `LossWeightAnnealing` callback, we specify the target (`target_loss`) for the mean squared covariance between the latent components. Also the step size by which the annealing weight factor is increased (`anneal_step`) and epoch from which annealing is started (`start_epoch`) is specified. If annealing is not used, the fit method is called without callback function."
   ],
   "id": "7e403c9a26d5d9b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "callback = [LossWeightAnnealing(cov_weight, target_loss=0.0001, anneal_step=0.2, start_epoch=3)]\n",
    "diresa.fit((train, train_twin), (train, train, train), \n",
    "           validation_data=((val, val_twin), (val, val, val)),\n",
    "           epochs=20, batch_size=512, shuffle=True, verbose=2, callbacks=callback)"
   ],
   "id": "aba3dfc9ba34fcd3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5. Encoder and decoder submodel\n",
    "We cut out the encoder and decoder submodels with the `encoder_decoder` function. If a dataset is given, the R2-scores of the latent components are calculated and a ranking layer, which orders the latent components based on the R2-scores, is added to the submodels."
   ],
   "id": "e61f04da08035f81"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from diresa.toolbox import encoder_decoder\n",
    "compress_model, decode_model = encoder_decoder(diresa, dataset=val)\n",
    "latent = compress_model.predict(val)\n",
    "predict = decode_model.predict(latent)"
   ],
   "id": "837b6531ac2dd10"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6. Show latent space\n",
    "We plot the 2D latent space."
   ],
   "id": "c662123b934540b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.title(\"Latent space\")\n",
    "plt.scatter(latent[:, 0], latent[:, 1], marker='.', s=0.1, color='C2')\n",
    "plt.show()"
   ],
   "id": "111c6a62c22ba780"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 7. Original versus decoded datset\n",
    "We compair the origonal dataset with the decoded one."
   ],
   "id": "4c3dac3c248dc9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(val[:, 0], val[:, 1], val[:, 2], marker='.', s=0.1)\n",
    "ax.scatter(predict[:, 0], predict[:, 1], predict[:, 2], marker='.', s=0.1, color='C1')\n",
    "plt.show()"
   ],
   "id": "27c6a9af783fc77"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 8. A convolutional and attention example\n",
    "If your dataset consists of a number of variables (e.g. temperature and pressure, so 2 variables) over a 2 dimensional grid, convolutional layers can be used in the encoder/decoder. Here is an example for a grid (y, x) = (32, 64). The dataset would then have a size of (nbr_of_samples, 32, 64, 2). We will use a stack of 4 convolutional/maxpooling blocks in the encoder (the decoder mirrors the encoder). The first block uses 3 Conv2D layers, the second bock 2 and the third block 1, followed by a MaxPooling2D layer (`stack=(3, 2, 1)`). The number of filters in the first block is 32, in the second 16 and in the third 8 (`stack_filters=(32, 16, 8)`). The number of filters in Latent space, before flattening, is 1 (`latent_filters=1`). This will result in a latent size (before flattening) of (8, 16, 1)."
   ],
   "id": "8214bca909d495b1"
  },
  {
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 10,
   "source": [
    "diresa = build_diresa(input_shape=(32, 64, 2), stack=(3, 2, 1), stack_filters=(32, 16, 8), latent_filters=1)\n",
    "diresa.summary(expand_nested=True)"
   ],
   "id": "9acf7f76-bc9d-4219-97c8-869fe00ae655"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can add an attention after the last convolutional layer in a block, to catch long distance relations. Here we add an attention layer in the second and third block. After the convolutional/attention blocks, 2 dense layers are added, bringing the dimension of the latent space to 10.",
   "id": "857e3e020967f71a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "diresa = build_diresa(input_shape=(32, 64, 2), stack=(3, 2, 1), stack_filters=(32, 16, 8), attention=(False, True, True), dense_units=(30, 10))\n",
    "diresa.summary(expand_nested=True)"
   ],
   "id": "749a7854747c28d7"
  },
  {
   "cell_type": "markdown",
   "id": "c3d06b52-991b-478f-a599-ec0a947bd6c8",
   "metadata": {},
   "source": [
    "### 9. Build DIRESA with custom encoder and decoder\n",
    "We can also build DIRESA models with custom encoder and decoder (reconstruction) models. We define those two here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fb3e14d-75d7-4aab-9ef3-794c9387e7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, Input\n",
    "from tensorflow.keras.models import Model\n",
    "def encoder_model(input_shape=(3,), output_shape=2, units=40):\n",
    "    x = Input(shape=input_shape)\n",
    "    y = layers.Dense(units=units, activation=\"relu\")(x)\n",
    "    y = layers.Dense(units=units // 2, activation=\"relu\")(y)\n",
    "    y = layers.Dense(output_shape, activation=\"linear\")(y)\n",
    "    model = Model(x, y, name=\"Encoder\")\n",
    "    return model\n",
    "def decoder_model(input_shape=(2,), output_shape=3, units=40):\n",
    "    x = Input(shape=input_shape)\n",
    "    y = layers.Dense(units=units // 2, activation=\"relu\")(x)\n",
    "    y = layers.Dense(units=units, activation=\"relu\")(y)\n",
    "    y = layers.Dense(output_shape, activation=\"linear\")(y)\n",
    "    model = Model(x, y, name=\"Recon\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d0bbda-de40-4b84-9d8d-1585e000f163",
   "metadata": {},
   "source": [
    "Based on the custom encoder and decoder model, we now build the DIRESA model with the `diresa_model` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0875727-7e9f-402e-b14e-a25bed19601c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from diresa.models import diresa_model\n",
    "from diresa.loss import mse_dist_loss, LatentCovLoss\n",
    "\n",
    "diresa = diresa_model(x=Input(shape=(3,)), x_twin=Input(shape=(3,)), encoder=encoder_model(), decoder=decoder_model())\n",
    "\n",
    "diresa.compile(loss=['MSE', LatentCovLoss(), mse_dist_loss], loss_weights=[1., 3., 1.])\n",
    "diresa.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5995f8fc-d658-489b-8f0c-179910981fb6",
   "metadata": {},
   "source": [
    "### 9. Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e369e09-4daf-4ccf-bbb5-e4f5c5975938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module diresa.models in diresa:\n",
      "\n",
      "NAME\n",
      "    diresa.models\n",
      "\n",
      "DESCRIPTION\n",
      "    Creates DIRESA and (V)AE models out of an encoder and decoder model\n",
      "    Creates DIRESA and AE models from hyperparameters\n",
      "    :Author:  Geert De Paepe\n",
      "    :Email:   geert.de.paepe@vub.be\n",
      "    :License: Apache 2.0\n",
      "    \n",
      "    1. Creating (V)AE and Diresa models out of an encoder and decoder model:\n",
      "     - autoencoder_model(x, encoder, decoder)\n",
      "     - diresa_model(x, x_twin, encoder, decoder)\n",
      "    \n",
      "    2. Creating AE and Diresa models from hyperparameters\n",
      "       - build_ae(input_shape, stack, stack_filters, latent_filters, kernel_size=(3, 3),\n",
      "                  conv_transpose=False, up_first=False, residual=False, batchnorm=False,\n",
      "                  dense_units=(), activation='relu', encoder_activation='linear', decoder_activation='linear')\n",
      "       - build_diresa(input_shape, stack, stack_filters, latent_filters, kernel_size=(3, 3),\n",
      "                      conv_transpose=False, up_first=False, residual=False, batchnorm=False,\n",
      "                      dense_units=(), activation='relu', encoder_activation='linear', decoder_activation='linear')\n",
      "    \n",
      "       Encoder:\n",
      "        - 0 or more [blocks] with C (Conv2D) or residual units and a P (MaxPooling layer)\n",
      "        - 0 or 1 [block] of D (Dense layers)\n",
      "       Decoder:\n",
      "        - 0 or 1 [block] with D (Dense layers)\n",
      "        - 0 or more [blocks] with C (Conv2D) or residual units and an U (UpSampling layer)\n",
      "    \n",
      "       stack     dense_units  Encoder                Decoder (up_first=True)    Decoder (up_first=False)\n",
      "       [1]       ()           [C-P]-Cout             [U-C]-Cout                 [C-U]-Cout\n",
      "       [3]       ()           [C-C-C-P]-Cout         [U-C-C-C]-Cout             [C-U-C-C]-Cout\n",
      "       [1,1]     ()           [C-P]-[C-P]-Cout       [U-C]-[U-C]-Cout           [C-U]-[C-U]-Cout\n",
      "       ()        [20,10]      [D-Dout]               [D-Dout]                   [D-Dout]\n",
      "       [2]       [20,10]      [C-C-P]-[D-Dout]       [D-D]-[U-C]-Cout           [D-D]-[C-U]-Cout\n",
      "       [1,1]     [20,10]      [C-P]-[C-P]-[D-Dout]   [D-D]-[U]-[U-C]-Cout       [D-D]-[U]-[C-U]-Cout\n",
      "    \n",
      "       If conv_transpose=True, C is a ConvTranspose layer, only possible for up_first=True\n",
      "       If residual=True, C is a residual unit with a skip connection, only possible for up_first=True\n",
      "       Input shape should be 3 if Conv2D blocks, first 2 dimensions of input_shape should be a multiple of 2^len(stack)\n",
      "       Input shape should be 1 if only a Dense block\n",
      "\n",
      "FUNCTIONS\n",
      "    autoencoder_model(x, encoder, decoder)\n",
      "        Creates autoencoder model out of an encoder and a decoder model\n",
      "        :param x: keras input tensor (keras.Input())\n",
      "        :param encoder: encoder functional Keras model\n",
      "        :param decoder: decoder functional Keras model\n",
      "        :return: autoencoder model\n",
      "    \n",
      "    build_ae(input_shape=(), stack=(), stack_filters=(), latent_filters=1, kernel_size=(3, 3), conv_transpose=False, up_first=False, residual=False, batchnorm=False, dense_units=(), activation='relu', encoder_activation='linear', decoder_activation='linear')\n",
      "        Creates an AE model out of hyperparameters\n",
      "        :param input_shape: 3-dimensional with Conv2D layers, first 2 dimensions should be a multiple of 2^len(stack)\n",
      "                            1-dimensional if only Dense layers\n",
      "        :param stack: elements are nbr of Conv2D or residual units in a block\n",
      "        :param stack_filters: elements are nbr of filters in a block\n",
      "        :param latent_filters: nbr of filters in convolutional output (only used if no dense units)\n",
      "        :param kernel_size: kernel size for convolution\n",
      "        :param conv_transpose: if True ConvTranspose is used in decoder, only possible for up_first=True\n",
      "        :param up_first: if True UpSampling is first in decoder block, if False UpSampling is second\n",
      "        :param residual: if True, elements in blocks are residual units, if False elements are Conv2D layers\n",
      "        :param batchnorm: if True, each Conv2D is followed by a BatchNormalization layer, if False no BN is used\n",
      "        :param dense_units: elements are nbr of nodes of a Dense layer in the dense block\n",
      "        :param activation: activation function used (except for output of encoder/decoder)\n",
      "        :param encoder_activation: activation function used for output of encoder\n",
      "        :param decoder_activation: activation function used for output of decoder\n",
      "        :return: AE functional Keras model\n",
      "    \n",
      "    build_diresa(input_shape=(), stack=(), stack_filters=(), latent_filters=1, kernel_size=(3, 3), conv_transpose=False, up_first=False, residual=False, batchnorm=False, dense_units=(), activation='relu', encoder_activation='linear', decoder_activation='linear')\n",
      "        Creates a Diresa model out of hyperparameters\n",
      "        :param input_shape: 3-dimensional with Conv2D layers, first 2 dimensions should be a multiple of 2^len(stack)\n",
      "                            1-dimensional if only Dense layers\n",
      "        :param stack: elements are nbr of Conv2D or residual units in a block\n",
      "        :param stack_filters: elements are nbr of filters in a block\n",
      "        :param latent_filters: nbr of filters in convolutional output (only used if no dense units)\n",
      "        :param kernel_size: kernel size for convolution\n",
      "        :param conv_transpose: if True ConvTranspose is used in decoder, only possible for up_first=True\n",
      "        :param up_first: if True UpSampling is first in decoder block, if False UpSampling is second\n",
      "        :param residual: if True, elements in blocks are residual units, if False elements are Conv2D layers\n",
      "        :param batchnorm: if True, each Conv2D is followed by a BatchNormalization layer, if False no BN is used\n",
      "        :param dense_units: elements are nbr of nodes of a Dense layer in the dense block\n",
      "        :param activation: activation function used (except for output of encoder/decoder)\n",
      "        :param encoder_activation: activation function used for output of encoder\n",
      "        :param decoder_activation: activation function used for output of decoder\n",
      "        :return: Diresa functional Keras model\n",
      "    \n",
      "    diresa_model(x, x_twin, encoder, decoder)\n",
      "        Creates a Diresa model out of an encoder and a decoder model\n",
      "        :param x: keras input tensor (keras.Input())\n",
      "        :param x_twin: keras input tensor for shuffled input\n",
      "        :param encoder: encoder functional Keras model\n",
      "        :param decoder: decoder functional Keras model\n",
      "        :return: Diresa model\n",
      "    \n",
      "FILE\n",
      "    c:\\users\\depaepeg\\python\\venv\\lib\\site-packages\\diresa\\models.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from diresa import *\n",
    "help(models)\n",
    "#help(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
