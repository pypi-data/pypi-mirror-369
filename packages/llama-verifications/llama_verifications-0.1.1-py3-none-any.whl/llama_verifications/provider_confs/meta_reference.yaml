# LLAMA_STACK_PORT=5002 llama stack run meta-reference-gpu --env INFERENCE_MODEL=meta-llama/Llama-4-Scout-17B-16E-Instruct --env INFERENCE_CHECKPOINT_DIR=<path_to_ckpt>
base_url: http://localhost:5002/v1/openai/v1
api_key_var: DUMMY_API_KEY
models:
  - id: meta-llama/Llama-4-Scout-17B-16E-Instruct
    canonical_id: Llama-4-Scout-Instruct
  - id: meta-llama/Llama-4-Maverick-17B-128E-Instruct
    canonical_id: Llama-4-Maverick-Instruct
test_exclusions: {}
self_hosted: true
