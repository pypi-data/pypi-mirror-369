# Bub Configuration
# Copy this file to .env and update with your settings

# AI Provider Configuration (Required)
# Supported providers: openai, anthropic, ollama, groq, mistral, cohere, etc.
BUB_PROVIDER=openai

# Model name from the selected provider (Required)
# Examples:
# - OpenAI: gpt-4, gpt-3.5-turbo, gpt-4o-mini
# - Anthropic: claude-3-5-sonnet-20241022, claude-3-haiku-20240307
# - Ollama: llama2, llama3, codellama
# - Groq: llama3-8b-8192, mixtral-8x7b-32768
BUB_MODEL_NAME=gpt-3.5-turbo

# API key for the provider (Required for cloud providers, not needed for local models like Ollama)
BUB_API_KEY=your_api_key_here

# Optional: Custom API base URL (for self-hosted or custom endpoints)
# BUB_API_BASE=https://api.custom-provider.com/v1

# Optional: Maximum tokens for AI responses (default varies by model)
# BUB_MAX_TOKENS=4096

# Optional: Custom workspace path (default: current directory)
# BUB_WORKSPACE_PATH=/path/to/your/workspace

# Optional: Custom system prompt for the AI agent
# BUB_SYSTEM_PROMPT="You are a helpful coding assistant..."

# Example configurations for different providers:

# OpenAI GPT-4
# BUB_PROVIDER=openai
# BUB_MODEL_NAME=gpt-4
# BUB_API_KEY=sk-...

# Anthropic Claude
# BUB_PROVIDER=anthropic
# BUB_MODEL_NAME=claude-3-5-sonnet-20241022
# BUB_API_KEY=sk-ant-...

# Local Ollama (no API key needed)
# BUB_PROVIDER=ollama
# BUB_MODEL_NAME=llama3
# # BUB_API_KEY not needed

# Groq (fast inference)
# BUB_PROVIDER=groq
# BUB_MODEL_NAME=llama3-8b-8192
# BUB_API_KEY=gsk_...

# Mistral AI
# BUB_PROVIDER=mistral
# BUB_MODEL_NAME=mistral-large-latest
# BUB_API_KEY=...
