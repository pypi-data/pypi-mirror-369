Metadata-Version: 2.4
Name: nvidia-pytriton
Version: 0.7.0
Summary: PyTriton - Flask/FastAPI-like interface to simplify Triton's deployment in Python environments.
License: Apache 2.0
Project-URL: Documentation, https://triton-inference-server.github.io/pytriton
Project-URL: Source, https://github.com/triton-inference-server/pytriton
Project-URL: Tracker, https://github.com/triton-inference-server/pytriton/issues
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Science/Research
Classifier: Intended Audience :: Developers
Classifier: Topic :: Software Development
Classifier: Topic :: Scientific/Engineering
Classifier: Programming Language :: Python
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Operating System :: Unix
Requires-Python: <4,>=3.8
Description-Content-Type: text/x-rst
License-File: LICENSE
Requires-Dist: numpy<2.0.0,>=1.21
Requires-Dist: protobuf>=3.7
Requires-Dist: pyzmq>=23.0
Requires-Dist: sh>=1.14
Requires-Dist: tritonclient[grpc,http]~=2.50
Requires-Dist: grpcio>=1.64.3
Requires-Dist: typing_inspect>=0.6.0
Requires-Dist: wrapt>=1.11
Requires-Dist: typer>=0.9.0
Requires-Dist: importlib_metadata>=7.0.1
Provides-Extra: test
Requires-Dist: pytest~=8.1.1; extra == "test"
Requires-Dist: pytest-codeblocks~=0.16; extra == "test"
Requires-Dist: pytest-mock~=3.14; extra == "test"
Requires-Dist: pytest-timeout~=2.2; extra == "test"
Requires-Dist: pytest-asyncio~=0.23.5; extra == "test"
Requires-Dist: pytype!=2021.11.18,!=2022.2.17; extra == "test"
Requires-Dist: pre-commit>=3.5; extra == "test"
Requires-Dist: tox>=4.13; extra == "test"
Requires-Dist: tqdm>=4.64.1; extra == "test"
Requires-Dist: psutil~=5.9; extra == "test"
Requires-Dist: py-spy~=0.3; extra == "test"
Requires-Dist: opentelemetry-api; extra == "test"
Requires-Dist: opentelemetry-sdk; extra == "test"
Requires-Dist: opentelemetry-instrumentation-requests; extra == "test"
Requires-Dist: opentelemetry-exporter-otlp; extra == "test"
Provides-Extra: doc
Requires-Dist: GitPython>=3.1; extra == "doc"
Requires-Dist: mike>=2.0.0; extra == "doc"
Requires-Dist: mkdocs-htmlproofer-plugin>=1.1; extra == "doc"
Requires-Dist: mkdocs-material>=9.5; extra == "doc"
Requires-Dist: mkdocstrings[python]<0.28.3; extra == "doc"
Requires-Dist: mkdocstrings-python>=1.8; extra == "doc"
Requires-Dist: mkdocs-mermaid2-plugin>=1.2.1; extra == "doc"
Provides-Extra: dev
Requires-Dist: nvidia-pytriton[test]; extra == "dev"
Requires-Dist: nvidia-pytriton[doc]; extra == "dev"
Requires-Dist: build<1.0.0,>=0.8; extra == "dev"
Requires-Dist: ipython>=8.12; extra == "dev"
Requires-Dist: packaging~=24.0; extra == "dev"
Requires-Dist: pudb>=2024.1; extra == "dev"
Requires-Dist: pip>=24.0; extra == "dev"
Requires-Dist: ruff>=0.3.0; extra == "dev"
Requires-Dist: twine>=5.0; extra == "dev"
Dynamic: license-file

..
    Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

PyTriton
==========

PyTriton - a Flask/FastAPI-like framework designed to streamline
the use of NVIDIA's `Triton Inference Server <https://github.com/triton-inference-server>`_.

For comprehensive guidance on how to deploy your models, optimize performance,
and explore the API, delve into the extensive resources found in our
`documentation <https://triton-inference-server.github.io/pytriton/>`_.

Features at a Glance
--------------------

The distinct capabilities of PyTriton are summarized in the feature matrix:

.. list-table::
   :header-rows: 1
   :widths: 25 75

   * - Feature
     - Description
   * - Native Python support
     - You can create any `Python function <https://triton-inference-server.github.io/pytriton/latest/inference_callables/>`_ and expose it as an HTTP/gRPC API.
   * - Framework-agnostic
     - You can run any Python code with any framework of your choice, such as: PyTorch, TensorFlow, or JAX.
   * - Performance optimization
     - You can benefit from `dynamic batching <https://triton-inference-server.github.io/pytriton/latest/inference_callables/decorators/#batch>`_, response cache, model pipelining, `clusters <https://triton-inference-server.github.io/pytriton/latest/guides/deploy/>`_, performance `tracing <https://triton-inference-server.github.io/pytriton/latest/guides/distributed_tracing/>`_, and GPU/CPU inference.
   * - Decorators
     - You can use batching `decorators <https://triton-inference-server.github.io/pytriton/latest/inference_callables/decorators/>`_ to handle batching and other pre-processing tasks for your inference function.
   * - Easy installation and setup
     - You can use a simple and familiar interface based on Flask/FastAPI for easy `installation <https://triton-inference-server.github.io/pytriton/latest/installation/>`_ and `setup <https://triton-inference-server.github.io/pytriton/latest/binding_models/>`_.
   * - Model clients
     - You can access high-level `model clients <https://triton-inference-server.github.io/pytriton/latest/clients/>`_ for HTTP/gRPC requests with configurable options and both synchronous and `asynchronous <https://triton-inference-server.github.io/pytriton/latest/clients/#asynciomodelclient>`_ API.
   * - Streaming (alpha)
     - You can stream partial responses from a model by serving it in a `decoupled mode <https://triton-inference-server.github.io/pytriton/latest/clients/#decoupledmodelclient>`_.

Learn more about PyTriton's `architecture <https://triton-inference-server.github.io/pytriton/latest/#architecture>`_.

Prerequisites
-------------

Before proceeding with the installation of PyTriton, ensure your system meets the following criteria:

- **Operating System**: Compatible with glibc version ``2.35`` or higher.
  - Primarily tested on Ubuntu 22.04.
  - Other supported OS include Debian 11+, Rocky Linux 9+, and Red Hat UBI 9+.
  - Use ``ldd --version`` to verify your glibc version.
- **Python**: Version ``3.8`` or newer.
- **pip**: Version ``20.3`` or newer.
- **libpython**: Ensure ``libpython3.*.so`` is installed, corresponding to your Python version.

Install
-------

The PyTriton can be installed from pypi.org by running the following command::

    pip install nvidia-pytriton

**Important**: The Triton Inference Server binary is installed as part of the PyTriton package.

Discover more about PyTriton's `installation procedures <https://triton-inference-server.github.io/pytriton/latest/installation/>`_, including Docker usage, prerequisites, and insights into `building binaries from source <https://triton-inference-server.github.io/pytriton/latest/guides/building/>`_ to match your specific Triton server versions.


Quick Start
-----------

The quick start presents how to run Python model in Triton Inference Server without need to change the current working
environment. In the example we are using a simple `Linear` model.

The `infer_fn` is a function that takes an `data` tensor and returns a list with single output tensor. The `@batch` from `batching decorators <https://triton-inference-server.github.io/pytriton/latest/inference_callables/decorators/>`_ is used to handle batching for the model.

.. code-block:: python

    import numpy as np
    from pytriton.decorators import batch

    @batch
    def infer_fn(data):
        result = data * np.array([[-1]], dtype=np.float32)  # Process inputs and produce result
        return [result]


In the next step, you can create the binding between the inference callable and Triton Inference Server using the `bind` method from pyTriton. This method takes the model name, the inference callable, the inputs and outputs tensors, and an optional model configuration object.

.. code-block:: python

    from pytriton.model_config import Tensor
    from pytriton.triton import Triton
    triton = Triton()
    triton.bind(
        model_name="Linear",
        infer_func=infer_fn,
        inputs=[Tensor(name="data", dtype=np.float32, shape=(-1,)),],
        outputs=[Tensor(name="result", dtype=np.float32, shape=(-1,)),],
    )
    triton.run()

Finally, you can send an inference query to the model using the `ModelClient` class. The `infer_sample` method takes the input data as a numpy array and returns the output data as a numpy array. You can learn more about the `ModelClient` class in the `clients <https://triton-inference-server.github.io/pytriton/latest/clients/>`_ section.

.. code-block:: python

    from pytriton.client import ModelClient

    client = ModelClient("localhost", "Linear")
    data = np.array([1, 2, ], dtype=np.float32)
    print(client.infer_sample(data=data))

After the inference is done, you can stop the Triton Inference Server and close the client:

.. code-block:: python

    client.close()
    triton.stop()

The output of the inference should be:

.. code-block:: python

    {'result': array([-1., -2.], dtype=float32)}


For the full example, including defining the model and binding it to the Triton server, check out our detailed `Quick Start <https://triton-inference-server.github.io/pytriton/latest/quick_start/>`_ instructions. Get your model up and running, explore how to serve it, and learn how to `invoke it from client applications <https://triton-inference-server.github.io/pytriton/latest/clients/>`_.


The full example code can be found in `examples/linear_random_pytorch <https://github.com/triton-inference-server/pytriton/tree/main/examples/linear_random_pytorch>`_.

Examples
--------

The `examples <https://triton-inference-server.github.io/pytriton/latest/examples/>`_ page showcases various use cases of serving models using PyTriton. This includes simple examples of running models in PyTorch, TensorFlow2, JAX, and plain Python. In addition, more advanced scenarios are covered, such as online learning, multi-node models, and deployment on Kubernetes using PyTriton. Each example is accompanied by instructions on how to build and run it. Discover more about utilizing PyTriton by exploring our examples.


Links
-------

* `Source <https://github.com/triton-inference-server/pytriton>`_
* `Issues  <https://github.com/triton-inference-server/pytriton/issues>`_
* `Changelog <https://github.com/triton-inference-server/pytriton/blob/main/CHANGELOG.md>`_
* `Known Issues <https://github.com/triton-inference-server/pytriton/blob/main/docs/known_issues.md>`_
* `Contributing <https://github.com/triton-inference-server/pytriton/blob/main/CONTRIBUTING.md>`_
