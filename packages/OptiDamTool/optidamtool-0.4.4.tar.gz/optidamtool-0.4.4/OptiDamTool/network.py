import GeoAnalyze
import geopandas
import shapely
import pandas
import typing
import os
from .watemsedem import WatemSedem


class Network:

    '''
    Provides functionality to establish network-based
    connectivity and operations between dams.
    '''

    def connectivity_adjacent_downstream_dam(
        self,
        stream_file: str,
        dam_list: list[int],
        sort_dam: bool = False
    ) -> dict[int, int]:

        '''
        Generates adjacent downstream connectivity between dams based on the input stream network.
        Each dam is represented by a unique stream segment identifier.

        Parameters
        ----------
        stream_file: str
            Path to the input stream shapefile,
            generated by :meth:`OptiDamTool.WatemSedem.dem_to_stream`.

        dam_list : list
            List of stream segment identifiers representing dam locations.

        sort_dam : bool, optional
            If True, the dam identifiers in the output dictionary keys will be sorted in ascending order.
            Default is False.

        Returns
        -------
        dict
            A dictionary with each key is a dam's stream identifier, and the corresponding value
            is the stream identifier of the directly connected downstream dam.
            A value of -1 indicates that the dam has no downstream connectivity.
        '''

        # check distinct stream identifiers for dams
        if len(set(dam_list)) < len(dam_list):
            raise Exception('Duplicate stream identifiers found in the input dam list.')

        # connectivity from upstream to downstream
        connect_dict = GeoAnalyze.Stream()._connectivity_upstream_to_downstream(
            stream_file=stream_file,
            stream_col='ws_id'
        )

        # adjacent downstream connectvity
        adc_dict = {}
        for i in dam_list:
            if i not in connect_dict:
                raise Exception(f'Invalid stream identifier {i} for a dam.')
            # all dam connectivity towards outlet
            stream_connect = connect_dict[i]
            dam_connect = list(
                filter(lambda x: x in stream_connect, dam_list)
            )
            # if no downstream dam is found
            if len(dam_connect) == 0:
                adc_dict[i] = -1
            # extract the adjacent downstream dam
            else:
                dam_indices = [
                    stream_connect.index(j) for j in dam_connect
                ]
                adc_dict[i] = stream_connect[min(dam_indices)]

        # filtered connectivity for stream outlet identifiers where key and value are same
        adc_dict = {
            k: v if k != v else -1 for k, v in adc_dict.items()
        }

        # output dictionary
        output = dict(sorted(adc_dict.items())) if sort_dam else adc_dict

        return output

    def connectivity_adjacent_upstream_dam(
        self,
        stream_file: str,
        dam_list: list[int],
        sort_dam: bool = False
    ) -> dict[int, list[int]]:

        '''
        Computes adjacent upstream connectivity between dams based on the input stream network.
        Each dam is represented by a unique stream segment identifier.

        Parameters
        ----------
        stream_file: str
            Path to the input stream shapefile,
            generated by :meth:`OptiDamTool.WatemSedem.dem_to_stream`.

        dam_list : list
            List of stream segment identifiers representing dam locations.

        sort_dam : bool, optional
            If True, both the dam identifier keys and their corresponding value lists
            in the output dictionary will be sorted in ascending order. Default is False.

        Returns
        -------
        dict
            A dictionary where each key is a dam's stream identifier, and the corresponding value
            is a list of adjacent upstream dam identifiers. An empty list indicates no upstream connectivity.
        '''

        # adjacent downstream connectivity dictionary
        adc_dict = self.connectivity_adjacent_downstream_dam(
            stream_file=stream_file,
            dam_list=dam_list
        )

        # DataFrame creation for adjacent downstream connectivity
        df = pandas.DataFrame(
            {
                'dam_id': adc_dict.keys(),
                'adc_id': adc_dict.values()
            }
        )

        # non-empty adjacent upstream connectivity
        auc_dict = {
            j: k['dam_id'].tolist() for j, k in df.groupby(by='adc_id')
        }

        # adjacent upstream connectivity of all dams
        auc_dict = {
            i: auc_dict[i] if i in auc_dict else list() for i in dam_list
        }

        # output dictionary
        output = {k: sorted(v) for k, v in sorted(auc_dict.items())} if sort_dam else auc_dict

        return output

    def controlled_drainage_area(
        self,
        stream_file: str,
        dam_list: list[int],
        sort_dam: bool = False
    ) -> dict[int, float]:

        '''
        Computes the controlled upstream drainage area for selected dams
        based on their locations within the input stream network.
        Each dam is represented by a unique stream segment identifier.

        Parameters
        ----------
        stream_file : str
            Path to the input stream shapefile,
            generated by :meth:`OptiDamTool.WatemSedem.dem_to_stream`.

        info_file : str
            Path to the stream information text file ``stream_information.txt``,
            generated by :meth:`OptiDamTool.WatemSedem.dem_to_stream`.

        dam_list : list
            List of stream segment identifiers representing dam locations.

        sort_dam : bool, optional
            If True, the dam identifiers in the output dictionary keys will be sorted in ascending order.
            Default is False.

        Returns
        -------
        dict
            A dictionary where each key is a dam's stream segment identifier,
            and the corresponding value is the effective upstream drainage area in square meters.
        '''

        # adjacent downstream connectivity dictionary
        auc_dict = self.connectivity_adjacent_upstream_dam(
            stream_file=stream_file,
            dam_list=dam_list
        )

        # cumulative subbasin area dictionary from stream information DataFrame
        stream_gdf = geopandas.read_file(
            filename=stream_file
        )
        cumarea_dict = dict(
            zip(stream_gdf['ws_id'], stream_gdf['csa_m2'])
        )

        # effective drainage area dictionary
        eda_dict = {}
        for i in dam_list:
            eda_dict[i] = cumarea_dict[i] - sum([cumarea_dict[j] for j in auc_dict[i]])

        # output dictionary
        output = dict(sorted(eda_dict.items())) if sort_dam else eda_dict

        return output

    def sediment_inflow_from_drainage_area(
        self,
        stream_file: str,
        dam_list: list[int],
        sort_dam: bool = False
    ) -> dict[int, float]:

        '''
        Computes the sediment inflows in kilograms for selected dams based on their
        local drainage areas within the input stream network. Each dam is identified by a
        unique stream segment identifier.

        Parameters
        ----------
        stream_file : str
            Path to the input stream GeoJSON file, generated by
            :meth:`OptiDamTool.Analysis.sediment_delivery_to_stream_geojson`.

        dam_list : list
            List of stream segment identifiers representing dam locations.

        sort_dam : bool, optional
            If True, the dam identifiers in the output dictionary keys will be sorted in ascending order.
            Default is False.

        Returns
        -------
        dict
            A dictionary where each key is a dam's stream segment identifier,
            and the corresponding value is the effective sediment inflow in kilograms.
        '''

        # adjacent downstream connectivity dictionary
        auc_dict = self.connectivity_adjacent_upstream_dam(
            stream_file=stream_file,
            dam_list=dam_list
        )

        # cumulative sediment input from stream information GeoDataFrame
        stream_gdf = geopandas.read_file(
            filename=stream_file
        )
        cumsed_dict = dict(
            zip(stream_gdf['ws_id'], stream_gdf['cumsed_kg'])
        )

        # effective sediment inflow dictionary
        esi_dict = {}
        for i in dam_list:
            esi_dict[i] = cumsed_dict[i] - sum([cumsed_dict[j] for j in auc_dict[i]])

        # output dictionary
        output = dict(sorted(esi_dict.items())) if sort_dam else esi_dict

        return output

    def upstream_metrics_summary(
        self,
        stream_file: str,
        dam_list: list[int],
        sort_dam: bool = False
    ) -> dict[str, dict[int, typing.Any]]:

        '''
        Computes a summary of upstream metrics for selected dams based on their
        locations within the input stream network. Each dam is identified by a unique stream
        segment identifier.

        The function returns a dictionary containing three sub-dictionaries, where each key
        corresponds to a dam's stream identifier:

        - **adjacent_upstream_dams**: A dictionary where each value is a list of directly
          connected upstream dam identifiers. An empty list indicates no upstream connectivity.

        - **controlled_drainage_m2**: A dictionary mapping each dam to its effective upstream
          drainage area in square meters, based on upstream dam connections.

        - **sediment_inflow_kg**: A dictionary mapping each dam to its effective sediment
          inflow in kilograms, based on upstream dam connections.

        Parameters
        ----------
        stream_file : str
            Path to the input stream GeoJSON file, generated by
            :meth:`OptiDamTool.Analysis.sediment_delivery_to_stream_geojson`.

        dam_list : list
            List of stream segment identifiers representing dam locations.

        sort_dam : bool, optional
            If True, the dam identifiers in three sub-dictionaries keys will be sorted in ascending order.
            Default is False.

        Returns
        -------
        dict
            A dictionary with three keys: ``adjacent_upstream_connections``,
            ``controlled_drainage_m2``, and ``sediment_inflow_kg``.
            Each key maps to a sub-dictionary where the keys are dam segment identifiers
            and the values are the computed metrics.
        '''

        # adjacent downstream connectivity dictionary
        auc_dict = self.connectivity_adjacent_upstream_dam(
            stream_file=stream_file,
            dam_list=dam_list
        )

        # cumulative inputs dictionary
        stream_gdf = geopandas.read_file(stream_file)
        cumarea_dict = dict(zip(stream_gdf['ws_id'], stream_gdf['csa_m2']))
        cumsed_dict = dict(zip(stream_gdf['ws_id'], stream_gdf['cumsed_kg']))

        # effective metric dictionaries
        eda_dict = {}
        esi_dict = {}
        for i in dam_list:
            # effective drainage area for upstream connections
            eda_dict[i] = cumarea_dict[i] - sum([cumarea_dict[j] for j in auc_dict[i]])
            # effective sediment inflow for upstream connections
            esi_dict[i] = cumsed_dict[i] - sum([cumsed_dict[j] for j in auc_dict[i]])

        # output metric dictionary
        output = {
            'adjacent_upstream_dams': {k: sorted(v) for k, v in sorted(auc_dict.items())} if sort_dam else auc_dict,
            'controlled_drainage_m2': dict(sorted(eda_dict.items())) if sort_dam else eda_dict,
            'sediment_inflow_kg': dict(sorted(esi_dict.items())) if sort_dam else esi_dict
        }

        return output

    def trap_efficiency_brown(
        self,
        storage_dict: dict[int, float],
        area_dict: dict[int, float],
        brown_d: float = 0.1
    ) -> dict[int, float]:

        '''
        Computes the Trap Efficiency (TE) without percentage conversion of a dam using the Brown (1943) formula.
        Further details can be found in `Verstraeten and Poesen (2000) <https://doi.org/10.1177/030913330002400204>`_.

        Parameters
        ----------
        storage_dict : dict
            A dictionary where each key is a dam identifier, and the corresponding value
            is the dam's storage capacity in cubic meters.

        area_dict : dict
            A dictionary where each key is a dam identifier, and the corresponding value
            is the dam's catchment area in square meters. The function internally
            converts the area from square meters to square kilometers.

        brown_d : float, optional
            Empirical parameter used in the equation. It ranges from 0.046 to 1,
            with a mean value of 0.1, which is also the default.

        Returns
        -------
        dict
            A dictionary where each key is a dam identifier, and the corresponding value is the computed TE.
        '''

        # check if both dictionaries have same keys
        if area_dict.keys() != storage_dict.keys():
            raise Exception('Mismatch of keys between two dictionaries.')

        # sediment trapping efficinecy
        ste_dict = {}
        for dam_id in storage_dict:
            area_km2 = area_dict[dam_id] / 10**6
            ste_dict[dam_id] = 1 - (1 / (1 + 0.0021 * brown_d * storage_dict[dam_id] / area_km2))

        return ste_dict

    def _df_from_dict(
        self,
        input_dict: dict[str, dict[int, typing.Any]]
    ) -> pandas.DataFrame:

        '''
        A private method that converts a nested dictionary into a DataFrame.
        The outer dictionary's keys become the column names of the resulting DataFrame.
        Each value in the outer dictionary is a subdictionary with the same set of keys,
        which become the row indices. Each cell in the DataFrame contains
        the corresponding value from the subdictionary.
        '''

        series_dict = {
            key: pandas.Series(val) for key, val in input_dict.items()
        }

        df = pandas.DataFrame(
            data=series_dict
        )

        return df

    def _base_storage_df(
        self,
        storage_dict: dict[int, float]
    ) -> pandas.DataFrame:

        '''
        A private method to convert the input storage dictionary of dams into a DataFrame.
        '''

        # DataFrame from dictionary
        df = pandas.DataFrame.from_dict(
            data=storage_dict,
            orient='index',
            columns=['storage_m3']
        )
        df.reset_index(
            names=['dam_id'],
            inplace=True
        )

        # storage percentage
        df['storage_%'] = 100 * df['storage_m3'] / df['storage_m3'].sum()

        # Cummulative storage
        df['cum_m3'] = df['storage_m3'].cumsum()

        return df

    def storage_dynamics_detailed(
        self,
        stream_file: str,
        storage_dict: dict[int, float],
        sediment_density: float,
        trap_threshold: float,
        year_limit: int,
        brown_d: float = 0.1,
        release_threshold: float = 0.95,
        write_output: bool = False,
        folder_path: typing.Optional[str] = None
    ) -> dict[str, pandas.DataFrame]:

        '''
        Simulates the annual storage dynamics of a dam system influenced by sedimentation,
        and returns a dictionary containing dam lifespan, system-wide statistics,
        individual metrics, and simulation parameters.

        The simulation runs for a user-defined number of years, unless all dams become inactive earlier.
        The amount of sediment trapped by a dam is calculated as the product of the sediment inflow
        to the dam and its trap efficiency, which ranges from 0 to 1. At the end of each simulation year,
        dam storage capacities are updated using a mass balance approach based on the trapped sediment.

        A dam is considered inactive if:

        - Its storage capacity becomes non-positive (``<= 0``).
        - Its sediment trap efficiency falls below or equals the specified threshold (``<= trap_threshold``).

        Each dam in a system is uniquely identified by its stream segment identifier.
        When a dam becomes inactive, system connectivity, controlled drainage areas,
        and sediment inflows to the remaining dams are updated accordingly.

        .. warning::

            If a dam retains positive storage and the `trap_threshold` is not set appropriately,
            it may remain active indefinitely. Since trap efficiency is computed via
            :meth:`OptiDamTool.Network.trap_efficiency_brown`, which depends on dam storage,
            using a very small threshold (e.g., 0.0001) may lead to unrealistic system behavior
            due to prolonged dam lifespan despite minimal sediment trapping.
            A recommended value is 0.1 based on `Pal et al. (2018) <https://doi.org/10.1016/j.jhydrol.2018.02.051>`_
            and `Pal and Galelli (2019) <https://doi.org/10.1016/j.envsoft.2019.05.007>`_.
            This value may be adjusted to suit user needs.

        The simulation results are returned as a dictionary and can optionally be saved to the input directory as a set of JSON files.
        Each JSON file is named after the corresponding dictionary key and contains the related DataFrame.

        A detailed description of the keys and their corresponding DataFrames is provided below.

        - **dam_initial_storage**: A DataFrame with the following columns:

            - ``dam_id``: Stream segment identifier of the dam.
            - ``storage_m3``: Initial storage capacity of the dam, in cubic meters.
            - ``storage_%``: Initial storage of the dam as a percentage of the total storage across all dams.
            - ``cum_m3``: Cumulative initial storage (up to this dam), in cubic meters, to give an idea of the total capacity.

        - **dam_lifespan**: A DataFrame with the following columns:

            - ``dam_id``: Stream segment identifier of the dam.
            - ``life_year``: Number of years the dam was active.

        - **system_statistics**: A DataFrame that stores system-wide statistics with the following columns:

            - ``start_year``: Start of each simulation year, beginning from 1.
            - ``drainage_%``: Percentage of the total controlled drainage area relative
              to the total stream drainage area, at the beginning of the simulation year.
            - ``storage_%``: Percentage of total remaining storage across all dams relative
              to the initial total storage, at the beginning of the simulation year.
            - ``sedtrap_%``: Percentage of total sediment trapped by all dams relative
              to the total sediment input across all stream segments, during the simulation year.
            - ``sedrelease_%``: Percentage of sediment released by terminal dams and
              drainage areas not covered by the dam system, relative to the total sediment input across all stream segments,
              during the simulation year.
            - ``dam_removed``: List of dams removed at the beginning of the year. An empty list
              implies all dams in the system remain active.
            - ``dam_active``: List of active dams at the beginning of the year. An empty list
              in the final simulation year, if it occurs, indicates that all dams in the system
              became inactive before completing the user-defined number of simulation years.

            .. note::

                The sum of sediment trapping and release percentages for the dam system may not equal 100%.
                This is because these percentages are calculated relative to the total sediment input
                across all stream segments, using a fixed base value for consistency. In practice,
                the actual sediment inflow to the dams is greater than the total sediment input
                across all stream segments, due to additional contributions from sediment released by upstream dams.
                Furthermore, a dam's sediment trapping efficiency depends on both its controlled drainage area
                and remaining storage capacity, making sediment dynamics within the system more complex.
                However, over time, the sum gradually converges to 100% as more dams become inactive,
                resulting in reduced sediment contributions from upstream dams.

        - **dam_drainage_area**: A DataFrame with columns ``start_year`` and dam identifiers.
          The cell values represent the annual controlled drainage area percentages for each dam,
          relative to the total stream drainage area at the beginning of the simulation year.

        - **dam_remaining_storage**: A DataFrame with columns ``start_year`` and dam identifiers.
          The cell values represent the annual remaining storage percentages, relative to each dam's initial storage,
          at the beginning of the simulation year.

        - **dam_trap_efficiency**: A DataFrame with columns ``start_year`` and dam identifiers.
          The cell values represent the annual trap efficiency values at the beginning of the simulation year.

        - **dam_trapped_sediment**: A DataFrame with columns ``start_year`` and dam identifiers.
          The cell values represent the annual sediment trapping percentages, relative to the total sediment input
          across all stream segments, at the end of the simulation year.

            .. note::

                The values in ``dam_drainage_area``, ``dam_remaining_storage``, and ``dam_trap_efficiency``
                are recorded at the beginning of each simulation year. As a result, for each dam, the final recorded
                value appears in the row where ``start_year`` equals the dam's lifespan **plus one**.
                This design ensures that the final state of each dam is captured just before its removal from the system,
                allowing users to understand the conditions that led to its inactivation.

        - **simulation_parameters**: A DataFrame that stores simulation parameters at annual time steps, with the following columns:

            - ``start_year``: Start of each simulation year, beginning from 1.
            - ``active_dams``: Number of active dams at the beginning of the simulation year. This column is introduced to track dam count
              because the stream segment identifiers of dams are neither consecutive nor sorted.
            - ``dam_id``: Stream segment identifier of the dam.
            - ``ds_conn``: Identifier of the directly connected downstream dam; -1 if there is no downstream connection.
            - ``us_conn``: List of directly connected upstream dam identifiers; empty if there is no upstream connection.
            - ``drainage_m2``: Controlled drainage area of the dam (in square meters) at the beginning of the simulation year.
            - ``drainage_%``: Controlled drainage area percentage relative to the total stream drainage area,
              at the beginning of the simulation year.
            - ``storage_m3``: Remaining storage of the dam (in cubic meters) at the beginning of the simulation year.
            - ``storage_%``: Remaining storage percentage relative to the dam’s initial storage,
              at the beginning of the simulation year.
            - ``trap_efficiency``: Sediment trap efficiency at the beginning of the simulation year.
            - ``sedlocal_kg``: Sediment input (in kilograms) from the dam’s own controlled drainage area during the simulation year.
            - ``sedtotal_kg``: Total sediment inflow to the dam (in kilograms) during the simulation year,
              including contributions from the dam’s own drainage area and sediment released from upstream dams during the simulation year.
            - ``sedtrap_kg``: Amount of sediment trapped by the dam (in kilograms) during the simulation year.
            - ``sedrelease_kg``: Sediment released from the dam (in kilograms) after trapping, during the simulation year.

        Parameters
        ----------
        stream_file : str
            Path to the input stream GeoJSON file, generated by
            :meth:`OptiDamTool.Analysis.sediment_delivery_to_stream_geojson`.

        storage_dict : dict
            Dictionary mapping dam identifiers to initial storage capacities in cubic meters.

        sediment_density : float
            Density of sediment in kilograms per cubic meter.

        trap_threshold : float
            Minimum trap efficiency. Dams with efficiency less than or equal to this value are considered inactive.

        year_limit : int
            Maximum number of simulation years. Simulation stops earlier if all dams retire.

        brown_d : float, optional
            Empirical parameter used in the trap efficiency equation. It ranges from 0.046 to 1,
            with a mean value of 0.1, which is also the default. Refer to
            `Verstraeten and Poesen (2000) <https://doi.org/10.1177/030913330002400204>`_.

        release_threshold: float, optional
            Minimum sediment release fraction threshold of the total stream sediment input to stop the simulation.
            The default is 0.95, meaning 95% of the total stream sediment input is leaving the study area.
            If the user does not want to consider this parameter in the simulation, set its value to 1.0.

        write_output : bool, optional
            If ``True``, saves the output dictionary to the specified folder. Default is ``False``.

        folder_path : str, optional
            Path to the folder where JSON files will be saved. Required if ``write_output=True``.

        Returns
        -------
        dict
            A dictionary containing detailed information on dam lifespan, system-wide statistics,
            individual metrics, and simulation parameters.
        '''

        # check validity of folder path
        if write_output:
            if folder_path is None:
                raise ValueError('A valid string of folder_path must be provided when write_output is True.')
            if not os.path.isdir(folder_path):
                raise NotADirectoryError('Input folder path is not valid.')

        # input location and storage capacity of dams
        base_storage = storage_dict.copy()
        storage_sum = sum(base_storage.values())
        dam_ids = list(storage_dict.keys())

        # base storage DataFrame
        bs_df = self._base_storage_df(
            storage_dict=base_storage
        )

        # Stream drainage area and sediment inflow to the stream
        stream_gdf = geopandas.read_file(
            filename=stream_file
        )
        stream_area = max(stream_gdf['csa_m2'])
        stream_sediment = max(stream_gdf['cumsed_kg'])

        # adjacent downstream connectivity of dams
        connection_downstream = self.connectivity_adjacent_downstream_dam(
            stream_file=stream_file,
            dam_list=dam_ids
        )

        # upstream metrics of dams
        metric_dict = self.upstream_metrics_summary(
            stream_file=stream_file,
            dam_list=dam_ids
        )

        # initial adjacent upstream connectivity of dams
        connection_upstream = metric_dict['adjacent_upstream_dams']

        # intial drainage area of dams
        drainage_area = metric_dict['controlled_drainage_m2']

        # intial sediment inflow to dams from own drainage area
        sediment_local = metric_dict['sediment_inflow_kg']

        # intial sediment release from dams
        sediment_release = {
            d_id: 0.0 for d_id in dam_ids
        }

        # DataFrames for storing dam-wise parameter in annual time steps
        drainage_df = pandas.DataFrame(
            columns=dam_ids
        )  # controlled drainage area percentage relative to total stream drainage area
        storage_df = pandas.DataFrame(
            columns=dam_ids
        )  # remaining storage percentage relative to own intial storage
        te_df = pandas.DataFrame(
            columns=dam_ids
        )  # trapping efficiency
        sedtrap_df = pandas.DataFrame(
            columns=dam_ids
        )  # trapped sediment percentage relative to total stream sediment input

        # a DataFrame for storing dam lifespan in years
        life_df = pandas.DataFrame(
            index=dam_ids
        )
        life_df['life_year'] = 0

        # a DataFrame to store dam-system paramters in annual time steps
        system_df = pandas.DataFrame(
            columns=[
                'drainage_%',
                'storage_%',
                'sedtrap_%',
                'sedrelease_%',
                'dam_removed',
                'dam_active'
            ]
        )

        # an empty list to store simulation paramters in annual time steps
        parameter_list = []

        # iterate years
        for year in range(year_limit):
            # break if all dams are removed
            if len(dam_ids) == 0:
                break
            # trap efficiency of dams
            trap_efficiency = self.trap_efficiency_brown(
                storage_dict=storage_dict,
                area_dict=drainage_area,
                brown_d=brown_d
            )
            # drainage area percentage of dams
            area_percent = {
                d_id: 100 * drainage_area[d_id] / stream_area for d_id in dam_ids
            }
            # remaming storage percentage of dams
            storage_percent = {
                d_id: 100 * storage_dict[d_id] / base_storage[d_id] for d_id in dam_ids
            }
            # store dam-wise parameters in annual time steps
            for d_id in dam_ids:
                drainage_df.loc[year + 1, d_id] = area_percent[d_id]
                storage_df.loc[year + 1, d_id] = storage_percent[d_id]
                te_df.loc[year + 1, d_id] = trap_efficiency[d_id]
            # check if any dam removal is required
            dam_removal = [
                d_id for d_id in dam_ids if trap_efficiency[d_id] <= trap_threshold or storage_dict[d_id] <= 0
            ]
            # store inactive dams
            system_df.loc[year + 1, 'dam_removed'] = dam_removal
            # if any dam is removed
            if len(dam_removal) > 0:
                # update dam identifiers
                dam_ids = [i for i in dam_ids if i not in dam_removal]
                # update functions
                connection_downstream = self.connectivity_adjacent_downstream_dam(
                    stream_file=stream_file,
                    dam_list=dam_ids
                )
                metric_dict = self.upstream_metrics_summary(
                    stream_file=stream_file,
                    dam_list=dam_ids
                )
                connection_upstream = metric_dict['adjacent_upstream_dams']
                drainage_area = metric_dict['controlled_drainage_m2']
                sediment_local = metric_dict['sediment_inflow_kg']
                # update storage dictionary
                storage_dict = {
                    d_id: storage_dict[d_id] for d_id in dam_ids
                }
            # released sediment from upstream dams
            sediment_upstream = {
                d_id: sum([sediment_release[j] for j in connection_upstream[d_id]]) for d_id in dam_ids
            }
            # sediment inflow to dams from own drainage area plus outflow from upstream dams
            sediment_inflow = {
                d_id: sediment_local[d_id] + sediment_upstream[d_id] for d_id in dam_ids
            }
            # sediment trapping behind dams
            sediment_trap = {
                d_id: sediment_inflow[d_id] * trap_efficiency[d_id] for d_id in dam_ids
            }
            # sediment trapping percentage relative to total stream sediment input
            sedtrap_percent = {
                d_id: 100 * sediment_trap[d_id] / stream_sediment for d_id in dam_ids
            }
            # store dam-wise sediment trapping in annual time steps
            for d_id in dam_ids:
                sedtrap_df.loc[year + 1, d_id] = sedtrap_percent[d_id]
            # sediment release from dams
            sediment_release = {
                d_id: sediment_inflow[d_id] - sediment_trap[d_id] for d_id in dam_ids
            }
            # annual DataFrame of dams
            year_dict = {
                'ds_conn': connection_downstream,
                'us_conn': connection_upstream,
                'drainage_m2': drainage_area,
                'drainage_%': area_percent,
                'storage_m3': storage_dict,
                'storage_%': storage_percent,
                'trap_efficiency': trap_efficiency,
                'sedlocal_kg': sediment_local,
                'sedtotal_kg': sediment_inflow,
                'sedtrap_kg': sediment_trap,
                'sedrelease_kg': sediment_release
            }
            year_df = self._df_from_dict(
                input_dict=year_dict
            )
            year_df = year_df.reset_index(
                names=['dam_id']
            )
            year_df = year_df[~year_df['dam_id'].isin(dam_removal)].reset_index(drop=True)
            year_df.insert(
                loc=0,
                column='active_dams',
                value=len(dam_ids)
            )
            year_df.insert(
                loc=0,
                column='start_year',
                value=year + 1
            )
            # store annual DataFrame
            if len(year_df) > 0:
                parameter_list.append(year_df)
            # store dam-system parameters in annual time steps
            system_df.loc[year + 1, 'dam_active'] = dam_ids
            system_df.loc[year + 1, 'drainage_%'] = 100 * sum(drainage_area.values()) / stream_area
            system_df.loc[year + 1, 'storage_%'] = 100 * sum(storage_dict.values()) / storage_sum
            system_df.loc[year + 1, 'sedtrap_%'] = 100 * sum(sediment_trap.values()) / stream_sediment
            release_outlets = sum([sediment_release[d_id] for d_id in dam_ids if connection_downstream[d_id] == -1])
            release_undrainage = stream_sediment - sum(sediment_local.values())
            release_fraction = (release_outlets + release_undrainage) / stream_sediment
            system_df.loc[year + 1, 'sedrelease_%'] = 100 * release_fraction
            # break if sediment release cross a certain threshold of total stream sediment inputs
            if release_fraction > release_threshold:
                break
            # annual filling of dam storage due to sediment trapping
            storage_filling = {
                d_id: sediment_trap[d_id] / sediment_density for d_id in dam_ids
            }
            # remaining storage of dams
            storage_dict = {
                d_id: storage_dict[d_id] - storage_filling[d_id] for d_id in dam_ids
            }
            # update dam-wise life expectancy
            for d_id in dam_ids:
                life_df.loc[d_id, 'life_year'] = life_df.loc[d_id, 'life_year'] + 1

        # parameter DataFrame of individual dam in annual time steps
        param_df = pandas.concat(
            objs=parameter_list,
            ignore_index=True
        )

        # dam lifespan DataFrame
        life_df.reset_index(
            names=['dam_id'],
            inplace=True
        )

        # ouptut dictionary
        output = {
            'dam_initial_storage': bs_df,
            'dam_lifespan': life_df,
            'system_statistics': system_df,
            'dam_drainage_area': drainage_df,
            'dam_remaining_storage': storage_df,
            'dam_trap_efficiency': te_df,
            'dam_trapped_sediment': sedtrap_df,
            'simulation_parameters': param_df
        }
        unchanged_keys = [
            'dam_initial_storage',
            'dam_lifespan',
            'simulation_parameters'
        ]
        for key, df in output.items():
            if key not in unchanged_keys:
                df.reset_index(
                    names=['start_year'],
                    inplace=True
                )
                output[key] = df

        # write output dictionary to JSON files
        if write_output and isinstance(folder_path, str):
            for key, df in output.items():
                json_file = os.path.join(folder_path, f'{key}.json')
                df.to_json(
                    path_or_buf=json_file,
                    orient='records',
                    lines=True
                )

        return output

    def storage_dynamics_lite(
        self,
        stream_file: str,
        storage_dict: dict[int, float],
        sediment_density: float,
        trap_threshold: float,
        year_limit: int,
        brown_d: float = 0.1,
        release_threshold: float = 0.95,
        write_output: bool = False,
        folder_path: typing.Optional[str] = None
    ) -> dict[str, pandas.DataFrame]:

        '''
        Simulates a lightweight version of the annual storage dynamics for a dam system
        influenced by sedimentation, and returns a dictionary with the keys ``dam_initial_storage``,
        ``dam_lifespan``, ``system_statistics``, ``dam_remaining_storage``, and ``dam_trap_efficiency``.
        Refer to the method :meth:`OptiDamTool.Network.storage_dynamics_detailed` for full details.

        Parameters
        ----------
        stream_file : str
            Path to the input stream GeoJSON file, generated by
            :meth:`OptiDamTool.Analysis.sediment_delivery_to_stream_geojson`.

        storage_dict : dict
            Dictionary mapping dam identifiers to initial storage capacities in cubic meters.

        sediment_density : float
            Density of sediment in kilograms per cubic meter.

        trap_threshold : float
            Minimum trap efficiency. Dams with efficiency less than or equal to this value are considered inactive.

        year_limit : int
            Maximum number of simulation years. Simulation stops earlier if all dams retire.

        brown_d : float, optional
            Empirical parameter used in the trap efficiency equation. It ranges from 0.046 to 1,
            with a mean value of 0.1, which is also the default. Refer to
            `Verstraeten and Poesen (2000) <https://doi.org/10.1177/030913330002400204>`_.

        release_threshold: float, optional
            Minimum sediment release fraction threshold of the total stream sediment input to stop the simulation.
            The default is 0.95, meaning 95% of the total stream sediment input is leaving the study area.
            If the user does not want to consider this parameter in the simulation, set its value to 1.0.

        write_output : bool, optional
            If ``True``, saves the output dictionary to the specified folder. Default is ``False``.

        folder_path : str, optional
            Path to the folder where JSON files will be saved. Required if ``write_output=True``.

        Returns
        -------
        dict
            A dictionary containing information on dam lifespan, system-wide statistics,
            and storage dynamics and trap efficiency for individual dams.
        '''

        # check validity of folder path
        if write_output:
            if folder_path is None:
                raise ValueError('A valid string of folder_path must be provided when write_output is True.')
            if not os.path.isdir(folder_path):
                raise NotADirectoryError('Input folder path is not valid.')

        # input location and storage capacity of dams
        base_storage = storage_dict.copy()
        storage_sum = sum(base_storage.values())
        dam_ids = list(storage_dict.keys())

        # base storage DataFrame
        bs_df = self._base_storage_df(
            storage_dict=base_storage
        )

        # Stream drainage area and sediment inflow to the stream
        stream_gdf = geopandas.read_file(
            filename=stream_file
        )
        stream_area = max(stream_gdf['csa_m2'])
        stream_sediment = max(stream_gdf['cumsed_kg'])

        # adjacent downstream connectivity of dams
        connection_downstream = self.connectivity_adjacent_downstream_dam(
            stream_file=stream_file,
            dam_list=dam_ids
        )

        # upstream metrics of dams
        metric_dict = self.upstream_metrics_summary(
            stream_file=stream_file,
            dam_list=dam_ids
        )

        # initial adjacent upstream connectivity of dams
        connection_upstream = metric_dict['adjacent_upstream_dams']

        # intial drainage area of dams
        drainage_area = metric_dict['controlled_drainage_m2']

        # intial sediment inflow to dams from own drainage area
        sediment_local = metric_dict['sediment_inflow_kg']

        # intial sediment release from dams
        sediment_release = {
            d_id: 0.0 for d_id in dam_ids
        }

        # DataFrames for storing dam-wise parameter in annual time steps
        storage_df = pandas.DataFrame(
            columns=dam_ids
        )  # remaining storage percentage relative to own intial storage
        te_df = pandas.DataFrame(
            columns=dam_ids
        )  # trapping efficiency

        # a DataFrame for storing dam lifespan in years
        life_df = pandas.DataFrame(
            index=dam_ids
        )
        life_df['life_year'] = 0

        # a DataFrame to store dam-system paramters in annual time steps
        system_df = pandas.DataFrame(
            columns=[
                'drainage_%',
                'storage_%',
                'sedtrap_%',
                'sedrelease_%',
                'dam_removed',
                'dam_active'
            ]
        )

        # iterate years
        for year in range(year_limit):
            # break if all dams are removed
            if len(dam_ids) == 0:
                break
            # trap efficiency of dams
            trap_efficiency = self.trap_efficiency_brown(
                storage_dict=storage_dict,
                area_dict=drainage_area,
                brown_d=brown_d
            )
            # store dam-wise remaining storage percentage and trap efficiency at the begining of the year
            for d_id in dam_ids:
                te_df.loc[year + 1, d_id] = trap_efficiency[d_id]
                storage_df.loc[year + 1, d_id] = 100 * storage_dict[d_id] / base_storage[d_id]
            # check if any dam removal is required based on lower trap efficiency or storage
            dam_removal = [
                d_id for d_id in dam_ids if trap_efficiency[d_id] <= trap_threshold or storage_dict[d_id] <= 0
            ]
            # store inactive dams
            system_df.loc[year + 1, 'dam_removed'] = dam_removal
            # if any dam is removed
            if len(dam_removal) > 0:
                # update dam identifiers
                dam_ids = [i for i in dam_ids if i not in dam_removal]
                # update functions
                connection_downstream = self.connectivity_adjacent_downstream_dam(
                    stream_file=stream_file,
                    dam_list=dam_ids
                )
                metric_dict = self.upstream_metrics_summary(
                    stream_file=stream_file,
                    dam_list=dam_ids
                )
                connection_upstream = metric_dict['adjacent_upstream_dams']
                drainage_area = metric_dict['controlled_drainage_m2']
                sediment_local = metric_dict['sediment_inflow_kg']
                # update storage dictionary
                storage_dict = {
                    d_id: storage_dict[d_id] for d_id in dam_ids
                }
            # released sediment from upstream dams
            sediment_upstream = {
                d_id: sum([sediment_release[j] for j in connection_upstream[d_id]]) for d_id in dam_ids
            }
            # sediment inflow to dams from own drainage area plus outflow from upstream dams
            sediment_inflow = {
                d_id: sediment_local[d_id] + sediment_upstream[d_id] for d_id in dam_ids
            }
            # sediment trapping behind dams
            sediment_trap = {
                d_id: sediment_inflow[d_id] * trap_efficiency[d_id] for d_id in dam_ids
            }
            # sediment release from dams
            sediment_release = {
                d_id: sediment_inflow[d_id] - sediment_trap[d_id] for d_id in dam_ids
            }
            # store dam-system parameters in annual time steps
            system_df.loc[year + 1, 'dam_active'] = dam_ids
            system_df.loc[year + 1, 'drainage_%'] = 100 * sum(drainage_area.values()) / stream_area
            system_df.loc[year + 1, 'storage_%'] = 100 * sum(storage_dict.values()) / storage_sum
            system_df.loc[year + 1, 'sedtrap_%'] = 100 * sum(sediment_trap.values()) / stream_sediment
            release_outlets = sum([sediment_release[d_id] for d_id in dam_ids if connection_downstream[d_id] == -1])
            release_undrainage = stream_sediment - sum(sediment_local.values())
            release_fraction = (release_outlets + release_undrainage) / stream_sediment
            system_df.loc[year + 1, 'sedrelease_%'] = 100 * release_fraction
            # break if sediment release cross a certain threshold of total stream sediment inputs
            if release_fraction > release_threshold:
                break
            # annual filling of dam storage due to sediment trapping
            storage_filling = {
                d_id: sediment_trap[d_id] / sediment_density for d_id in dam_ids
            }
            # remaining storage of dams
            storage_dict = {
                d_id: storage_dict[d_id] - storage_filling[d_id] for d_id in dam_ids
            }
            # update dam-wise life expectancy
            for d_id in dam_ids:
                life_df.loc[d_id, 'life_year'] = life_df.loc[d_id, 'life_year'] + 1

        # dam lifespan DataFrame
        life_df.reset_index(
            names=['dam_id'],
            inplace=True
        )

        # ouptut dictionary
        output = {
            'dam_initial_storage': bs_df,
            'dam_lifespan': life_df,
            'system_statistics': system_df,
            'dam_remaining_storage': storage_df,
            'dam_trap_efficiency': te_df
        }
        unchanged_keys = [
            'dam_initial_storage',
            'dam_lifespan'
        ]
        for key, df in output.items():
            if key not in unchanged_keys:
                df.reset_index(
                    names=['start_year'],
                    inplace=True
                )
                output[key] = df

        # write output dictionary to JSON files
        if write_output and isinstance(folder_path, str):
            for key, df in output.items():
                json_file = os.path.join(folder_path, f'{key}.json')
                df.to_json(
                    path_or_buf=json_file,
                    orient='records',
                    lines=True
                )

        return output

    def storage_dynamics_and_drainage_scenarios(
        self,
        stream_file: str,
        flwdir_file: str,
        storage_dict: dict[int, float],
        sediment_density: float,
        trap_threshold: float,
        year_limit: int,
        folder_path: str,
        brown_d: float = 0.1
    ) -> pandas.DataFrame:

        '''
        Simulates the detailed annual storage dynamics of a dam system and saves the output
        to the specified input directory. Refer to :meth:`OptiDamTool.Network.storage_dynamics_detailed`
        for full implementation details.

        Based on the output DataFrame ``system_statistics``, which contains a ``start_year`` column
        (starting from 1), this method generates GeoDataFrames for dam location points and their
        controlled drainage polygons. These are saved as:

        - ``year_<start_year>_dam_location_point.geojson``
        - ``year_<start_year>_dam_drainage_polygon.geojson``

        Files are created for each year where a dam becomes inactive, along with two additional
        GeoJSON files for ``start_year = 0``, representing the initial condition.

        Refer to :meth:`OptiDamTool.WatemSedem.dam_controlled_drainage_polygons` for more details
        on GeoDataFrame generation.

        In addition, a file ``all_potential_dam_location.geojson`` is created, which contains potential dam locations.
        Each point corresponds to a unique stream segment.

        All GeoJSON files include the ``ws_id`` field to support cross-referencing with the stream network.
        The ``year_{start_year}_dam_drainage_polygon.geojson`` files also contain an ``area_%`` column,
        which represents the percentage of the total stream drainage area controlled by each dam.

        Parameters
        ----------
        stream_file : str
            Path to the input stream GeoJSON file, generated by
            :meth:`OptiDamTool.Analysis.sediment_delivery_to_stream_geojson`.

        flwdir_file : str
            Path to the input flow direction raster file ``flowdir.tif``,
            generated by :meth:`OptiDamTool.WatemSedem.dem_to_stream`.

        storage_dict : dict
            Dictionary mapping dam identifiers to initial storage capacities in cubic meters.

        sediment_density : float
            Density of sediment in kilograms per cubic meter.

        trap_threshold : float
            Minimum trap efficiency. Dams with efficiency less than or equal to this value are considered inactive.

        year_limit : int
            Maximum number of simulation years. Simulation stops earlier if all dams retire.

        folder_path : str
            Path to the folder where JSON and GeoJSON files will be saved.

        brown_d : float, optional
            Empirical parameter used in the trap efficiency equation. It ranges from 0.046 to 1,
            with a mean value of 0.1, which is also the default. Refer to
            `Verstraeten and Poesen (2000) <https://doi.org/10.1177/030913330002400204>`_.

        Returns
        -------
        DataFrame
            A DataFrame with columns ``start_year``, ``dam_removed``, and ``dam_active``.
            The ``start_year`` starts from 0 and helps trace the creation of corresponding GeoJSON files.
        '''

        # stream GeoDataFrame
        stream_gdf = geopandas.read_file(
            filename=stream_file
        )
        stream_area = max(stream_gdf['csa_m2'])

        # all dam location Point GeoDataFrame
        adl_gdf = stream_gdf.copy()
        adl_gdf = adl_gdf[['ws_id', 'geometry']]
        adl_gdf['pour_coords'] = adl_gdf.geometry.apply(
            lambda x: x.coords[-2]
        )
        adl_gdf['geometry'] = adl_gdf.apply(
            lambda row: shapely.Point(row['pour_coords']),
            axis=1
        )
        adl_gdf = adl_gdf.drop(
            columns=['pour_coords']
        )
        location_file = os.path.join(folder_path, 'all_potential_dam_location.geojson')
        adl_gdf.to_file(
            filename=location_file
        )

        # lite version of dam system storage dynamics and save output
        lite_dict = self.storage_dynamics_detailed(
            stream_file=stream_file,
            storage_dict=storage_dict,
            sediment_density=sediment_density,
            trap_threshold=trap_threshold,
            year_limit=year_limit,
            brown_d=brown_d,
            write_output=True,
            folder_path=folder_path
        )

        # zipped iterator of year, dam_removed, and dam_active
        system_df = lite_dict['system_statistics']
        variable_list = list(
            zip(
                [0] + system_df['start_year'].tolist(),
                [[]] + system_df['dam_removed'].tolist(),
                [list(storage_dict.keys())] + system_df['dam_active'].tolist()
            )
        )

        # iterate variables
        for year, dam_removed, dam_active in variable_list:
            # if dam is active
            if len(dam_active) > 0:
                # if year is 0 or dam is removed
                if year == 0 or len(dam_removed) > 0:
                    # dam location points and their controlled drainage area polygons
                    gdf_dict = WatemSedem().dam_controlled_drainage_polygons(
                        flwdir_file=flwdir_file,
                        location_file=location_file,
                        dam_list=dam_active,
                        folder_path=folder_path
                    )
                    # area percentage of polygons
                    gdf_dict['dam_drainage_polygon']['area_%'] = 100 * gdf_dict['dam_drainage_polygon']['area_m2'] / stream_area
                    # save the GeoDataFrames
                    for key, gdf in gdf_dict.items():
                        geojson_file = os.path.join(folder_path, f'year_{year}_{key}.geojson')
                        gdf.to_file(
                            filename=geojson_file
                        )

        # delete files that are not required
        WatemSedem().file.delete_by_name(
            folder_path=folder_path,
            file_names=[
                'dam_location_point',
                'dam_drainage_polygon'
            ]
        )

        # output DataFrame of dam scenarios
        output = pandas.DataFrame(
            data=variable_list,
            columns=[
                'start_year',
                'dam_removed',
                'dam_active'
            ]
        )

        return output
