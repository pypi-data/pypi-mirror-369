import logging
import os
import json
import pandas as pd
import io
import recordlinkage
from typing import List, Tuple, Optional
from pydantic import BaseModel
from .llm import prompt

log = logging.getLogger(__name__)


class ReconciliationPair(BaseModel):
    """Represents a matched pair between source and target records."""

    source_id: str
    target_id: str
    confidence: float = 1.0
    reason: str = ""


class ReconciliationResult(BaseModel):
    """Result of data reconciliation showing linked pairs."""

    pairs: List[ReconciliationPair]
    unmatched_source: List[str] = []
    unmatched_target: List[str] = []
    summary: str = ""


class MatchingPlan(BaseModel):
    """Plan generated by LLM for entity matching."""

    blocking_keys: List[dict] = []
    comparison_vectors: List[dict]


class ReconciliationConfig(BaseModel):
    """Configuration for advanced reconciliation."""

    high_threshold: float = 0.99
    low_threshold: float = 0.75


class SourceTargetMatch(BaseModel):
    """A single source to target match decision."""

    source_index: int
    target_indices: List[int]  # Empty list means no matches


class LLMAdjudicationResult(BaseModel):
    """Structured response from LLM for batch adjudication."""

    matches: List[SourceTargetMatch]


def _load_and_parse_file(file_path: str) -> pd.DataFrame:
    """Load and parse a file into a pandas DataFrame based on file extension."""
    with open(file_path, "r", encoding="utf-8") as f:
        content = f.read()

    file_ext = os.path.splitext(file_path)[1].lower()

    if file_ext == ".json":
        return pd.read_json(io.StringIO(content))
    elif file_ext == ".jsonl":
        records = [
            json.loads(line) for line in content.strip().split("\n") if line.strip()
        ]
        return pd.DataFrame(records)
    elif file_ext in [".csv", ".tsv"]:
        separator = "\t" if file_ext == ".tsv" else ","
        return pd.read_csv(io.StringIO(content), sep=separator)
    elif file_ext in [".txt", ".dat"]:
        # Auto-detect separator for generic text files
        return pd.read_csv(io.StringIO(content), sep=None, engine="python")
    else:
        # Default to CSV auto-detection for unknown extensions
        return pd.read_csv(io.StringIO(content), sep=None, engine="python")


async def _generate_matching_plan(
    rule: str, df_a: pd.DataFrame, df_b: pd.DataFrame, is_deduplication: bool = False
) -> Optional[MatchingPlan]:
    """Uses LLM to create a matching plan based on rule and data samples."""
    task_type = "deduplication" if is_deduplication else "reconciliation"

    df_a_sample = df_a.head(5).to_string()
    df_b_sample = df_b.head(5).to_string()

    system_prompt = f"""
You are an expert data analyst creating entity resolution plans. This is a {task_type} task.

{"You are finding duplicate records within the same dataset and Source A is the same as Source B." if is_deduplication else "You are matching records between two different datasets."}

Analyze a user's rule and data samples to produce a JSON dictionary specifying how to match records.

The JSON dictionary must have (optionally) 'blocking_keys' and 'comparison_vectors':

- 'blocking_keys': A list of dictionaries with 'left_on' and 'right_on' keys.
  CRITICAL: Blocking filters candidate pairs to only those with ONLY EXACT (word-for-word) matches on the blocking columns.
  This should be almost never be filled (usually empty), as there is usually some (expected) variation in duplicate records. 
  
- 'comparison_vectors': A list of dictionaries defining detailed comparisons.
  Each dictionary needs 'left_on' (from Source A), 'right_on' (from Source B), and 'method'.
  For 'method', you MUST choose from: 'jaro_winkler', 'levenshtein', 'damerau_levenshtein', 'cosine'.
  Use fuzzy methods for names/addresses. These comparisons are applied to all candidate pairs.
  Do not include columns that are in the blocking_keys.
"""

    user_prompt = f"""
Natural Language Rule: "{rule}"
---
Source A Data [first 5 rows]:
{df_a_sample}
---
Source B Data [first 5 rows]:
{df_b_sample}
---
IMPORTANT: Only use column names that actually exist in the data above.
Source A columns: {list(df_a.columns)}
Source B columns: {list(df_b.columns)}

Generate the JSON matching plan.
"""

    try:
        full_prompt = system_prompt + "\n\n" + user_prompt
        log.info("Sending plan generation request to LLM...")
        plan = await prompt(full_prompt, response_model=MatchingPlan)
        log.info(
            f"Plan received from LLM: {len(plan.blocking_keys)} blocking keys, {len(plan.comparison_vectors)} comparison vectors"
        )
        return plan

    except Exception as e:
        log.error(f"Error generating match plan: {e}")
        return None


async def _adjudicate_pairs_bulk(
    df_a: pd.DataFrame,
    df_b: pd.DataFrame,
    candidate_pairs: List[Tuple[int, int]],
    instructions: str = "",
) -> List[Tuple[int, int]]:
    """Asks LLM to evaluate multiple pairs at once and return matches using structured output."""
    if not candidate_pairs:
        return []

    # Group by source index to enable many-to-many matching
    source_groups = {}
    for src_idx, tgt_idx in candidate_pairs:
        if src_idx not in source_groups:
            source_groups[src_idx] = []
        source_groups[src_idx].append(tgt_idx)

    # Process 50 sources at a time (smaller batches for structured output)
    batch_size = 50
    all_matches = []

    for i in range(0, len(source_groups), batch_size):
        batch_sources = list(source_groups.keys())[i : i + batch_size]

        # Build structured prompt
        prompt_parts = [
            "You are an entity resolution system. For each source record, determine which target records (if any) refer to the same entity.",
        ]

        if instructions:
            prompt_parts.append(f"\nMatching Instructions: {instructions}")

        prompt_parts.append(
            "\nAnalyze the following data and return matches in the specified JSON format:\n"
        )

        for j, src_idx in enumerate(batch_sources):
            source_record = df_a.loc[src_idx]
            target_indices = source_groups[src_idx]

            prompt_parts.append(f"\nSource {j} (DataFrame index {src_idx}):")
            prompt_parts.append(source_record.to_string())
            prompt_parts.append(f"\nAvailable targets for Source {j}:")

            for tgt_idx in target_indices:
                target_record = df_b.loc[tgt_idx]
                prompt_parts.append(f"  Target {tgt_idx}: {target_record.to_string()}")

        prompt_parts.append(
            f"""
        
For each source (0 to {len(batch_sources) - 1}), provide the target indices that match, or an empty list if no matches.
Be conservative - only match if you're confident they refer to the same entity."""
        )

        full_prompt = "\n".join(prompt_parts)

        try:
            result = await prompt(full_prompt, response_model=LLMAdjudicationResult)

            # Process structured results
            for match in result.matches:
                if 0 <= match.source_index < len(batch_sources):
                    src_idx = batch_sources[match.source_index]
                    for tgt_idx in match.target_indices:
                        if tgt_idx in source_groups[src_idx]:
                            all_matches.append((src_idx, tgt_idx))
                        else:
                            log.warning(
                                f"Target {tgt_idx} not in candidate group for source {src_idx}"
                            )
                else:
                    log.warning(
                        f"Invalid source index: {match.source_index} (max: {len(batch_sources) - 1})"
                    )

        except Exception as e:
            log.error(f"Error in structured adjudication: {e}")
            # Continue with next batch instead of failing completely

    return all_matches


async def reconcile(
    source_path: str,
    target_path: str = "",
    instructions: str = "",
    config: Optional[ReconciliationConfig] = None,
) -> ReconciliationResult:
    """
    Reconciles data between source and target files using advanced two-phase LLM approach.

    Phase 1: LLM creates a matching plan based on data structure and instructions
    Phase 2: LLM adjudicates ambiguous pairs after automated matching

    Args:
        source_path: The file path of the source data to reconcile from.
        target_path: The file path of the target data to reconcile to.
        instructions: The instructions or configuration to apply during reconciliation.
        config: Advanced reconciliation configuration. If None, uses default.

    Returns:
        ReconciliationResult containing matched pairs, unmatched records, and summary.
    """
    # Initialize config if not provided
    if config is None:
        config = ReconciliationConfig()

    # Load and parse data files
    try:
        df_a = _load_and_parse_file(source_path).reset_index(drop=True)
        is_deduplication = not target_path
        if is_deduplication:
            df_b = df_a.copy()
        else:
            df_b = _load_and_parse_file(target_path).reset_index(drop=True)

        # Convert all columns to strings for consistent comparison
        df_a = df_a.astype(str)
        df_b = df_b.astype(str)

        log.info(
            f"Successfully parsed data: {len(df_a)} source records, {len(df_b)} target records"
        )

        # Phase 1: Generate matching plan
        plan = await _generate_matching_plan(
            str(instructions), df_a, df_b, is_deduplication
        )

        if not plan:
            log.error("No plan generated")
            return ReconciliationResult(
                pairs=[],
                unmatched_source=[str(i) for i in range(len(df_a))],
                unmatched_target=[str(i) for i in range(len(df_b))],
                summary="Plan generation failed, no matches found",
            )

        # Phase 2: Execute advanced reconciliation using recordlinkage + LLM
        log.info("Generating and comparing candidate pairs based on plan...")

        # Configure recordlinkage based on the plan
        indexer = recordlinkage.Index()
        if plan.blocking_keys:
            for key in plan.blocking_keys:
                indexer.block(left_on=key["left_on"], right_on=key["right_on"])
        else:
            # No blocking keys - compare all pairs
            indexer.full()
            log.info("No blocking - comparing all pairs")

        compare = recordlinkage.Compare()
        for comp in plan.comparison_vectors:
            label = f"{comp['left_on']}_{comp['method']}"
            method = (
                comp["method"]
                if comp["method"]
                in ["jaro_winkler", "levenshtein", "damerau_levenshtein", "cosine"]
                else "levenshtein"
            )
            compare.string(
                comp["left_on"], comp["right_on"], method=method, label=label
            )

        # Generate candidate pairs and compute similarity scores
        candidate_links = indexer.index(df_a, df_b)

        # For deduplication, filter out self-matches and redundant pairs
        if is_deduplication:
            candidate_links = candidate_links[
                candidate_links.get_level_values(0)
                < candidate_links.get_level_values(1)
            ]

        features = compare.compute(candidate_links, df_a, df_b)

        # Score and triage pairs
        features["total_score"] = features.mean(axis=1)
        clear_matches = features[features["total_score"] >= config.high_threshold]
        ambiguous_pairs = features[
            (features["total_score"] >= config.low_threshold)
            & (features["total_score"] < config.high_threshold)
        ]

        log.info(
            f"Found {len(clear_matches)} clear matches (score >= {config.high_threshold})."
        )
        log.info(f"Found {len(ambiguous_pairs)} ambiguous pairs for LLM review.")

        # Send all ambiguous pairs to LLM adjudicator
        llm_decisions = []
        if not ambiguous_pairs.empty:
            candidate_pairs = list(ambiguous_pairs.index)
            llm_decisions = await _adjudicate_pairs_bulk(
                df_a, df_b, candidate_pairs, str(instructions)
            )

        log.info(
            f"LLM confirmed {len(llm_decisions)} additional matches (including many-to-many)."
        )

        # Convert to ReconciliationPairs
        pairs = []

        # Add clear matches
        for idx_a, idx_b in clear_matches.index:
            pairs.append(
                ReconciliationPair(
                    source_id=str(idx_a),
                    target_id=str(idx_b),
                    confidence=float(clear_matches.loc[(idx_a, idx_b), "total_score"]),
                    reason="High similarity score (automated match)",
                )
            )

        # Add LLM confirmed matches
        for idx_a, idx_b in llm_decisions:
            score = (
                float(ambiguous_pairs.loc[(idx_a, idx_b), "total_score"])
                if (idx_a, idx_b) in ambiguous_pairs.index
                else 0.8
            )
            pairs.append(
                ReconciliationPair(
                    source_id=str(idx_a),
                    target_id=str(idx_b),
                    confidence=score,
                    reason="LLM confirmed match",
                )
            )

        # Determine unmatched records
        matched_source_ids = {pair.source_id for pair in pairs}
        matched_target_ids = {pair.target_id for pair in pairs}

        if is_deduplication:
            all_matched_ids = matched_source_ids | matched_target_ids
            unmatched_source = [
                str(i) for i in range(len(df_a)) if str(i) not in all_matched_ids
            ]
            unmatched_target = unmatched_source
        else:
            unmatched_source = [
                str(i) for i in range(len(df_a)) if str(i) not in matched_source_ids
            ]
            unmatched_target = [
                str(i) for i in range(len(df_b)) if str(i) not in matched_target_ids
            ]

        return ReconciliationResult(
            pairs=pairs,
            unmatched_source=unmatched_source,
            unmatched_target=unmatched_target,
            summary=f"Advanced reconciliation: {len(pairs)} matches found using two-phase LLM approach",
        )

    except Exception as e:
        raise ValueError(f"Failed to process reconciliation: {str(e)}")
