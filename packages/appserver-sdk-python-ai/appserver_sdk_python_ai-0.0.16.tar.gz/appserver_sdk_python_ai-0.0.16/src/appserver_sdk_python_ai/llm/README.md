# M√≥dulo LLM - AppServer SDK Python AI

Um m√≥dulo profissional para integra√ß√£o com modelos de linguagem (LLM) e APIs de intelig√™ncia artificial.

## üöÄ Caracter√≠sticas

- **üîÑ Cliente Ass√≠ncrono e S√≠ncrono**: Suporte completo para opera√ß√µes s√≠ncronas e ass√≠ncronas
- **üõ°Ô∏è Retry Autom√°tico**: Sistema robusto de retry com backoff exponencial
- **üìä Modelos Pydantic**: Valida√ß√£o de dados com type hints completos
- **üîå M√∫ltiplos Provedores**: Suporte para diferentes APIs de IA (OpenAI, Anthropic, etc.)
- **üìù Logging Estruturado**: Sistema de logs detalhado para debugging e monitoramento
- **‚ö° Performance Otimizada**: Conex√µes reutiliz√°veis e pool de conex√µes
- **üîê Seguran√ßa**: Gerenciamento seguro de API keys e autentica√ß√£o
- **üéØ Interface Simples**: API intuitiva para uso b√°sico e avan√ßado

## üìÅ Estrutura do M√≥dulo

```
llm/
‚îú‚îÄ‚îÄ __init__.py                 # Inicializa√ß√£o e exports principais
‚îú‚îÄ‚îÄ README.md                   # Esta documenta√ß√£o
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ model_manager.py       # Gerenciamento de modelos
‚îÇ   ‚îî‚îÄ‚îÄ token_counter.py       # Contagem de tokens
‚îú‚îÄ‚îÄ service/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ ai_service.py          # Servi√ßo principal de IA
‚îÇ   ‚îî‚îÄ‚îÄ providers/             # Provedores espec√≠ficos
‚îî‚îÄ‚îÄ exceptions/
    ‚îî‚îÄ‚îÄ llm_exceptions.py      # Exce√ß√µes customizadas
```

## üì¶ Instala√ß√£o

### Depend√™ncias Obrigat√≥rias
```bash
pip install httpx pydantic tiktoken
```

### Depend√™ncias Opcionais
```bash
pip install openai anthropic  # Para provedores espec√≠ficos
```

## üî• Uso R√°pido

### Cliente B√°sico
```python
from appserver_sdk_python_ai.llm import AIService

# Configurar cliente
ai_service = AIService(
    api_key="sua-api-key",
    base_url="https://api.appserver.com.br/ai/v1"
)

# Fazer requisi√ß√£o simples
response = ai_service.chat(
    prompt="Explique machine learning em termos simples",
    model="gpt-4",
    max_tokens=500
)

print(response.content)
print(f"Tokens utilizados: {response.usage.total_tokens}")
```

### Chat com Contexto
```python
from appserver_sdk_python_ai.llm import AIService, Message

ai_service = AIService(api_key="sua-api-key")

# Conversa com contexto
messages = [
    Message(role="system", content="Voc√™ √© um assistente especializado em Python."),
    Message(role="user", content="Como criar uma fun√ß√£o recursiva?"),
    Message(role="assistant", content="Uma fun√ß√£o recursiva √© uma fun√ß√£o que chama a si mesma..."),
    Message(role="user", content="Pode dar um exemplo pr√°tico?")
]

response = ai_service.chat_with_messages(
    messages=messages,
    model="gpt-4",
    temperature=0.7
)

print(response.content)
```

### Cliente Ass√≠ncrono
```python
import asyncio
from appserver_sdk_python_ai.llm import AsyncAIService

async def main():
    ai_service = AsyncAIService(api_key="sua-api-key")
    
    response = await ai_service.chat(
        prompt="O que √© intelig√™ncia artificial?",
        model="gpt-3.5-turbo"
    )
    
    print(response.content)
    await ai_service.close()

asyncio.run(main())
```

## ‚öôÔ∏è Configura√ß√£o Avan√ßada

### AIConfig - Todas as Op√ß√µes
```python
from appserver_sdk_python_ai.llm import AIService, AIConfig

config = AIConfig(
    # Configura√ß√µes de rede
    base_url="https://api.appserver.com.br/ai/v1",
    api_key="sua-api-key",
    timeout=30,                    # Timeout em segundos
    verify_ssl=True,               # Verificar certificados SSL
    
    # Configura√ß√µes de retry
    max_retries=3,                 # M√°ximo de tentativas
    retry_delay=1.0,               # Delay inicial entre tentativas
    backoff_factor=2.0,            # Fator de backoff exponencial
    
    # Configura√ß√µes de logging
    debug=False,                   # Modo debug
    log_requests=True,             # Log de requisi√ß√µes
    log_responses=False,           # Log de respostas (cuidado com dados sens√≠veis)
    
    # Headers customizados
    headers={
        "User-Agent": "AppServer-SDK/1.0",
        "Accept": "application/json"
    }
)

ai_service = AIService(config=config)
```

### Modelos de Dados

#### ChatRequest
```python
from appserver_sdk_python_ai.llm import ChatRequest

request = ChatRequest(
    prompt="Sua pergunta aqui",
    model="gpt-4",                 # Modelo a ser usado
    max_tokens=1000,               # M√°ximo de tokens na resposta
    temperature=0.7,               # Criatividade (0.0 a 1.0)
    top_p=1.0,                     # Nucleus sampling
    frequency_penalty=0.0,         # Penalidade de frequ√™ncia
    presence_penalty=0.0,          # Penalidade de presen√ßa
    stop=["\n\n"],                # Tokens de parada
    stream=False                   # Streaming de resposta
)
```

#### ChatResponse
```python
# A resposta cont√©m:
response.content          # Conte√∫do da resposta
response.model           # Modelo usado
response.usage           # Informa√ß√µes de uso (tokens)
response.finish_reason   # Raz√£o do fim da gera√ß√£o
response.created_at      # Timestamp da cria√ß√£o
response.id              # ID √∫nico da resposta
```

## üéØ Casos de Uso Pr√°ticos

### 1. An√°lise de Sentimento
```python
from appserver_sdk_python_ai.llm import AIService

def analisar_sentimento(texto):
    ai_service = AIService(api_key="sua-api-key")
    
    prompt = f"""
    Analise o sentimento do seguinte texto e retorne apenas uma das op√ß√µes:
    - POSITIVO
    - NEGATIVO
    - NEUTRO
    
    Texto: "{texto}"
    
    Sentimento:
    """
    
    response = ai_service.chat(
        prompt=prompt,
        model="gpt-3.5-turbo",
        max_tokens=10,
        temperature=0.1
    )
    
    return response.content.strip()

# Exemplo de uso
textos = [
    "Adorei o produto, superou minhas expectativas!",
    "O servi√ßo foi terr√≠vel, n√£o recomendo.",
    "O produto chegou no prazo esperado."
]

for texto in textos:
    sentimento = analisar_sentimento(texto)
    print(f"'{texto}' -> {sentimento}")
```

### 2. Gera√ß√£o de Resumos
```python
from appserver_sdk_python_ai.llm import AIService

def gerar_resumo(texto, max_palavras=100):
    ai_service = AIService(api_key="sua-api-key")
    
    prompt = f"""
    Crie um resumo conciso do seguinte texto em no m√°ximo {max_palavras} palavras.
    Mantenha os pontos principais e informa√ß√µes essenciais.
    
    Texto:
    {texto}
    
    Resumo:
    """
    
    response = ai_service.chat(
        prompt=prompt,
        model="gpt-4",
        max_tokens=max_palavras * 2,  # Margem de seguran√ßa
        temperature=0.3
    )
    
    return response.content.strip()

# Exemplo de uso
texto_longo = """
A intelig√™ncia artificial (IA) √© uma √°rea da ci√™ncia da computa√ß√£o que se concentra 
no desenvolvimento de sistemas capazes de realizar tarefas que normalmente requerem 
intelig√™ncia humana. Isso inclui aprendizado, racioc√≠nio, percep√ß√£o, compreens√£o 
de linguagem natural e resolu√ß√£o de problemas. A IA tem aplica√ß√µes em diversos 
setores, desde sa√∫de e educa√ß√£o at√© transporte e entretenimento.
"""

resumo = gerar_resumo(texto_longo, max_palavras=50)
print(f"Resumo: {resumo}")
```

### 3. Chatbot com Mem√≥ria
```python
from appserver_sdk_python_ai.llm import AIService, Message

class ChatBot:
    def __init__(self, api_key, system_prompt="Voc√™ √© um assistente √∫til."):
        self.ai_service = AIService(api_key=api_key)
        self.messages = [Message(role="system", content=system_prompt)]
    
    def chat(self, user_input):
        # Adicionar mensagem do usu√°rio
        self.messages.append(Message(role="user", content=user_input))
        
        # Obter resposta
        response = self.ai_service.chat_with_messages(
            messages=self.messages,
            model="gpt-3.5-turbo",
            max_tokens=500,
            temperature=0.7
        )
        
        # Adicionar resposta do assistente ao hist√≥rico
        self.messages.append(Message(role="assistant", content=response.content))
        
        return response.content
    
    def reset(self):
        # Manter apenas a mensagem do sistema
        self.messages = self.messages[:1]

# Exemplo de uso
bot = ChatBot(
    api_key="sua-api-key",
    system_prompt="Voc√™ √© um especialista em Python. Responda de forma did√°tica."
)

print("ChatBot iniciado! Digite 'sair' para encerrar.")
while True:
    user_input = input("Voc√™: ")
    if user_input.lower() == 'sair':
        break
    
    response = bot.chat(user_input)
    print(f"Bot: {response}")
```

### 4. Processamento em Lote Ass√≠ncrono
```python
import asyncio
from appserver_sdk_python_ai.llm import AsyncAIService

async def processar_textos_em_lote(textos, prompt_template):
    ai_service = AsyncAIService(api_key="sua-api-key")
    
    async def processar_texto(texto):
        prompt = prompt_template.format(texto=texto)
        response = await ai_service.chat(
            prompt=prompt,
            model="gpt-3.5-turbo",
            max_tokens=200
        )
        return {"texto_original": texto, "resultado": response.content}
    
    # Processar todos os textos em paralelo
    tasks = [processar_texto(texto) for texto in textos]
    resultados = await asyncio.gather(*tasks)
    
    await ai_service.close()
    return resultados

# Exemplo de uso
async def main():
    textos = [
        "Python √© uma linguagem de programa√ß√£o.",
        "Machine learning √© um subcampo da IA.",
        "APIs facilitam a integra√ß√£o entre sistemas."
    ]
    
    prompt_template = """
    Traduza o seguinte texto para ingl√™s:
    
    Texto: {texto}
    
    Tradu√ß√£o:
    """
    
    resultados = await processar_textos_em_lote(textos, prompt_template)
    
    for resultado in resultados:
        print(f"Original: {resultado['texto_original']}")
        print(f"Tradu√ß√£o: {resultado['resultado']}")
        print("-" * 50)

# asyncio.run(main())
```

### 5. Extra√ß√£o de Informa√ß√µes Estruturadas
```python
from appserver_sdk_python_ai.llm import AIService
import json

def extrair_informacoes_contato(texto):
    ai_service = AIService(api_key="sua-api-key")
    
    prompt = f"""
    Extraia as informa√ß√µes de contato do seguinte texto e retorne em formato JSON.
    Se alguma informa√ß√£o n√£o estiver dispon√≠vel, use null.
    
    Formato esperado:
    {{
        "nome": "string",
        "email": "string",
        "telefone": "string",
        "empresa": "string",
        "cargo": "string"
    }}
    
    Texto:
    {texto}
    
    JSON:
    """
    
    response = ai_service.chat(
        prompt=prompt,
        model="gpt-4",
        max_tokens=300,
        temperature=0.1
    )
    
    try:
        return json.loads(response.content)
    except json.JSONDecodeError:
        return None

# Exemplo de uso
texto_contato = """
Ol√°, meu nome √© Jo√£o Silva e trabalho como Desenvolvedor Senior na TechCorp.
Voc√™ pode me contatar pelo email joao.silva@techcorp.com ou pelo telefone (11) 99999-9999.
Estou interessado em discutir oportunidades de parceria.
"""

info = extrair_informacoes_contato(texto_contato)
if info:
    print(json.dumps(info, indent=2, ensure_ascii=False))
else:
    print("N√£o foi poss√≠vel extrair as informa√ß√µes.")
```

## üîÑ Streaming de Respostas

```python
from appserver_sdk_python_ai.llm import AsyncAIService

async def chat_com_streaming():
    ai_service = AsyncAIService(api_key="sua-api-key")
    
    prompt = "Conte uma hist√≥ria interessante sobre intelig√™ncia artificial"
    
    print("Resposta: ", end="", flush=True)
    
    async for chunk in ai_service.chat_stream(
        prompt=prompt,
        model="gpt-4",
        max_tokens=500
    ):
        print(chunk.content, end="", flush=True)
    
    print()  # Nova linha no final
    await ai_service.close()

# asyncio.run(chat_com_streaming())
```

## üö® Tratamento de Erros

```python
from appserver_sdk_python_ai.llm import (
    AIService,
    AIException,
    AIConnectionError,
    AIAuthenticationError,
    AIRateLimitError,
    AITimeoutError,
    AIModelNotFoundError
)

def chat_com_tratamento_erro(prompt):
    ai_service = AIService(api_key="sua-api-key")
    
    try:
        response = ai_service.chat(
            prompt=prompt,
            model="gpt-4",
            max_tokens=500
        )
        return response.content
        
    except AIAuthenticationError:
        return "Erro: API key inv√°lida ou expirada"
    except AIRateLimitError as e:
        return f"Erro: Limite de taxa excedido. Tente novamente em {e.retry_after} segundos"
    except AIModelNotFoundError:
        return "Erro: Modelo especificado n√£o encontrado"
    except AITimeoutError:
        return "Erro: Timeout na requisi√ß√£o"
    except AIConnectionError:
        return "Erro: Problema de conex√£o com a API"
    except AIException as e:
        return f"Erro geral da API: {e}"
    except Exception as e:
        return f"Erro inesperado: {e}"

# Exemplo de uso
resultado = chat_com_tratamento_erro("Explique quantum computing")
print(resultado)
```

## üìä Monitoramento e M√©tricas

### Contagem de Tokens
```python
from appserver_sdk_python_ai.llm import TokenCounter

# Contar tokens antes de enviar
counter = TokenCounter()

prompt = "Explique machine learning em detalhes"
tokens_prompt = counter.count_tokens(prompt, model="gpt-4")
print(f"Tokens no prompt: {tokens_prompt}")

# Estimar custo
custo_estimado = counter.estimate_cost(
    prompt_tokens=tokens_prompt,
    completion_tokens=500,  # Estimativa
    model="gpt-4"
)
print(f"Custo estimado: ${custo_estimado:.4f}")
```

### Logging Personalizado
```python
import logging
from appserver_sdk_python_ai.llm import AIService

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger("meu_app")

ai_service = AIService(
    api_key="sua-api-key",
    debug=True  # Habilita logs detalhados
)

# Os logs ser√£o automaticamente gerados
response = ai_service.chat(
    prompt="Teste de logging",
    model="gpt-3.5-turbo"
)
```

## üîê Seguran√ßa e Boas Pr√°ticas

### Gerenciamento Seguro de API Keys
```python
import os
from appserver_sdk_python_ai.llm import AIService

# Usar vari√°veis de ambiente
ai_service = AIService(
    api_key=os.getenv("APPSERVER_API_KEY"),
    base_url=os.getenv("APPSERVER_BASE_URL", "https://api.appserver.com.br/ai/v1")
)

# Verificar se a API key est√° configurada
if not os.getenv("APPSERVER_API_KEY"):
    raise ValueError("API key n√£o configurada. Defina a vari√°vel APPSERVER_API_KEY")
```

### Valida√ß√£o de Entrada
```python
from appserver_sdk_python_ai.llm import AIService
import re

def validar_prompt(prompt):
    """Valida e sanitiza o prompt antes de enviar"""
    if not prompt or not prompt.strip():
        raise ValueError("Prompt n√£o pode estar vazio")
    
    if len(prompt) > 10000:  # Limite de caracteres
        raise ValueError("Prompt muito longo")
    
    # Remover caracteres potencialmente problem√°ticos
    prompt_limpo = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', prompt)
    
    return prompt_limpo.strip()

def chat_seguro(prompt):
    try:
        prompt_validado = validar_prompt(prompt)
        ai_service = AIService(api_key=os.getenv("APPSERVER_API_KEY"))
        
        response = ai_service.chat(
            prompt=prompt_validado,
            model="gpt-3.5-turbo",
            max_tokens=500
        )
        
        return response.content
        
    except ValueError as e:
        return f"Erro de valida√ß√£o: {e}"
    except Exception as e:
        return f"Erro: {e}"
```

## üöÄ Performance e Otimiza√ß√£o

### Pool de Conex√µes
```python
from appserver_sdk_python_ai.llm import AsyncAIService

# Cliente com pool de conex√µes otimizado
ai_service = AsyncAIService(
    api_key="sua-api-key",
    max_connections=100,
    max_keepalive_connections=20,
    timeout=30
)
```

### Cache de Respostas
```python
from appserver_sdk_python_ai.llm import AIService
import hashlib
import json
from functools import lru_cache

class CachedAIService:
    def __init__(self, api_key):
        self.ai_service = AIService(api_key=api_key)
        self._cache = {}
    
    def _get_cache_key(self, prompt, model, **kwargs):
        """Gera chave √∫nica para o cache"""
        cache_data = {
            "prompt": prompt,
            "model": model,
            **kwargs
        }
        cache_str = json.dumps(cache_data, sort_keys=True)
        return hashlib.md5(cache_str.encode()).hexdigest()
    
    def chat(self, prompt, model="gpt-3.5-turbo", **kwargs):
        cache_key = self._get_cache_key(prompt, model, **kwargs)
        
        # Verificar cache
        if cache_key in self._cache:
            print("Cache hit!")
            return self._cache[cache_key]
        
        # Fazer requisi√ß√£o
        response = self.ai_service.chat(
            prompt=prompt,
            model=model,
            **kwargs
        )
        
        # Armazenar no cache
        self._cache[cache_key] = response
        return response

# Uso
cached_ai = CachedAIService(api_key="sua-api-key")
response1 = cached_ai.chat("O que √© Python?")  # Requisi√ß√£o √† API
response2 = cached_ai.chat("O que √© Python?")  # Cache hit!
```

## üß™ Testes

```python
import pytest
from unittest.mock import Mock, patch
from appserver_sdk_python_ai.llm import AIService, ChatResponse

@pytest.fixture
def ai_service():
    return AIService(api_key="test-key")

def test_chat_success(ai_service):
    # Mock da resposta
    mock_response = ChatResponse(
        content="Resposta de teste",
        model="gpt-3.5-turbo",
        usage={"total_tokens": 50}
    )
    
    with patch.object(ai_service, 'chat', return_value=mock_response):
        response = ai_service.chat("Teste")
        assert response.content == "Resposta de teste"
        assert response.model == "gpt-3.5-turbo"

def test_chat_error(ai_service):
    with patch.object(ai_service, 'chat', side_effect=Exception("API Error")):
        with pytest.raises(Exception):
            ai_service.chat("Teste")

# Executar testes
# pytest src/appserver_sdk_python_ai/llm/tests/ -v
```

## ü§ù Contribui√ß√£o

Para contribuir com o m√≥dulo LLM:

1. **Novos Provedores**: Adicione suporte a novos provedores de IA
2. **Otimiza√ß√µes**: Melhore performance e efici√™ncia
3. **Funcionalidades**: Adicione novos recursos e capacidades
4. **Testes**: Expanda a cobertura de testes
5. **Documenta√ß√£o**: Melhore exemplos e documenta√ß√£o

```bash
# Executar testes do m√≥dulo LLM
python -m pytest src/appserver_sdk_python_ai/llm/tests/ -v

# Verificar cobertura
python -m pytest src/appserver_sdk_python_ai/llm/tests/ --cov=llm --cov-report=html
```

## üìö Recursos Adicionais

- **[Documenta√ß√£o da API](https://docs.appserver.com.br/ai/)**
- **[Exemplos no GitHub](https://github.com/appserver/appserver-sdk-python-ai/tree/main/examples)**
- **[Changelog](../../../CHANGELOG.md)**
- **[Issues e Suporte](https://github.com/appserver/appserver-sdk-python-ai/issues)**

## üìÑ Licen√ßa

Este m√≥dulo faz parte do AppServer SDK Python AI e segue a mesma licen√ßa do projeto principal.

---

**Vers√£o**: 1.0.0  
**√öltima atualiza√ß√£o**: 2024  
**Compatibilidade**: Python 3.8+  
**Desenvolvido com ‚ù§Ô∏è pela equipe AppServer**