"""Code chunk processor for creating enhanced code chunk documents."""

from typing import Any

import structlog

from qdrant_loader.core.chunking.strategy.base.chunk_processor import BaseChunkProcessor
from qdrant_loader.core.document import Document

logger = structlog.get_logger(__name__)


class CodeChunkProcessor(BaseChunkProcessor):
    """Chunk processor for code documents with programming language context."""

    def __init__(self, settings):
        """Initialize the code chunk processor.

        Args:
            settings: Configuration settings
        """
        super().__init__(settings)
        self.logger = logger

        # Code-specific configuration
        self.code_config = getattr(
            settings.global_config.chunking.strategies, "code", None
        )
        self.max_chunk_size_for_nlp = getattr(
            self.code_config, "max_chunk_size_for_nlp", 20000
        )

        # NLP skip conditions for code
        self.skip_conditions = {
            "large_content": self.max_chunk_size_for_nlp,
            "binary_patterns": ["\x00", "\xff", "\xfe"],
            "minified_code_threshold": 0.1,  # Ratio of meaningful chars
            "generated_code_patterns": [
                "auto-generated",
                "do not edit",
                "generated by",
            ],
        }

    def create_chunk_document(
        self,
        original_doc: Document,
        chunk_content: str,
        chunk_index: int,
        total_chunks: int,
        chunk_metadata: dict[str, Any],
        skip_nlp: bool = False,
    ) -> Document:
        """Create a document for a code chunk with enhanced metadata.

        Args:
            original_doc: The original document being chunked
            chunk_content: The content of this chunk
            chunk_index: Index of this chunk (0-based)
            total_chunks: Total number of chunks
            chunk_metadata: Metadata specific to this chunk
            skip_nlp: Whether to skip semantic analysis for this chunk

        Returns:
            Document instance representing the code chunk
        """
        # Generate unique chunk ID
        chunk_id = self.generate_chunk_id(original_doc, chunk_index)

        # Create base metadata
        base_metadata = self.create_base_chunk_metadata(
            original_doc, chunk_index, total_chunks, chunk_metadata
        )

        # Add code-specific metadata
        code_metadata = self._create_code_specific_metadata(
            chunk_content, chunk_metadata, original_doc
        )
        base_metadata.update(code_metadata)

        # Determine if we should skip NLP for this chunk
        if not skip_nlp:
            skip_nlp, skip_reason = self.should_skip_semantic_analysis(
                chunk_content, chunk_metadata
            )
            if skip_nlp:
                base_metadata["nlp_skip_reason"] = skip_reason

        # Create chunk document
        chunk_doc = Document(
            id=chunk_id,
            content=chunk_content,
            metadata=base_metadata,
            source=original_doc.source,
            source_type=original_doc.source_type,
            url=original_doc.url,
            content_type=original_doc.content_type,
            title=self._generate_chunk_title(original_doc, chunk_metadata, chunk_index),
        )

        return chunk_doc

    def should_skip_semantic_analysis(
        self, chunk_content: str, chunk_metadata: dict[str, Any]
    ) -> tuple[bool, str]:
        """Determine whether to skip semantic analysis for a code chunk.

        Args:
            chunk_content: Content of the chunk
            chunk_metadata: Metadata for the chunk

        Returns:
            Tuple of (should_skip, reason)
        """
        content_length = len(chunk_content)

        # Skip if content is too large
        if content_length > self.skip_conditions["large_content"]:
            return True, "content_too_large"

        # Skip if content appears to be binary
        if any(
            pattern in chunk_content
            for pattern in self.skip_conditions["binary_patterns"]
        ):
            return True, "binary_content"

        # Skip if content appears to be minified
        if self._is_minified_code(chunk_content):
            return True, "minified_code"

        # Skip if content appears to be auto-generated
        if self._is_generated_code(chunk_content):
            return True, "generated_code"

        # Skip if it's mostly comments (low semantic value)
        if self._is_mostly_comments(chunk_content):
            return True, "mostly_comments"

        # Skip test files with many assertions (low semantic complexity)
        if chunk_metadata.get("element_type") == "test" and content_length < 500:
            return True, "simple_test_code"

        # Skip configuration or data files
        if chunk_metadata.get("language") in ["json", "yaml", "xml", "ini"]:
            return True, "configuration_file"

        return False, "suitable_for_nlp"

    def _create_code_specific_metadata(
        self, content: str, chunk_metadata: dict[str, Any], original_doc: Document
    ) -> dict[str, Any]:
        """Create code-specific metadata for the chunk.

        Args:
            content: Chunk content
            chunk_metadata: Existing chunk metadata
            original_doc: Original document

        Returns:
            Code-specific metadata dictionary
        """
        metadata = {
            "content_analysis": self._analyze_code_content(content),
            "language_context": self._extract_language_context(content, chunk_metadata),
            "code_quality": self._assess_code_quality(content, chunk_metadata),
            "educational_value": self._assess_educational_value(
                content, chunk_metadata
            ),
            "reusability_score": self._calculate_reusability_score(
                content, chunk_metadata
            ),
            "chunking_strategy": "code_modular",
        }

        # Add element-specific context
        element_type = chunk_metadata.get("element_type", "unknown")
        if element_type != "unknown":
            metadata["element_context"] = self._extract_element_context(
                content, element_type
            )

        return metadata

    def _analyze_code_content(self, content: str) -> dict[str, Any]:
        """Analyze the code content characteristics.

        Args:
            content: Code content

        Returns:
            Content analysis metrics
        """
        lines = content.split("\n")
        non_empty_lines = [line for line in lines if line.strip()]
        comment_lines = [
            line for line in lines if line.strip().startswith(("#", "//", "/*", "--"))
        ]

        return {
            "total_lines": len(lines),
            "code_lines": len(non_empty_lines) - len(comment_lines),
            "comment_lines": len(comment_lines),
            "blank_lines": len(lines) - len(non_empty_lines),
            "comment_ratio": (
                len(comment_lines) / len(non_empty_lines) if non_empty_lines else 0
            ),
            "avg_line_length": (
                sum(len(line) for line in lines) / len(lines) if lines else 0
            ),
            "max_line_length": max(len(line) for line in lines) if lines else 0,
            "indentation_consistency": self._check_indentation_consistency(lines),
            "has_documentation": '"""' in content
            or "'''" in content
            or "/*" in content,
        }

    def _extract_language_context(
        self, content: str, chunk_metadata: dict[str, Any]
    ) -> dict[str, Any]:
        """Extract programming language context.

        Args:
            content: Code content
            chunk_metadata: Chunk metadata

        Returns:
            Language context information
        """
        language = chunk_metadata.get("language", "unknown")

        context = {
            "language": language,
            "paradigm": self._identify_programming_paradigm(content, language),
            "framework_indicators": self._identify_frameworks(content, language),
            "version_indicators": self._identify_language_version(content, language),
            "style_conventions": self._analyze_style_conventions(content, language),
        }

        return context

    def _assess_code_quality(
        self, content: str, chunk_metadata: dict[str, Any]
    ) -> dict[str, Any]:
        """Assess code quality indicators.

        Args:
            content: Code content
            chunk_metadata: Chunk metadata

        Returns:
            Code quality assessment
        """
        # Get complexity from metadata if available
        complexity = chunk_metadata.get("complexity", 0)

        quality_score = 100  # Start with perfect score

        # Deduct points for various quality issues
        if complexity > 10:
            quality_score -= 20
        elif complexity > 5:
            quality_score -= 10

        # Check for long lines
        lines = content.split("\n")
        long_lines = [line for line in lines if len(line) > 120]
        if len(long_lines) > len(lines) * 0.3:
            quality_score -= 15

        # Check for documentation
        has_docs = '"""' in content or "'''" in content
        if not has_docs and len(content) > 500:
            quality_score -= 10

        # Check for meaningful naming
        if self._has_meaningful_names(content):
            quality_score += 5
        else:
            quality_score -= 10

        return {
            "quality_score": max(0, quality_score),
            "complexity_level": (
                "low" if complexity < 3 else "medium" if complexity < 8 else "high"
            ),
            "readability_indicators": {
                "has_documentation": has_docs,
                "reasonable_line_length": (
                    len(long_lines) / len(lines) < 0.1 if lines else True
                ),
                "meaningful_names": self._has_meaningful_names(content),
            },
        }

    def _assess_educational_value(
        self, content: str, chunk_metadata: dict[str, Any]
    ) -> dict[str, Any]:
        """Assess educational value of the code chunk.

        Args:
            content: Code content
            chunk_metadata: Chunk metadata

        Returns:
            Educational value assessment
        """
        educational_indicators = []

        # Check for common educational patterns
        if "example" in content.lower() or "demo" in content.lower():
            educational_indicators.append("example_code")

        if '"""' in content or "'''" in content:
            educational_indicators.append("well_documented")

        if "TODO" in content or "FIXME" in content:
            educational_indicators.append("learning_opportunity")

        # Check complexity level for learning
        complexity = chunk_metadata.get("complexity", 0)
        if 2 <= complexity <= 6:
            educational_indicators.append("good_complexity_for_learning")

        # Check for design patterns
        element_type = chunk_metadata.get("element_type", "unknown")
        if element_type in ["class", "interface"]:
            educational_indicators.append("object_oriented_concepts")

        return {
            "educational_indicators": educational_indicators,
            "learning_level": self._determine_learning_level(content, chunk_metadata),
            "concepts_demonstrated": self._identify_programming_concepts(content),
        }

    def _calculate_reusability_score(
        self, content: str, chunk_metadata: dict[str, Any]
    ) -> int:
        """Calculate reusability score for the code chunk.

        Args:
            content: Code content
            chunk_metadata: Chunk metadata

        Returns:
            Reusability score (0-100)
        """
        score = 50  # Base score

        # Higher score for certain element types
        element_type = chunk_metadata.get("element_type", "unknown")
        if element_type in ["function", "class", "interface"]:
            score += 20
        elif element_type == "method":
            score += 10

        # Higher score for documented code
        if '"""' in content or "'''" in content:
            score += 15

        # Higher score for parameterized code
        if "def " in content and "(" in content:
            param_count = content.count(",") + 1 if "(" in content else 0
            if param_count > 0:
                score += min(15, param_count * 3)

        # Lower score for hardcoded values
        if any(
            pattern in content
            for pattern in ["localhost", "127.0.0.1", "C:\\", "/tmp/"]
        ):
            score -= 10

        # Lower score for very specific implementations
        if any(
            keyword in content.lower()
            for keyword in ["specific", "hardcode", "hack", "temporary"]
        ):
            score -= 15

        return max(0, min(100, score))

    def _generate_chunk_title(
        self, original_doc: Document, chunk_metadata: dict[str, Any], chunk_index: int
    ) -> str:
        """Generate a descriptive title for the code chunk.

        Args:
            original_doc: Original document
            chunk_metadata: Chunk metadata
            chunk_index: Chunk index

        Returns:
            Generated chunk title
        """
        base_title = original_doc.title

        # Try to use element name if available
        element_name = chunk_metadata.get("element_name")
        element_type = chunk_metadata.get("element_type", "code")
        language = chunk_metadata.get("language", "unknown")

        if element_name and element_name != "unknown":
            if element_type in ["function", "method"]:
                return f"{base_title} - {element_type.title()}: {element_name}()"
            elif element_type == "class":
                return f"{base_title} - Class: {element_name}"
            else:
                return f"{base_title} - {element_type.title()}: {element_name}"

        # Fallback to generic naming
        if language != "unknown":
            return f"{base_title} - {language.title()} Code Chunk {chunk_index + 1}"
        else:
            return f"{base_title} - Code Chunk {chunk_index + 1}"

    def _is_minified_code(self, content: str) -> bool:
        """Check if code appears to be minified.

        Args:
            content: Code content

        Returns:
            True if code appears minified
        """
        lines = content.split("\n")
        if not lines:
            return False

        # Check for very long lines (typical of minified code)
        avg_line_length = sum(len(line) for line in lines) / len(lines)
        max_line_length = max(len(line) for line in lines)

        # Check ratio of meaningful characters
        meaningful_chars = sum(1 for char in content if char.isalnum() or char in "_$")
        total_chars = len(content)
        meaningful_ratio = meaningful_chars / total_chars if total_chars > 0 else 0

        return (
            avg_line_length > 200
            or max_line_length > 1000
            or meaningful_ratio < self.skip_conditions["minified_code_threshold"]
        )

    def _is_generated_code(self, content: str) -> bool:
        """Check if code appears to be auto-generated.

        Args:
            content: Code content

        Returns:
            True if code appears auto-generated
        """
        content_lower = content.lower()
        return any(
            pattern in content_lower
            for pattern in self.skip_conditions["generated_code_patterns"]
        )

    def _is_mostly_comments(self, content: str) -> bool:
        """Check if content is mostly comments.

        Args:
            content: Code content

        Returns:
            True if content is mostly comments
        """
        lines = content.split("\n")
        comment_lines = sum(
            1 for line in lines if line.strip().startswith(("#", "//", "/*", "--"))
        )
        non_empty_lines = sum(1 for line in lines if line.strip())

        return comment_lines / non_empty_lines > 0.8 if non_empty_lines > 0 else False

    def _check_indentation_consistency(self, lines: list) -> bool:
        """Check if indentation is consistent.

        Args:
            lines: List of code lines

        Returns:
            True if indentation is consistent
        """
        indentations = []
        for line in lines:
            if line.strip():  # Only check non-empty lines
                leading_spaces = len(line) - len(line.lstrip())
                if leading_spaces > 0:
                    indentations.append(leading_spaces)

        if not indentations:
            return True

        # Check if indentations follow a pattern (multiples of 2, 4, or 8)
        for base in [2, 4, 8]:
            if all(indent % base == 0 for indent in indentations):
                return True

        return False

    def _identify_programming_paradigm(self, content: str, language: str) -> str:
        """Identify the programming paradigm used.

        Args:
            content: Code content
            language: Programming language

        Returns:
            Identified paradigm
        """
        paradigms = []

        if "class " in content:
            paradigms.append("object_oriented")
        if any(keyword in content for keyword in ["def ", "function ", "func "]):
            paradigms.append("procedural")
        if any(
            keyword in content for keyword in ["lambda", "map(", "filter(", "reduce("]
        ):
            paradigms.append("functional")
        if "async" in content or "await" in content:
            paradigms.append("asynchronous")

        return paradigms[0] if paradigms else "unknown"

    def _identify_frameworks(self, content: str, language: str) -> list:
        """Identify frameworks used in the code.

        Args:
            content: Code content
            language: Programming language

        Returns:
            List of identified frameworks
        """
        frameworks = []
        content_lower = content.lower()

        # Python frameworks
        if language == "python":
            framework_indicators = {
                "django": ["django", "models.model", "request.get"],
                "flask": ["flask", "app.route", "@app."],
                "fastapi": ["fastapi", "pydantic", "async def"],
                "pandas": ["pandas", "dataframe", "pd."],
                "numpy": ["numpy", "np.", "array"],
                "tensorflow": ["tensorflow", "tf.", "keras"],
                "pytorch": ["torch", "pytorch", "tensor"],
            }
        elif language in ["javascript", "typescript"]:
            framework_indicators = {
                "react": ["react", "usestate", "component"],
                "vue": ["vue", "v-if", "v-for"],
                "angular": ["angular", "@component", "ngfor"],
                "express": ["express", "app.get", "middleware"],
                "jquery": ["jquery", "$", ".click"],
            }
        else:
            framework_indicators = {}

        for framework, indicators in framework_indicators.items():
            if any(indicator in content_lower for indicator in indicators):
                frameworks.append(framework)

        return frameworks

    def _identify_language_version(self, content: str, language: str) -> str:
        """Identify language version indicators.

        Args:
            content: Code content
            language: Programming language

        Returns:
            Version indicators
        """
        if language == "python":
            if ":=" in content:
                return "3.8+"
            elif 'f"' in content or "f'" in content:
                return "3.6+"
            elif "async def" in content:
                return "3.5+"
            elif "yield from" in content:
                return "3.3+"
        elif language == "javascript":
            if "=>" in content:
                return "ES6+"
            elif "const " in content or "let " in content:
                return "ES6+"

        return "unknown"

    def _analyze_style_conventions(self, content: str, language: str) -> dict[str, Any]:
        """Analyze coding style conventions.

        Args:
            content: Code content
            language: Programming language

        Returns:
            Style analysis
        """
        conventions = {}

        if language == "python":
            # Check naming conventions
            conventions["snake_case_functions"] = bool(
                re.search(r"def [a-z_]+\(", content)
            )
            conventions["pascal_case_classes"] = bool(
                re.search(r"class [A-Z][a-zA-Z]*", content)
            )

        elif language in ["javascript", "typescript"]:
            # Check naming conventions
            conventions["camel_case_functions"] = bool(
                re.search(r"function [a-z][a-zA-Z]*\(", content)
            )
            conventions["pascal_case_classes"] = bool(
                re.search(r"class [A-Z][a-zA-Z]*", content)
            )

        return conventions

    def _has_meaningful_names(self, content: str) -> bool:
        """Check if the code uses meaningful variable/function names.

        Args:
            content: Code content

        Returns:
            True if names appear meaningful
        """
        # Extract identifiers
        import re

        identifiers = re.findall(r"\b[a-zA-Z_][a-zA-Z0-9_]*\b", content)

        # Filter out keywords and single character names
        meaningful_names = [
            name
            for name in identifiers
            if len(name) > 2
            and name not in ["def", "class", "for", "if", "else", "try", "except"]
        ]

        # Check for non-descriptive patterns
        non_descriptive = [
            name for name in meaningful_names if re.match(r"^[a-z]{1,2}\d*$", name)
        ]

        if not meaningful_names:
            return True  # No names to judge

        return len(non_descriptive) / len(meaningful_names) < 0.3

    def _determine_learning_level(
        self, content: str, chunk_metadata: dict[str, Any]
    ) -> str:
        """Determine the learning level of the code.

        Args:
            content: Code content
            chunk_metadata: Chunk metadata

        Returns:
            Learning level (beginner, intermediate, advanced)
        """
        complexity = chunk_metadata.get("complexity", 0)
        element_type = chunk_metadata.get("element_type", "unknown")

        # Advanced indicators
        advanced_patterns = [
            "metaclass",
            "decorator",
            "generator",
            "async",
            "threading",
            "multiprocessing",
        ]
        if any(pattern in content.lower() for pattern in advanced_patterns):
            return "advanced"

        # Intermediate indicators
        if complexity > 5 or element_type in ["class", "interface"]:
            return "intermediate"

        # Simple function or straightforward code
        if complexity <= 3 and len(content.split("\n")) < 20:
            return "beginner"

        return "intermediate"

    def _identify_programming_concepts(self, content: str) -> list:
        """Identify programming concepts demonstrated in the code.

        Args:
            content: Code content

        Returns:
            List of programming concepts
        """
        concepts = []
        content_lower = content.lower()

        # Basic concepts
        if "if " in content_lower:
            concepts.append("conditionals")
        if "for " in content_lower or "while " in content_lower:
            concepts.append("loops")
        if "def " in content_lower or "function " in content_lower:
            concepts.append("functions")
        if "class " in content_lower:
            concepts.append("classes")

        # Advanced concepts
        if "try:" in content_lower or "except:" in content_lower:
            concepts.append("exception_handling")
        if "async" in content_lower:
            concepts.append("asynchronous_programming")
        if "yield" in content_lower:
            concepts.append("generators")
        if "@" in content:
            concepts.append("decorators")
        if "lambda" in content_lower:
            concepts.append("lambda_functions")

        return concepts

    def _extract_element_context(
        self, content: str, element_type: str
    ) -> dict[str, Any]:
        """Extract context specific to the code element type.

        Args:
            content: Code content
            element_type: Type of code element

        Returns:
            Element-specific context
        """
        context = {"element_type": element_type}

        if element_type in ["function", "method"]:
            context.update(
                {
                    "parameter_count": content.count(",") + 1 if "(" in content else 0,
                    "has_return_statement": "return " in content,
                    "has_docstring": '"""' in content or "'''" in content,
                    "is_recursive": content.count(self._extract_function_name(content))
                    > 1,
                }
            )
        elif element_type == "class":
            context.update(
                {
                    "method_count": content.count("def "),
                    "has_constructor": "__init__" in content
                    or "constructor" in content,
                    "has_inheritance": "extends" in content
                    or "(" in content.split("class")[1].split(":")[0],
                    "has_docstring": '"""' in content or "'''" in content,
                }
            )

        return context

    def _extract_function_name(self, content: str) -> str:
        """Extract function name from content.

        Args:
            content: Code content

        Returns:
            Function name or empty string
        """
        import re

        match = re.search(r"def\s+([a-zA-Z_][a-zA-Z0-9_]*)", content)
        return match.group(1) if match else ""
