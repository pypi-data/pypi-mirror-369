{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88000902",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Persian text preprocessing presents unique challenges in natural language processing (NLP), due to issues like inconsistent orthography, pseudo-spaces, complex morphology, and limited standardized tools. Handling these nuances properly is essential for building accurate and reliable language models and downstream NLP applications.\n",
    "\n",
    "**[Shekar](https://github.com/amirivojdan/shekar)** is an open-source Python library designed to simplify and enhance Persian text preprocessing. It offers a modular and efficient pipeline for a variety of tasks including normalization, punctuation and stopword removal, stemming and lemmatization, spell correction, and word embedding generation.\n",
    "\n",
    "This notebook demonstrates practical examples of how to use Shekar for preprocessing Persian text. By the end, youâ€™ll be able to integrate Shekar into your own NLP workflows with ease and clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31697924",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "To install Shekar, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dba69c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: shekar in c:\\users\\amiri\\documents\\github\\shekar\\.venv\\lib\\site-packages (0.1.16)\n",
      "Requirement already satisfied: arabic-reshaper>=3.0.0 in c:\\users\\amiri\\documents\\github\\shekar\\.venv\\lib\\site-packages (from shekar) (3.0.0)\n",
      "Requirement already satisfied: emoji>=2.14.1 in c:\\users\\amiri\\documents\\github\\shekar\\.venv\\lib\\site-packages (from shekar) (2.14.1)\n",
      "Requirement already satisfied: pillow>=11.2.1 in c:\\users\\amiri\\documents\\github\\shekar\\.venv\\lib\\site-packages (from shekar) (11.2.1)\n",
      "Requirement already satisfied: pip>=25.1.1 in c:\\users\\amiri\\documents\\github\\shekar\\.venv\\lib\\site-packages (from shekar) (25.1.1)\n",
      "Requirement already satisfied: python-bidi>=0.6.6 in c:\\users\\amiri\\documents\\github\\shekar\\.venv\\lib\\site-packages (from shekar) (0.6.6)\n",
      "Requirement already satisfied: regex>=2024.11.6 in c:\\users\\amiri\\documents\\github\\shekar\\.venv\\lib\\site-packages (from shekar) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.32.3 in c:\\users\\amiri\\documents\\github\\shekar\\.venv\\lib\\site-packages (from shekar) (2.32.3)\n",
      "Requirement already satisfied: wordcloud>=1.9.4 in c:\\users\\amiri\\documents\\github\\shekar\\.venv\\lib\\site-packages (from shekar) (1.9.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\amiri\\documents\\github\\shekar\\.venv\\lib\\site-packages (from requests>=2.32.3->shekar) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\amiri\\documents\\github\\shekar\\.venv\\lib\\site-packages (from requests>=2.32.3->shekar) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\amiri\\documents\\github\\shekar\\.venv\\lib\\site-packages (from requests>=2.32.3->shekar) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\amiri\\documents\\github\\shekar\\.venv\\lib\\site-packages (from requests>=2.32.3->shekar) (2024.12.14)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\amiri\\documents\\github\\shekar\\.venv\\lib\\site-packages (from wordcloud>=1.9.4->shekar) (1.24.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\amiri\\documents\\github\\shekar\\.venv\\lib\\site-packages (from wordcloud>=1.9.4->shekar) (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\amiri\\documents\\github\\shekar\\.venv\\lib\\site-packages (from matplotlib->wordcloud>=1.9.4->shekar) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\amiri\\documents\\github\\shekar\\.venv\\lib\\site-packages (from matplotlib->wordcloud>=1.9.4->shekar) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\amiri\\documents\\github\\shekar\\.venv\\lib\\site-packages (from matplotlib->wordcloud>=1.9.4->shekar) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\amiri\\documents\\github\\shekar\\.venv\\lib\\site-packages (from matplotlib->wordcloud>=1.9.4->shekar) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\amiri\\documents\\github\\shekar\\.venv\\lib\\site-packages (from matplotlib->wordcloud>=1.9.4->shekar) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\amiri\\documents\\github\\shekar\\.venv\\lib\\site-packages (from matplotlib->wordcloud>=1.9.4->shekar) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\amiri\\documents\\github\\shekar\\.venv\\lib\\site-packages (from matplotlib->wordcloud>=1.9.4->shekar) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\amiri\\documents\\github\\shekar\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud>=1.9.4->shekar) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install shekar -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deffd2f4",
   "metadata": {},
   "source": [
    "### Preprocessing with Shekar\n",
    "\n",
    "The `shekar.preprocessing` module provides a rich set of building blocks for cleaning, normalizing, and transforming Persian text. These classes form the foundation of text preprocessing workflows and can be used independently or combined in a `Pipeline`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3fd497",
   "metadata": {},
   "source": [
    "Here are some of the key text transformers available in the module:\n",
    "\n",
    "- **`SpacingStandardizer`**: Removes extra spaces and adjusts spacing around punctuation.\n",
    "- **`AlphabetNormalizer`**: Converts Arabic characters to standard Persian forms.\n",
    "- **`NumericNormalizer`**: Converts English and Arabic numerals into Persian digits.\n",
    "- **`PunctuationNormalizer`**: Standardizes punctuation symbols.\n",
    "- **`EmojiRemover`**: Removes emojis.\n",
    "- **`EmailMasker` / `URLMasker`**: Mask or remove emails and URLs.\n",
    "- **`DiacriticsRemover`**: Removes Persian/Arabic diacritics.\n",
    "- **`PunctuationRemover`**: Removes all punctuation characters.\n",
    "- **`RedundantCharacterRemover`**: Shrinks repeated characters like \"Ø³Ø³Ø³Ù„Ø§Ù…\".\n",
    "- **`ArabicUnicodeNormalizer`**: Converts Arabic presentation forms (e.g., ï·½) into Persian equivalents.\n",
    "- **`StopWordsRemover`**: Removes frequent Persian stopwords.\n",
    "- **`NonPersianRemover`**: Removes all non-Persian content (optionally keeps English).\n",
    "- **`HTMLTagRemover`**: Cleans HTML tags but retains content.\n",
    "- **`PunctuationSpacingStandardizer`**: Standardizes the spaces around punctuations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df924aab",
   "metadata": {},
   "source": [
    "##### Example 1: Remove Emojis and Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f22b699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø§ÛŒØ±Ø§Ù† Ø³Ø±Ø§ÛŒ Ù…Ù† Ø§Ø³Øª\n"
     ]
    }
   ],
   "source": [
    "from shekar.preprocessing import EmojiRemover, PunctuationRemover\n",
    "\n",
    "emoji_remover = EmojiRemover()\n",
    "punct_remover = PunctuationRemover()\n",
    "\n",
    "text = \"Ø§ÛŒØ±Ø§Ù† Ø³Ø±Ø§ÛŒ Ù…Ù† Ø§Ø³Øª! ğŸŒğŸ˜Š\"\n",
    "text = emoji_remover.fit_transform(text)\n",
    "text = punct_remover.fit_transform(text)\n",
    "text = text.strip()\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8892226d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before standardization: Ø´Ø±Ú©Øª Â« Ú¯ÙˆÚ¯Ù„ Â»Ø§Ø¹Ù„Ø§Ù… Ú©Ø±Ø¯ .\n",
      "after standardization: Ø´Ø±Ú©Øª Â«Ú¯ÙˆÚ¯Ù„Â» Ø§Ø¹Ù„Ø§Ù… Ú©Ø±Ø¯.\n"
     ]
    }
   ],
   "source": [
    "from shekar.preprocessing import PunctuationSpacingStandardizer\n",
    "\n",
    "punct_spacing_standardizer = PunctuationSpacingStandardizer()\n",
    "text = \"Ø´Ø±Ú©Øª Â« Ú¯ÙˆÚ¯Ù„ Â»Ø§Ø¹Ù„Ø§Ù… Ú©Ø±Ø¯ .\"\n",
    "print(\"before standardization:\", text)\n",
    "text = punct_spacing_standardizer.fit_transform(text).strip()\n",
    "print(\"after standardization:\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f0ff6c",
   "metadata": {},
   "source": [
    "In Shekar, all preprocessing transformers implement both the **fit_transform()** method and the **__call__()** method. This allows you to use them like functions. Calling a transformer directly is the same as calling .fit_transform().\n",
    "\n",
    "So we could rewrite the previous cell as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f31122a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø§ÛŒØ±Ø§Ù† Ø³Ø±Ø§ÛŒ Ù…Ù† Ø§Ø³Øª\n"
     ]
    }
   ],
   "source": [
    "from shekar.preprocessing import EmojiRemover, PunctuationRemover\n",
    "\n",
    "emoji_remover = EmojiRemover()\n",
    "punct_remover = PunctuationRemover()\n",
    "\n",
    "text = \"Ø§ÛŒØ±Ø§Ù† Ø³Ø±Ø§ÛŒ Ù…Ù† Ø§Ø³Øª! ğŸŒğŸ˜Š\"\n",
    "text = punct_remover(emoji_remover(text)).strip()\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03a87ef",
   "metadata": {},
   "source": [
    "This version is more concise and produces the exact same output!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae7bdef",
   "metadata": {},
   "source": [
    "##### Example 2: Normalize Persian Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c037e00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ù†Ø´Ø§Ù†â€ŒØ¯Ù‡Ù†Ø¯Ù‡ Ù‚Ø§Ø¦Ø¯Ù‡\n"
     ]
    }
   ],
   "source": [
    "from shekar.preprocessing import AlphabetNormalizer\n",
    "alphabet_normalizer = AlphabetNormalizer()\n",
    "text = \"Ù†Ø´Ø§Ù†â€ŒØ¯Ù‡Ù†Ø¯Ø© Ù‚Ø§Ø¦Ø¯Ø©\"\n",
    "normalized = alphabet_normalizer(text)\n",
    "print(normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e3be24",
   "metadata": {},
   "source": [
    "##### Example 3: Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "258d4ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø¬Ù…Ù„Ù‡Ù” Ù†Ù…ÙˆÙ†Ù‡\n"
     ]
    }
   ],
   "source": [
    "from shekar.preprocessing import StopWordRemover\n",
    "stopword_remover = StopWordRemover()\n",
    "text = \"Ø§ÛŒÙ† ÛŒÚ© Ø¬Ù…Ù„Ù‡Ù” Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø³Øª\"\n",
    "cleaned = stopword_remover(text)\n",
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284e6758",
   "metadata": {},
   "source": [
    "#### Creating Custom Transformers\n",
    "\n",
    "In Shekar, you can easily define your own text transformation logic by subclassing `BaseTextTransformer`. This allows you to integrate any custom rule-based or pattern-based transformation into the Shekar pipeline system.\n",
    "\n",
    "All you need to do is implement the `_function(self, text: str) -> str` method, which takes a string and returns the transformed version.\n",
    "\n",
    "Note that the _function() method is automatically invoked by the class when you call the transformer. In most cases, defining this method is sufficient. However, if you need more control over the transformation logic (such as managing state, performing setup, or handling input types differently), you can also override the __init__(), fit(), transform(), and fit_transform() methods directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6151bd",
   "metadata": {},
   "source": [
    "##### Example: WhitespaceStripper\n",
    "\n",
    "This custom transformer removes leading and trailing whitespace from input strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7061ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shekar.base import BaseTextTransform\n",
    "\n",
    "class WhitespaceStripper(BaseTextTransform):\n",
    "    def _function(self, text: str) -> str:\n",
    "        return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377af5af",
   "metadata": {},
   "source": [
    "You can now use it like any other Shekar component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f326d970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø³Ù„Ø§Ù… Ø¯Ù†ÛŒØ§!\n"
     ]
    }
   ],
   "source": [
    "text = \"   Ø³Ù„Ø§Ù… Ø¯Ù†ÛŒØ§!   \"\n",
    "whitespace_stripper = WhitespaceStripper()\n",
    "\n",
    "print(whitespace_stripper(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1686724",
   "metadata": {},
   "source": [
    "#### Pipelines: Chaining Text Transformations\n",
    "\n",
    "Shekar's `Pipeline` class allows you to chain multiple text preprocessing steps together into a seamless and reusable workflow. Inspired by Unix-style piping, Shekar also supports the `|` operator for combining transformers, making your code not only more readable but also expressive and modular.\n",
    "\n",
    "##### Why Pipelines?\n",
    "\n",
    "Text preprocessing often involves applying several transformations in sequence. Instead of writing nested function calls or multiple intermediate steps, Shekarâ€™s `Pipeline` lets you define a clean and testable chain of operations.\n",
    "\n",
    "For example, instead of writing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d331371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø§ÛŒØ±Ø§Ù† Ø³Ø±Ø§ÛŒ Ù…Ù† Ø§Ø³Øª\n"
     ]
    }
   ],
   "source": [
    "text = \"Ø§ÛŒØ±Ø§Ù† Ø³Ø±Ø§ÛŒ Ù…Ù† Ø§Ø³Øª! ğŸŒğŸ˜Š\"\n",
    "text = whitespace_stripper(punct_remover(emoji_remover(text)))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ec85a5",
   "metadata": {},
   "source": [
    "The same sequence of transformations can be constructed using the | operator, creating a concise and expressive pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f64b3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø§ÛŒØ±Ø§Ù† Ø³Ø±Ø§ÛŒ Ù…Ù† Ø§Ø³Øª\n"
     ]
    }
   ],
   "source": [
    "text = \"Ø§ÛŒØ±Ø§Ù† Ø³Ø±Ø§ÛŒ Ù…Ù† Ø§Ø³Øª! ğŸŒğŸ˜Š\"\n",
    "pipeline = EmojiRemover() | PunctuationRemover() | WhitespaceStripper()\n",
    "output = pipeline(text)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec89b9d",
   "metadata": {},
   "source": [
    "This approach clearly shows the order of transformations: first remove emojis, then punctuation, and finally trim whitespace. It reads naturally and makes the preprocessing flow easy to understand at a glance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb42367",
   "metadata": {},
   "source": [
    "The same transformation chain can also be written explicitly using the Pipeline class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fe9eeb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø§ÛŒØ±Ø§Ù† Ø³Ø±Ø§ÛŒ Ù…Ù† Ø§Ø³Øª\n"
     ]
    }
   ],
   "source": [
    "from shekar import Pipeline\n",
    "from shekar.preprocessing import EmojiRemover, PunctuationRemover\n",
    " \n",
    "pipeline = Pipeline([\n",
    "    (\"emoji\", EmojiRemover()),\n",
    "    (\"punct\", PunctuationRemover()),\n",
    "    (\"strip\", WhitespaceStripper())\n",
    "])\n",
    "\n",
    "text = \"Ø§ÛŒØ±Ø§Ù† Ø³Ø±Ø§ÛŒ Ù…Ù† Ø§Ø³Øª! ğŸŒğŸ˜Š\"\n",
    "output = pipeline(text)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667d127b",
   "metadata": {},
   "source": [
    "##### Batch Processing with Pipelines\n",
    "\n",
    "Note that Pipelines also support batch processing. You can pass a list (or any iterable) of strings to the pipeline, and it will apply the transformations to each item in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9cf63fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Pipeline.fit_transform.<locals>.generator at 0x0000024160587610>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\"Ø¯Ø±ÙˆØ¯! ğŸŒŸ\", \"Ú†Ø·ÙˆØ±ÛŒØŸ! ğŸ˜„\"]\n",
    "cleaned_texts = pipeline(texts)\n",
    "cleaned_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbf16ae",
   "metadata": {},
   "source": [
    "Keep in mind that the result is a generator, not a list. This makes the pipeline more memory-efficient, especially when processing large datasets. You can convert the output to a list if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d926d4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ø¯Ø±ÙˆØ¯', 'Ú†Ø·ÙˆØ±ÛŒ']\n"
     ]
    }
   ],
   "source": [
    "print(list(cleaned_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0204e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"Ø¯Ø±ÙˆØ¯! ğŸŒŸ\", \"Ú†Ø·ÙˆØ±ÛŒØŸ! ğŸ˜„\"]\n",
    "cleaned_texts = pipeline(texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b627a83",
   "metadata": {},
   "source": [
    "##### Using Pipelines as Decorators\n",
    "You can apply a pipeline to specific arguments in a function using the `.on_args()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "620708e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø¹Ù„ÛŒ Ø§Ø­Ù…Ø¯ÛŒ\n"
     ]
    }
   ],
   "source": [
    "@pipeline.on_args([\"first_name\", \"last_name\"])\n",
    "def process(first_name: str, last_name: str) -> str:\n",
    "    return f\"{first_name} {last_name}\"\n",
    "\n",
    "processed_texts = process(first_name=\"ğŸŒŸØ¹Ù„ÛŒ\", last_name=\"!Ø§Ø­Ù…Ø¯ÛŒ\")\n",
    "print(processed_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f3a60a",
   "metadata": {},
   "source": [
    "Summary\n",
    "\n",
    "- Pipelines let you chain transformations cleanly.\n",
    "- You can build them explicitly or using the `|` operator.\n",
    "- Pipelines support strings, lists, and even decorators.\n",
    "- The result is more modular, testable, and elegant preprocessing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8761df8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ù‡Ø¯Ù Ù…Ø§ Ú©Ù…Ú© Ø¨Ù‡ ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ø§Ø³Øª!\n",
      "Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ø¨Ø§ Ù‡Ù… Ú©Ø§Ø± Ú©Ù†ÛŒÙ….\n"
     ]
    }
   ],
   "source": [
    "from shekar.tokenizers import SentenceTokenizer\n",
    "\n",
    "text = \"Ù‡Ø¯Ù Ù…Ø§ Ú©Ù…Ú© Ø¨Ù‡ ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ø§Ø³Øª! Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ø¨Ø§ Ù‡Ù… Ú©Ø§Ø± Ú©Ù†ÛŒÙ….\"\n",
    "sentence_tokenizer = SentenceTokenizer()\n",
    "sentences = sentence_tokenizer.tokenize(text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdc66fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object WordTokenizer._function.<locals>.<genexpr> at 0x000002E4C575E260>\n"
     ]
    }
   ],
   "source": [
    "from shekar import WordTokenizer\n",
    "\n",
    "tokenizer = WordTokenizer()\n",
    "\n",
    "text = \"Ú†Ù‡ Ø³ÛŒØ¨â€ŒÙ‡Ø§ÛŒ Ù‚Ø´Ù†Ú¯ÛŒ! Ø­ÛŒØ§Øª Ù†Ø´Ø¦Ù‡Ù” ØªÙ†Ù‡Ø§ÛŒÛŒ Ø§Ø³Øª.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a2b9174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shekar.transforms import Flatten\n",
    "flatten = Flatten()\n",
    "text = [[\"Ø³Ù„Ø§Ù…\", \"Ø¯Ù†ÛŒØ§\"], [\"Ø§ÛŒÙ†\", \"ÛŒÚ©\", \"Ø¬Ù…Ù„Ù‡\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0b8327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f765c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<generator object WordTokenizer._function.<locals>.<genexpr> at 0x000002E4C5D8C2E0>, <generator object WordTokenizer._function.<locals>.<genexpr> at 0x000002E4C5D8C510>]\n"
     ]
    }
   ],
   "source": [
    "text = \"Ù‡Ø¯Ù Ù…Ø§ Ú©Ù…Ú© Ø¨Ù‡ ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ø§Ø³Øª! Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ø¨Ø§ Ù‡Ù… Ú©Ø§Ø± Ú©Ù†ÛŒÙ….\"\n",
    "\n",
    "pipeline = SentenceTokenizer() | WordTokenizer() | Flatten()\n",
    "output = pipeline(text)\n",
    "print(list(output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
