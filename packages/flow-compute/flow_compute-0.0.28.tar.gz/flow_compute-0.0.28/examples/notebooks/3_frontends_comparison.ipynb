{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow SDK Frontends - Multiple Ways to Submit Tasks\n",
    "\n",
    "Flow SDK provides multiple interfaces (frontends) for submitting tasks, catering to different workflows and user preferences. This notebook explores all available options.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Python API** - Direct programmatic interface\n",
    "2. **YAML Configuration** - Declarative task definitions\n",
    "3. **SLURM Adapter** - For HPC users familiar with SLURM\n",
    "4. **CLI Commands** - Command-line interface\n",
    "5. **Submitit Integration** - Facebook's job submission library\n",
    "6. **When to Use Each** - Choosing the right frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import yaml\n",
    "\n",
    "from flow import Flow, TaskConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Python API - Direct Programmatic Interface\n",
    "\n",
    "The Python API is the most flexible and powerful way to use Flow SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Python API usage\n",
    "config = TaskConfig(\n",
    "    name=\"python-api-demo\",\n",
    "    instance_type=\"gpu.nvidia.t4\",\n",
    "    command=\"nvidia-smi && echo 'Task completed'\",\n",
    ")\n",
    "\n",
    "# Submit task\n",
    "with Flow() as flow:\n",
    "    task = flow.run(config, wait=True)\n",
    "    print(f\"Task {task.id} completed with status: {task.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Python API features\n",
    "# Dynamic task generation\n",
    "def create_training_task(model_name, epochs, batch_size):\n",
    "    \"\"\"Create a training task with dynamic parameters.\"\"\"\n",
    "    return TaskConfig(\n",
    "        name=f\"train-{model_name}\",\n",
    "        instance_type=\"gpu.nvidia.a10g\",\n",
    "        environment={\n",
    "            \"MODEL_NAME\": model_name,\n",
    "            \"EPOCHS\": str(epochs),\n",
    "            \"BATCH_SIZE\": str(batch_size),\n",
    "        },\n",
    "        volumes=[{\"name\": \"model-checkpoints\", \"size_gb\": 100, \"mount_path\": \"/checkpoints\"}],\n",
    "        command=\"\"\"\n",
    "            echo \"Training $MODEL_NAME for $EPOCHS epochs\"\n",
    "            echo \"Batch size: $BATCH_SIZE\"\n",
    "            # python train.py --model $MODEL_NAME --epochs $EPOCHS --batch-size $BATCH_SIZE\n",
    "        \"\"\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Submit multiple tasks programmatically\n",
    "models = [(\"resnet50\", 10, 32), (\"efficientnet\", 20, 16), (\"vit\", 15, 8)]\n",
    "\n",
    "with Flow() as flow:\n",
    "    tasks = []\n",
    "    for model, epochs, batch_size in models:\n",
    "        config = create_training_task(model, epochs, batch_size)\n",
    "        task = flow.run(config, wait=False)\n",
    "        tasks.append(task)\n",
    "        print(f\"Submitted {model} training: {task.id}\")\n",
    "\n",
    "    # Wait for all tasks\n",
    "    print(\"\\nWaiting for all tasks to complete...\")\n",
    "    for task in tasks:\n",
    "        task.wait()\n",
    "        print(f\"Task {task.id} completed with status: {task.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. YAML Configuration - Declarative Task Definitions\n",
    "\n",
    "YAML files provide a declarative way to define tasks, perfect for reproducibility and version control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a YAML configuration\n",
    "yaml_config = \"\"\"\n",
    "name: yaml-demo-task\n",
    "instance_type: gpu.nvidia.t4\n",
    "max_price_per_hour: 5.0\n",
    "\n",
    "# Environment variables\n",
    "environment:\n",
    "  CUDA_VISIBLE_DEVICES: \"0\"\n",
    "  PYTHONPATH: \"/workspace/src\"\n",
    "  LOG_LEVEL: \"INFO\"\n",
    "\n",
    "# Storage volumes\n",
    "volumes:\n",
    "  - name: data-volume\n",
    "    size_gb: 50\n",
    "    mount_path: /data\n",
    "  - name: output-volume\n",
    "    size_gb: 20\n",
    "    mount_path: /output\n",
    "\n",
    "# Open ports for services\n",
    "ports:\n",
    "  - 8888  # Jupyter\n",
    "  - 6006  # TensorBoard\n",
    "\n",
    "# Task command\n",
    "command: |\n",
    "  echo \"=== YAML Task Demo ===\"\n",
    "  echo \"Instance: $(hostname)\"\n",
    "  echo \"GPU: $(nvidia-smi --query-gpu=name --format=csv,noheader)\"\n",
    "  echo \"Environment:\"\n",
    "  env | grep -E \"^(CUDA_|PYTHON|LOG_)\" | sort\n",
    "  echo \"Volumes:\"\n",
    "  df -h | grep -E \"(Filesystem|/data|/output)\"\n",
    "\"\"\"\n",
    "\n",
    "# Save to file\n",
    "with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".yaml\", delete=False) as f:\n",
    "    f.write(yaml_config)\n",
    "    yaml_file = f.name\n",
    "\n",
    "print(f\"Created YAML config: {yaml_file}\")\n",
    "print(\"\\nContent:\")\n",
    "print(yaml_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and submit YAML configuration\n",
    "from flow.models import TaskConfig\n",
    "\n",
    "# Method 1: Load from file\n",
    "config = TaskConfig.from_yaml(yaml_file)\n",
    "print(\"Loaded configuration:\")\n",
    "print(f\"  Name: {config.name}\")\n",
    "print(f\"  Instance: {config.instance_type}\")\n",
    "print(f\"  Volumes: {len(config.volumes)}\")\n",
    "print(f\"  Ports: {config.ports}\")\n",
    "\n",
    "# Submit the task\n",
    "with Flow() as flow:\n",
    "    task = flow.run(config, wait=True)\n",
    "    print(f\"\\nTask completed: {task.status}\")\n",
    "    if task.status.value == \"completed\":\n",
    "        print(\"\\nOutput:\")\n",
    "        print(task.logs())\n",
    "\n",
    "# Clean up\n",
    "os.unlink(yaml_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced YAML with multiple task configurations\n",
    "multi_task_yaml = \"\"\"\n",
    "# Base configuration shared by all tasks\n",
    "defaults:\n",
    "  instance_type: gpu.nvidia.a10g\n",
    "  max_price_per_hour: 15.0\n",
    "  volumes:\n",
    "    - name: shared-data\n",
    "      size_gb: 100\n",
    "      mount_path: /data\n",
    "\n",
    "# Task definitions\n",
    "tasks:\n",
    "  # Data preprocessing task\n",
    "  - name: preprocess-data\n",
    "    instance_type: cpu.large  # Override default\n",
    "    command: |\n",
    "      echo \"Preprocessing data...\"\n",
    "      # python preprocess.py --input /data/raw --output /data/processed\n",
    "  \n",
    "  # Training task\n",
    "  - name: train-model\n",
    "    environment:\n",
    "      MODEL_TYPE: transformer\n",
    "      BATCH_SIZE: \"32\"\n",
    "    command: |\n",
    "      echo \"Training $MODEL_TYPE model\"\n",
    "      # python train.py --data /data/processed --model $MODEL_TYPE\n",
    "  \n",
    "  # Evaluation task\n",
    "  - name: evaluate-model\n",
    "    command: |\n",
    "      echo \"Evaluating model performance\"\n",
    "      # python evaluate.py --model /data/model.pt --test /data/test\n",
    "\"\"\"\n",
    "\n",
    "# Parse multi-task YAML\n",
    "config_dict = yaml.safe_load(multi_task_yaml)\n",
    "print(\"Multi-task configuration:\")\n",
    "print(f\"  Defaults: {list(config_dict['defaults'].keys())}\")\n",
    "print(f\"  Tasks: {[t['name'] for t in config_dict['tasks']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SLURM Adapter - For HPC Users\n",
    "\n",
    "The SLURM adapter allows users familiar with SLURM to use Flow SDK with minimal changes to their workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLURM-style job script\n",
    "slurm_script = \"\"\"\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=slurm-flow-demo\n",
    "#SBATCH --partition=gpu\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=4\n",
    "#SBATCH --gres=gpu:1\n",
    "#SBATCH --mem=32G\n",
    "#SBATCH --time=01:00:00\n",
    "#SBATCH --output=job_%j.out\n",
    "#SBATCH --error=job_%j.err\n",
    "\n",
    "# Job commands\n",
    "echo \"SLURM Job ID: $SLURM_JOB_ID\"\n",
    "echo \"Node: $SLURM_NODELIST\"\n",
    "echo \"GPUs: $CUDA_VISIBLE_DEVICES\"\n",
    "\n",
    "# Load modules (simulated)\n",
    "echo \"Loading CUDA module...\"\n",
    "# module load cuda/11.8\n",
    "\n",
    "# Run application\n",
    "nvidia-smi\n",
    "python3 -c \"import torch; print(f'PyTorch CUDA: {torch.cuda.is_available()}')\"\n",
    "\"\"\"\n",
    "\n",
    "# Convert SLURM script to Flow TaskConfig\n",
    "from flow.frontends.slurm import SlurmAdapter\n",
    "\n",
    "# Parse SLURM directives\n",
    "adapter = SlurmAdapter()\n",
    "config = adapter.parse_script(slurm_script)\n",
    "\n",
    "print(\"Converted SLURM job to Flow config:\")\n",
    "print(f\"  Name: {config.name}\")\n",
    "print(f\"  CPUs: {config.cpu_count}\")\n",
    "print(f\"  Memory: {config.memory_gb}GB\")\n",
    "print(f\"  GPUs: {config.gpu_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLURM array jobs in Flow\n",
    "array_script = \"\"\"\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=array-demo\n",
    "#SBATCH --array=0-9\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=2\n",
    "#SBATCH --mem=8G\n",
    "\n",
    "echo \"Array Task ID: $SLURM_ARRAY_TASK_ID\"\n",
    "echo \"Processing dataset chunk $SLURM_ARRAY_TASK_ID\"\n",
    "\n",
    "# Process different data chunks based on array ID\n",
    "python process_chunk.py --chunk-id $SLURM_ARRAY_TASK_ID --total-chunks 10\n",
    "\"\"\"\n",
    "\n",
    "# Convert to Flow array jobs\n",
    "print(\"Converting SLURM array job to Flow tasks:\\n\")\n",
    "\n",
    "# Create array of tasks\n",
    "base_config = TaskConfig(name=\"array-demo\", instance_type=\"cpu.medium\", cpu_count=2, memory_gb=8)\n",
    "\n",
    "# Submit array tasks\n",
    "with Flow() as flow:\n",
    "    tasks = []\n",
    "    for i in range(10):\n",
    "        # Create task with array index\n",
    "        config = base_config.model_copy()\n",
    "        config.name = f\"{base_config.name}-{i}\"\n",
    "        config.environment = {\"ARRAY_TASK_ID\": str(i)}\n",
    "        config.command = f\"\"\"\n",
    "            echo \"Array Task ID: {i}\"\n",
    "            echo \"Processing dataset chunk {i}\"\n",
    "            # python process_chunk.py --chunk-id {i} --total-chunks 10\n",
    "        \"\"\"\n",
    "\n",
    "        task = flow.run(config, wait=False)\n",
    "        tasks.append(task)\n",
    "        print(f\"Submitted array task {i}: {task.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CLI Commands - Command Line Interface\n",
    "\n",
    "Flow SDK provides a comprehensive CLI for users who prefer working from the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate CLI commands (executed via subprocess for notebook compatibility)\n",
    "import subprocess\n",
    "\n",
    "# Show available CLI commands\n",
    "result = subprocess.run([\"flow\", \"--help\"], capture_output=True, text=True)\n",
    "print(\"Flow CLI Commands:\")\n",
    "print(result.stdout[:1000])  # First 1000 chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example CLI commands (as strings for documentation)\n",
    "cli_examples = \"\"\"\n",
    "# Submit a simple task\n",
    "flow run --instance-type gpu.nvidia.t4 --command \"nvidia-smi\"\n",
    "\n",
    "# Submit from YAML file\n",
    "flow submit config.yaml\n",
    "\n",
    "# Submit with environment variables\n",
    "flow run --instance-type cpu.large \\\n",
    "  --env MODEL=bert --env EPOCHS=10 \\\n",
    "  --command \"python train.py\"\n",
    "\n",
    "# List running tasks\n",
    "flow list --status running\n",
    "\n",
    "# Get task logs\n",
    "flow logs task-123\n",
    "\n",
    "# Stream logs in real-time\n",
    "flow logs task-123 --follow\n",
    "\n",
    "# Cancel a task\n",
    "flow cancel task-123\n",
    "\n",
    "# Find available instances\n",
    "flow instances --max-price 10 --gpu-type a100\n",
    "\n",
    "# Manage volumes\n",
    "flow volume create --size 100 --name ml-data\n",
    "flow volume list\n",
    "flow volume delete ml-data\n",
    "\"\"\"\n",
    "\n",
    "print(\"Common CLI Commands:\")\n",
    "print(cli_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Submitit Integration - Facebook's Job Submission Library\n",
    "\n",
    "For users already using Submitit, Flow SDK provides a compatible adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submitit-style job submission\n",
    "# Note: This is a conceptual example\n",
    "\n",
    "submitit_example = \"\"\"\n",
    "import submitit\n",
    "from flow.frontends.submitit import FlowExecutor\n",
    "\n",
    "def train_model(learning_rate, batch_size):\n",
    "    '''Function to be executed on remote instance.'''\n",
    "    import torch\n",
    "    print(f\"Training with LR={learning_rate}, BS={batch_size}\")\n",
    "    # Training logic here\n",
    "    return {\"accuracy\": 0.95, \"loss\": 0.05}\n",
    "\n",
    "# Create Flow-backed executor\n",
    "executor = FlowExecutor(folder=\"logs\")\n",
    "executor.update_parameters(\n",
    "    instance_type=\"gpu.nvidia.a10g\",\n",
    "    timeout_min=60,\n",
    "    cpus_per_task=4,\n",
    "    gpus_per_node=1,\n",
    ")\n",
    "\n",
    "# Submit jobs\n",
    "jobs = []\n",
    "for lr in [0.001, 0.01, 0.1]:\n",
    "    for bs in [16, 32, 64]:\n",
    "        job = executor.submit(train_model, lr, bs)\n",
    "        jobs.append(job)\n",
    "\n",
    "# Wait for results\n",
    "for job in jobs:\n",
    "    result = job.result()\n",
    "    print(f\"Job {job.job_id}: {result}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"Submitit Integration Example:\")\n",
    "print(submitit_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparing Frontends - When to Use Each\n",
    "\n",
    "Let's compare the different frontends to help you choose the right one for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frontend comparison matrix\n",
    "import pandas as pd\n",
    "\n",
    "comparison_data = {\n",
    "    \"Frontend\": [\"Python API\", \"YAML\", \"SLURM\", \"CLI\", \"Submitit\"],\n",
    "    \"Best For\": [\n",
    "        \"Dynamic workflows, integration\",\n",
    "        \"Reproducible experiments\",\n",
    "        \"HPC users, batch jobs\",\n",
    "        \"Quick tasks, scripting\",\n",
    "        \"Hyperparameter sweeps\",\n",
    "    ],\n",
    "    \"Flexibility\": [\"High\", \"Medium\", \"Medium\", \"Low\", \"High\"],\n",
    "    \"Learning Curve\": [\"Medium\", \"Low\", \"Low*\", \"Low\", \"Medium\"],\n",
    "    \"Version Control\": [\"Code\", \"Excellent\", \"Good\", \"Scripts\", \"Code\"],\n",
    "    \"Dynamic Config\": [\"Yes\", \"No\", \"Limited\", \"Limited\", \"Yes\"],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"Frontend Comparison:\")\n",
    "print(df.to_string(index=False))\n",
    "print(\"\\n* Low learning curve for users already familiar with SLURM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree for choosing a frontend\n",
    "decision_guide = \"\"\"\n",
    "Choosing the Right Frontend:\n",
    "\n",
    "1. Do you need dynamic task generation?\n",
    "   YES → Python API or Submitit\n",
    "   NO  → Continue to 2\n",
    "\n",
    "2. Are you migrating from SLURM?\n",
    "   YES → SLURM Adapter\n",
    "   NO  → Continue to 3\n",
    "\n",
    "3. Do you need version-controlled configs?\n",
    "   YES → YAML\n",
    "   NO  → Continue to 4\n",
    "\n",
    "4. Are you running one-off tasks?\n",
    "   YES → CLI\n",
    "   NO  → Python API\n",
    "\n",
    "Special Cases:\n",
    "- Hyperparameter sweeps → Submitit\n",
    "- CI/CD pipelines → YAML or CLI\n",
    "- Jupyter notebooks → Python API\n",
    "- Shell scripts → CLI\n",
    "\"\"\"\n",
    "\n",
    "print(decision_guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Real-World Examples - Same Task, Different Frontends\n",
    "\n",
    "Let's implement the same ML training task using different frontends to see the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Train a model with specific parameters\n",
    "# Requirements: GPU instance, 50GB storage, environment variables\n",
    "\n",
    "task_description = \"\"\"\n",
    "Task Requirements:\n",
    "- Train a ResNet50 model\n",
    "- Use A10G GPU\n",
    "- 50GB storage for datasets\n",
    "- Set learning rate to 0.001\n",
    "- Run for 10 epochs\n",
    "\"\"\"\n",
    "\n",
    "print(task_description)\n",
    "print(\"\\nImplementations:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Python API Implementation\n",
    "print(\"1. Python API:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "python_impl = \"\"\"\n",
    "from flow import Flow, TaskConfig\n",
    "\n",
    "config = TaskConfig(\n",
    "    name=\"train-resnet50\",\n",
    "    instance_type=\"gpu.nvidia.a10g\",\n",
    "    volumes=[{\n",
    "        \"name\": \"training-data\",\n",
    "        \"size_gb\": 50,\n",
    "        \"mount_path\": \"/data\"\n",
    "    }],\n",
    "    environment={\n",
    "        \"MODEL\": \"resnet50\",\n",
    "        \"LEARNING_RATE\": \"0.001\",\n",
    "        \"EPOCHS\": \"10\"\n",
    "    },\n",
    "    command=\"python train.py --model $MODEL --lr $LEARNING_RATE --epochs $EPOCHS\"\n",
    ")\n",
    "\n",
    "with Flow() as flow:\n",
    "    task = flow.run(config)\n",
    "\"\"\"\n",
    "\n",
    "print(python_impl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. YAML Implementation\n",
    "print(\"\\n2. YAML Configuration:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "yaml_impl = \"\"\"\n",
    "# train-resnet50.yaml\n",
    "name: train-resnet50\n",
    "instance_type: gpu.nvidia.a10g\n",
    "\n",
    "volumes:\n",
    "  - name: training-data\n",
    "    size_gb: 50\n",
    "    mount_path: /data\n",
    "\n",
    "environment:\n",
    "  MODEL: resnet50\n",
    "  LEARNING_RATE: \"0.001\"\n",
    "  EPOCHS: \"10\"\n",
    "\n",
    "command: |\n",
    "  python train.py --model $MODEL --lr $LEARNING_RATE --epochs $EPOCHS\n",
    "\"\"\"\n",
    "\n",
    "print(yaml_impl)\n",
    "print(\"\\n# Submit with: flow submit train-resnet50.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. SLURM Implementation\n",
    "print(\"\\n3. SLURM Script:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "slurm_impl = \"\"\"\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=train-resnet50\n",
    "#SBATCH --partition=gpu\n",
    "#SBATCH --gres=gpu:a10g:1\n",
    "#SBATCH --mem=32G\n",
    "#SBATCH --time=02:00:00\n",
    "\n",
    "export MODEL=resnet50\n",
    "export LEARNING_RATE=0.001\n",
    "export EPOCHS=10\n",
    "\n",
    "# Note: Storage handled by Flow automatically\n",
    "python train.py --model $MODEL --lr $LEARNING_RATE --epochs $EPOCHS\n",
    "\"\"\"\n",
    "\n",
    "print(slurm_impl)\n",
    "print(\"\\n# Submit with: flow slurm submit train.sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. CLI Implementation\n",
    "print(\"\\n4. CLI Command:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "cli_impl = \"\"\"\n",
    "flow run \\\n",
    "  --name train-resnet50 \\\n",
    "  --instance-type gpu.nvidia.a10g \\\n",
    "  --volume training-data:50:/data \\\n",
    "  --env MODEL=resnet50 \\\n",
    "  --env LEARNING_RATE=0.001 \\\n",
    "  --env EPOCHS=10 \\\n",
    "  --command \"python train.py --model \\\\$MODEL --lr \\\\$LEARNING_RATE --epochs \\\\$EPOCHS\"\n",
    "\"\"\"\n",
    "\n",
    "print(cli_impl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Patterns - Combining Frontends\n",
    "\n",
    "You can combine different frontends for maximum flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid approach: YAML template + Python customization\n",
    "base_yaml = \"\"\"\n",
    "name: base-training-template\n",
    "instance_type: gpu.nvidia.a10g\n",
    "max_price_per_hour: 20.0\n",
    "\n",
    "volumes:\n",
    "  - name: shared-data\n",
    "    size_gb: 100\n",
    "    mount_path: /data\n",
    "\n",
    "environment:\n",
    "  PYTHONPATH: /workspace/src\n",
    "  CUDA_VISIBLE_DEVICES: \"0\"\n",
    "\n",
    "command: python train.py\n",
    "\"\"\"\n",
    "\n",
    "# Load base config\n",
    "base_config = yaml.safe_load(base_yaml)\n",
    "\n",
    "# Customize for different experiments\n",
    "experiments = [\n",
    "    {\"model\": \"resnet50\", \"lr\": 0.001, \"batch_size\": 32},\n",
    "    {\"model\": \"efficientnet\", \"lr\": 0.0001, \"batch_size\": 16},\n",
    "    {\"model\": \"vit\", \"lr\": 0.0005, \"batch_size\": 8},\n",
    "]\n",
    "\n",
    "print(\"Hybrid Approach - Base + Customization:\\n\")\n",
    "\n",
    "for exp in experiments:\n",
    "    # Create customized config\n",
    "    config_dict = base_config.copy()\n",
    "    config_dict[\"name\"] = f\"train-{exp['model']}\"\n",
    "    config_dict[\"environment\"].update(\n",
    "        {\n",
    "            \"MODEL\": exp[\"model\"],\n",
    "            \"LEARNING_RATE\": str(exp[\"lr\"]),\n",
    "            \"BATCH_SIZE\": str(exp[\"batch_size\"]),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Convert to TaskConfig\n",
    "    config = TaskConfig(**config_dict)\n",
    "    print(f\"Generated config for {exp['model']}:\")\n",
    "    print(f\"  LR: {exp['lr']}, BS: {exp['batch_size']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "### Quick Reference Guide\n",
    "\n",
    "| Use Case | Recommended Frontend | Example |\n",
    "|----------|---------------------|----------|\n",
    "| Quick test | CLI | `flow run --instance-type cpu.small --command \"echo hello\"` |\n",
    "| Reproducible experiments | YAML | `flow submit experiment.yaml` |\n",
    "| Dynamic workflows | Python API | `with Flow() as flow: flow.run(config)` |\n",
    "| HPC migration | SLURM | `flow slurm submit job.sh` |\n",
    "| Parameter sweeps | Submitit | `executor.submit(func, param1, param2)` |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Start Simple**: Use CLI for exploration, then move to YAML/Python\n",
    "2. **Version Control**: Store YAML configs in git for reproducibility\n",
    "3. **Modularize**: Use Python API for reusable components\n",
    "4. **Document**: Include comments in YAML and docstrings in Python\n",
    "5. **Test Locally**: Use `--dry-run` flag to preview before submitting\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Notebook 4**: Advanced Features - Catalogs, storage, multi-node\n",
    "- **Notebook 5**: Real-World Examples - Complete ML workflows\n",
    "\n",
    "Choose the frontend that best fits your workflow and start building!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}