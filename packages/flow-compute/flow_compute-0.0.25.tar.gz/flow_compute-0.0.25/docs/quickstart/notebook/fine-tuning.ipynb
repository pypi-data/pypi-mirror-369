{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow SDK Fine-tuning Notebook\n",
    "\n",
    "Fine-tune large language models using Flow SDK with parameter-efficient methods like LoRA and QLoRA.\n",
    "\n",
    "This notebook covers:\n",
    "- Quick LoRA fine-tuning\n",
    "- QLoRA for memory efficiency\n",
    "- Custom dataset preparation\n",
    "- Multi-GPU fine-tuning\n",
    "- Evaluation and deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install and configure the Flow SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Flow SDK\n",
    "!pip install flow-sdk --upgrade\n",
    "\n",
    "# Import required libraries\n",
    "import flow\n",
    "from flow import TaskConfig\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from typing import Dict, List\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Flow client\n",
    "flow_client = flow.Flow()\n",
    "\n",
    "# Check authentication\n",
    "print(\"‚úì Flow SDK initialized\")\n",
    "print(f\"API Endpoint: {flow_client.api_endpoint}\")\n",
    "\n",
    "# Set HuggingFace token if you have one\n",
    "import os\n",
    "HF_TOKEN = os.environ.get('HF_TOKEN', '')\n",
    "if HF_TOKEN:\n",
    "    print(\"‚úì HuggingFace token detected\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No HF_TOKEN found. Some models may not be accessible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Quick LoRA Fine-tuning\n",
    "\n",
    "Fine-tune Mistral-7B on a custom dataset using LoRA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample training data\n",
    "training_data = [\n",
    "    {\n",
    "        \"instruction\": \"What is machine learning?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain neural networks\",\n",
    "        \"input\": \"in simple terms\",\n",
    "        \"output\": \"Neural networks are computing systems inspired by biological brains, consisting of interconnected nodes that process information in layers.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What are the benefits of cloud computing?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Cloud computing offers scalability, cost-efficiency, accessibility from anywhere, automatic updates, and reduced IT maintenance.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save as JSON\n",
    "with open(\"/tmp/training_data.json\", \"w\") as f:\n",
    "    json.dump(training_data, f, indent=2)\n",
    "\n",
    "print(f\"‚úì Created training dataset with {len(training_data)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA fine-tuning script\n",
    "lora_script = \"\"\"\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from trl import SFTTrainer\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "output_dir = \"./mistral-lora-finetuned\"\n",
    "\n",
    "print(f\"Loading model: {model_name}\")\n",
    "\n",
    "# Load model in 4-bit for QLoRA\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    token=os.environ.get(\"HF_TOKEN\")\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    token=os.environ.get(\"HF_TOKEN\")\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=16,  # LoRA rank\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Load dataset\n",
    "with open('/workspace/training_data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Format data for training\n",
    "def format_instruction(sample):\n",
    "    instruction = sample['instruction']\n",
    "    input_text = sample.get('input', '')\n",
    "    output = sample['output']\n",
    "    \n",
    "    if input_text:\n",
    "        text = f\"### Instruction: {instruction}\\n### Input: {input_text}\\n### Response: {output}\"\n",
    "    else:\n",
    "        text = f\"### Instruction: {instruction}\\n### Response: {output}\"\n",
    "    \n",
    "    return {\"text\": text}\n",
    "\n",
    "# Create dataset\n",
    "from datasets import Dataset\n",
    "dataset = Dataset.from_list([format_instruction(item) for item in data])\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)} examples\")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=1,\n",
    "    report_to=\"none\",\n",
    "    save_total_limit=2,\n",
    "    warmup_ratio=0.1,\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save LoRA weights\n",
    "trainer.save_model(f\"{output_dir}/final\")\n",
    "print(f\"\\nLoRA weights saved to {output_dir}/final\")\n",
    "\n",
    "# Test the fine-tuned model\n",
    "print(\"\\nTesting fine-tuned model:\")\n",
    "test_prompt = \"### Instruction: What is deep learning?\\n### Response:\"\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100, temperature=0.7)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)\n",
    "\"\"\"\n",
    "\n",
    "# Save script\n",
    "with open(\"/tmp/lora_finetune.py\", \"w\") as f:\n",
    "    f.write(lora_script)\n",
    "\n",
    "print(\"‚úì LoRA fine-tuning script created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LoRA fine-tuning\n",
    "lora_config = TaskConfig(\n",
    "    name=\"mistral-lora-finetuning\",\n",
    "    command=\"\"\"\n",
    "    pip install transformers accelerate peft datasets bitsandbytes trl\n",
    "    python /workspace/lora_finetune.py\n",
    "    \"\"\",\n",
    "    instance_type=\"a100\",  # Single A100 80GB\n",
    "    upload_files={\n",
    "        \"/tmp/training_data.json\": \"training_data.json\",\n",
    "        \"/tmp/lora_finetune.py\": \"lora_finetune.py\"\n",
    "    },\n",
    "    download_patterns=[\"mistral-lora-finetuned/*\"],\n",
    "    environment={\"HF_TOKEN\": HF_TOKEN} if HF_TOKEN else {},\n",
    "    max_price_per_hour=10.00,\n",
    "    max_run_time_hours=2\n",
    ")\n",
    "\n",
    "print(\"üöÄ Starting LoRA fine-tuning...\")\n",
    "lora_task = flow_client.run(lora_config)\n",
    "print(f\"Task ID: {lora_task.task_id}\")\n",
    "print(f\"Status: {lora_task.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Custom Dataset Fine-tuning\n",
    "\n",
    "Fine-tune on a larger custom dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a larger synthetic dataset\n",
    "import random\n",
    "\n",
    "# Templates for generating training data\n",
    "templates = [\n",
    "    {\n",
    "        \"instruction\": \"Summarize the following text\",\n",
    "        \"topics\": [\"AI advancements\", \"climate change\", \"space exploration\", \"medical breakthroughs\"],\n",
    "        \"generate_output\": lambda topic: f\"This text discusses {topic}, highlighting recent developments and future implications.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Translate to a professional tone\",\n",
    "        \"inputs\": [\"hey whats up\", \"thx for ur help\", \"gonna be late sry\"],\n",
    "        \"outputs\": [\"Hello, how are you?\", \"Thank you for your assistance.\", \"I apologize, but I will be arriving late.\"]\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Generate a Python function that\",\n",
    "        \"tasks\": [\"sorts a list\", \"finds prime numbers\", \"calculates fibonacci\"],\n",
    "        \"generate_output\": lambda task: f\"def function():\\n    # Implementation for {task}\\n    pass\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Generate dataset\n",
    "large_dataset = []\n",
    "for _ in range(100):\n",
    "    template = random.choice(templates)\n",
    "    \n",
    "    if \"topics\" in template:\n",
    "        topic = random.choice(template[\"topics\"])\n",
    "        large_dataset.append({\n",
    "            \"instruction\": template[\"instruction\"],\n",
    "            \"input\": f\"Recent research shows progress in {topic}...\",\n",
    "            \"output\": template[\"generate_output\"](topic)\n",
    "        })\n",
    "    elif \"inputs\" in template:\n",
    "        idx = random.randint(0, len(template[\"inputs\"]) - 1)\n",
    "        large_dataset.append({\n",
    "            \"instruction\": template[\"instruction\"],\n",
    "            \"input\": template[\"inputs\"][idx],\n",
    "            \"output\": template[\"outputs\"][idx]\n",
    "        })\n",
    "    elif \"tasks\" in template:\n",
    "        task = random.choice(template[\"tasks\"])\n",
    "        large_dataset.append({\n",
    "            \"instruction\": template[\"instruction\"] + \" \" + task,\n",
    "            \"input\": \"\",\n",
    "            \"output\": template[\"generate_output\"](task)\n",
    "        })\n",
    "\n",
    "# Save dataset\n",
    "with open(\"/tmp/large_training_data.json\", \"w\") as f:\n",
    "    json.dump(large_dataset, f, indent=2)\n",
    "\n",
    "print(f\"‚úì Generated large dataset with {len(large_dataset)} examples\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample data:\")\n",
    "for i in range(3):\n",
    "    sample = large_dataset[i]\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Instruction: {sample['instruction']}\")\n",
    "    if sample['input']:\n",
    "        print(f\"Input: {sample['input']}\")\n",
    "    print(f\"Output: {sample['output'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced fine-tuning with validation\n",
    "advanced_script = \"\"\"\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "from trl import SFTTrainer\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "with open('/workspace/large_training_data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Split into train/validation\n",
    "train_data, val_data = train_test_split(data, test_size=0.1, random_state=42)\n",
    "print(f\"Train: {len(train_data)}, Validation: {len(val_data)}\")\n",
    "\n",
    "# Model configuration\n",
    "model_name = \"microsoft/phi-2\"  # Smaller model for faster iteration\n",
    "output_dir = \"./phi2-custom-finetuned\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Enhanced LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=32,  # Higher rank for more capacity\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"Wqkv\", \"fc1\", \"fc2\"],  # Phi-2 specific\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Format datasets\n",
    "def format_instruction(sample):\n",
    "    instruction = sample['instruction']\n",
    "    input_text = sample.get('input', '')\n",
    "    output = sample['output']\n",
    "    \n",
    "    # Use a chat-like format\n",
    "    if input_text:\n",
    "        text = f\"User: {instruction}\\nInput: {input_text}\\nAssistant: {output}\"\n",
    "    else:\n",
    "        text = f\"User: {instruction}\\nAssistant: {output}\"\n",
    "    \n",
    "    return {\"text\": text}\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = Dataset.from_list([format_instruction(item) for item in train_data])\n",
    "val_dataset = Dataset.from_list([format_instruction(item) for item in val_data])\n",
    "\n",
    "# Advanced training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,\n",
    "    logging_steps=5,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    report_to=\"none\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# Custom compute metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # Simple perplexity calculation\n",
    "    loss = np.mean(predictions)\n",
    "    perplexity = np.exp(loss)\n",
    "    return {\"perplexity\": perplexity}\n",
    "\n",
    "# Create trainer with validation\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    packing=True,  # Enable packing for efficiency\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(\"\\nStarting training with validation...\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "# Save model and metrics\n",
    "trainer.save_model(f\"{output_dir}/final\")\n",
    "trainer.save_state()\n",
    "\n",
    "# Save training metrics\n",
    "metrics = train_result.metrics\n",
    "with open(f\"{output_dir}/training_metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"Final train loss: {metrics.get('train_loss', 'N/A')}\")\n",
    "print(f\"Model saved to {output_dir}/final\")\n",
    "\n",
    "# Test the model\n",
    "print(\"\\nTesting fine-tuned model on new prompts:\")\n",
    "test_prompts = [\n",
    "    \"User: What is quantum computing?\\nAssistant:\",\n",
    "    \"User: Translate to a professional tone\\nInput: gonna grab lunch brb\\nAssistant:\",\n",
    "    \"User: Generate a Python function that reverses a string\\nAssistant:\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=50, \n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9\n",
    "        )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"\\n{response}\")\n",
    "    print(\"-\" * 50)\n",
    "\"\"\"\n",
    "\n",
    "# Save script\n",
    "with open(\"/tmp/advanced_finetune.py\", \"w\") as f:\n",
    "    f.write(advanced_script)\n",
    "\n",
    "print(\"‚úì Advanced fine-tuning script created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run advanced fine-tuning\n",
    "advanced_config = TaskConfig(\n",
    "    name=\"phi2-advanced-finetuning\",\n",
    "    command=\"\"\"\n",
    "    pip install transformers accelerate peft datasets bitsandbytes trl scikit-learn\n",
    "    python /workspace/advanced_finetune.py\n",
    "    \"\"\",\n",
    "    instance_type=\"a100\",\n",
    "    upload_files={\n",
    "        \"/tmp/large_training_data.json\": \"large_training_data.json\",\n",
    "        \"/tmp/advanced_finetune.py\": \"advanced_finetune.py\"\n",
    "    },\n",
    "    download_patterns=[\"phi2-custom-finetuned/*\"],\n",
    "    max_price_per_hour=10.00,\n",
    "    max_run_time_hours=3\n",
    ")\n",
    "\n",
    "print(\"üöÄ Starting advanced fine-tuning with validation...\")\n",
    "advanced_task = flow_client.run(advanced_config)\n",
    "print(f\"Task ID: {advanced_task.task_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-GPU Fine-tuning\n",
    "\n",
    "Scale up fine-tuning with distributed training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributed fine-tuning configuration\n",
    "distributed_config = TaskConfig(\n",
    "    name=\"distributed-llama-finetuning\",\n",
    "    command=\"\"\"\n",
    "    pip install transformers accelerate peft datasets deepspeed\n",
    "    \n",
    "    # Create DeepSpeed config\n",
    "    cat > ds_config.json << 'EOF'\n",
    "    {\n",
    "        \"fp16\": {\n",
    "            \"enabled\": true\n",
    "        },\n",
    "        \"zero_optimization\": {\n",
    "            \"stage\": 3,\n",
    "            \"offload_optimizer\": {\n",
    "                \"device\": \"cpu\",\n",
    "                \"pin_memory\": true\n",
    "            },\n",
    "            \"offload_param\": {\n",
    "                \"device\": \"cpu\",\n",
    "                \"pin_memory\": true\n",
    "            },\n",
    "            \"overlap_comm\": true,\n",
    "            \"contiguous_gradients\": true,\n",
    "            \"sub_group_size\": 1e9\n",
    "        },\n",
    "        \"gradient_accumulation_steps\": 4,\n",
    "        \"train_batch_size\": \"auto\",\n",
    "        \"train_micro_batch_size_per_gpu\": \"auto\"\n",
    "    }\n",
    "    EOF\n",
    "    \n",
    "    # Run distributed training\n",
    "    accelerate launch \\\n",
    "        --config_file accelerate_config.yaml \\\n",
    "        --num_processes 4 \\\n",
    "        --num_machines 1 \\\n",
    "        --mixed_precision fp16 \\\n",
    "        --deepspeed_config_file ds_config.json \\\n",
    "        finetune_distributed.py\n",
    "    \"\"\",\n",
    "    instance_type=\"4xa100\",  # 4x A100 for distributed training\n",
    "    environment={\"HF_TOKEN\": HF_TOKEN} if HF_TOKEN else {},\n",
    "    max_price_per_hour=40.00,\n",
    "    max_run_time_hours=6\n",
    ")\n",
    "\n",
    "print(\"üöÄ Distributed fine-tuning configuration created\")\n",
    "print(\"This would fine-tune LLaMA-13B across 4x A100 GPUs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine-tuning Pipeline\n",
    "\n",
    "Complete pipeline with data preprocessing, training, and evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete fine-tuning pipeline\n",
    "pipeline_script = \"\"\"\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "class FineTuningPipeline:\n",
    "    def __init__(self, model_name, dataset_name, output_dir):\n",
    "        self.model_name = model_name\n",
    "        self.dataset_name = dataset_name\n",
    "        self.output_dir = output_dir\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        \"\"\"Load and preprocess dataset.\"\"\"\n",
    "        print(\"Loading dataset...\")\n",
    "        \n",
    "        # Load from HuggingFace or local file\n",
    "        if os.path.exists(self.dataset_name):\n",
    "            with open(self.dataset_name, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            dataset = Dataset.from_list(data)\n",
    "        else:\n",
    "            dataset = load_dataset(self.dataset_name, split=\"train\")\n",
    "        \n",
    "        # Split dataset\n",
    "        split = dataset.train_test_split(test_size=0.1)\n",
    "        self.train_dataset = split[\"train\"]\n",
    "        self.eval_dataset = split[\"test\"]\n",
    "        \n",
    "        print(f\"Train: {len(self.train_dataset)}, Eval: {len(self.eval_dataset)}\")\n",
    "        \n",
    "    def setup_model(self):\n",
    "        \"\"\"Initialize model with LoRA.\"\"\"\n",
    "        print(f\"Loading model: {self.model_name}\")\n",
    "        \n",
    "        # Model setup\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            load_in_8bit=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # LoRA configuration\n",
    "        lora_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"q_proj\", \"v_proj\"]\n",
    "        )\n",
    "        \n",
    "        self.model = get_peft_model(self.model, lora_config)\n",
    "        self.model.print_trainable_parameters()\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"Fine-tune the model.\"\"\"\n",
    "        from transformers import TrainingArguments\n",
    "        from trl import SFTTrainer\n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            per_device_train_batch_size=4,\n",
    "            gradient_accumulation_steps=4,\n",
    "            num_train_epochs=3,\n",
    "            learning_rate=2e-4,\n",
    "            fp16=True,\n",
    "            save_strategy=\"epoch\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            logging_steps=10,\n",
    "            report_to=\"none\",\n",
    "            load_best_model_at_end=True,\n",
    "        )\n",
    "        \n",
    "        trainer = SFTTrainer(\n",
    "            model=self.model,\n",
    "            train_dataset=self.train_dataset,\n",
    "            eval_dataset=self.eval_dataset,\n",
    "            tokenizer=self.tokenizer,\n",
    "            args=training_args,\n",
    "            dataset_text_field=\"text\",\n",
    "            max_seq_length=512,\n",
    "        )\n",
    "        \n",
    "        print(\"Starting training...\")\n",
    "        trainer.train()\n",
    "        \n",
    "        # Save model\n",
    "        trainer.save_model(f\"{self.output_dir}/final\")\n",
    "        \n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate the fine-tuned model.\"\"\"\n",
    "        print(\"\\nEvaluating model...\")\n",
    "        \n",
    "        # Load evaluation metrics\n",
    "        perplexity = evaluate.load(\"perplexity\")\n",
    "        \n",
    "        # Test on evaluation set\n",
    "        test_texts = self.eval_dataset[\"text\"][:10]\n",
    "        \n",
    "        results = []\n",
    "        for text in test_texts:\n",
    "            inputs = self.tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs, labels=inputs[\"input_ids\"])\n",
    "                loss = outputs.loss.item()\n",
    "                \n",
    "            results.append({\"text\": text[:100], \"loss\": loss})\n",
    "        \n",
    "        avg_loss = np.mean([r[\"loss\"] for r in results])\n",
    "        print(f\"Average loss: {avg_loss:.4f}\")\n",
    "        print(f\"Perplexity: {np.exp(avg_loss):.2f}\")\n",
    "        \n",
    "        # Save evaluation results\n",
    "        with open(f\"{self.output_dir}/evaluation_results.json\", \"w\") as f:\n",
    "            json.dump({\n",
    "                \"avg_loss\": avg_loss,\n",
    "                \"perplexity\": np.exp(avg_loss),\n",
    "                \"sample_results\": results[:5]\n",
    "            }, f, indent=2)\n",
    "        \n",
    "    def run(self):\n",
    "        \"\"\"Run complete pipeline.\"\"\"\n",
    "        self.prepare_data()\n",
    "        self.setup_model()\n",
    "        self.train()\n",
    "        self.evaluate()\n",
    "        print(f\"\\nPipeline complete! Model saved to {self.output_dir}\")\n",
    "\n",
    "# Run pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    pipeline = FineTuningPipeline(\n",
    "        model_name=\"gpt2\",  # Using smaller model for demo\n",
    "        dataset_name=\"imdb\",  # or path to local JSON\n",
    "        output_dir=\"./pipeline-output\"\n",
    "    )\n",
    "    pipeline.run()\n",
    "\"\"\"\n",
    "\n",
    "# Save pipeline script\n",
    "with open(\"/tmp/finetune_pipeline.py\", \"w\") as f:\n",
    "    f.write(pipeline_script)\n",
    "\n",
    "print(\"‚úì Complete fine-tuning pipeline created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison & Evaluation\n",
    "\n",
    "Compare different fine-tuning approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter-efficient fine-tuning comparison\n",
    "methods = {\n",
    "    \"LoRA\": {\n",
    "        \"trainable_params\": \"0.1-1%\",\n",
    "        \"memory\": \"Low\",\n",
    "        \"performance\": \"90-95% of full fine-tuning\",\n",
    "        \"training_time\": \"Fast\",\n",
    "        \"use_case\": \"Most common, general purpose\"\n",
    "    },\n",
    "    \"QLoRA\": {\n",
    "        \"trainable_params\": \"0.1-1%\",\n",
    "        \"memory\": \"Very Low (4-bit)\",\n",
    "        \"performance\": \"85-90% of full fine-tuning\",\n",
    "        \"training_time\": \"Fast\",\n",
    "        \"use_case\": \"Large models on limited GPU\"\n",
    "    },\n",
    "    \"Prefix Tuning\": {\n",
    "        \"trainable_params\": \"0.01-0.1%\",\n",
    "        \"memory\": \"Very Low\",\n",
    "        \"performance\": \"80-90% of full fine-tuning\",\n",
    "        \"training_time\": \"Very Fast\",\n",
    "        \"use_case\": \"Task-specific adaptations\"\n",
    "    },\n",
    "    \"Full Fine-tuning\": {\n",
    "        \"trainable_params\": \"100%\",\n",
    "        \"memory\": \"Very High\",\n",
    "        \"performance\": \"100% (baseline)\",\n",
    "        \"training_time\": \"Slow\",\n",
    "        \"use_case\": \"When maximum performance needed\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame(methods).T\n",
    "print(\"üîç Fine-tuning Methods Comparison\\n\")\n",
    "print(comparison_df.to_string())\n",
    "\n",
    "# Visualize comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Memory usage comparison\n",
    "memory_scores = {\"LoRA\": 2, \"QLoRA\": 1, \"Prefix Tuning\": 1, \"Full Fine-tuning\": 5}\n",
    "ax1.bar(memory_scores.keys(), memory_scores.values(), color=['green', 'darkgreen', 'darkgreen', 'red'])\n",
    "ax1.set_title(\"Relative Memory Usage\")\n",
    "ax1.set_ylabel(\"Memory (relative)\")\n",
    "\n",
    "# Training time comparison\n",
    "time_scores = {\"LoRA\": 2, \"QLoRA\": 2, \"Prefix Tuning\": 1, \"Full Fine-tuning\": 5}\n",
    "ax2.bar(time_scores.keys(), time_scores.values(), color=['green', 'green', 'darkgreen', 'red'])\n",
    "ax2.set_title(\"Relative Training Time\")\n",
    "ax2.set_ylabel(\"Time (relative)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deploy Fine-tuned Model\n",
    "\n",
    "Deploy your fine-tuned model for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deployment script for fine-tuned model\n",
    "deploy_script = \"\"\"\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import torch\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Load fine-tuned model\n",
    "base_model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "adapter_path = \"/workspace/model/mistral-lora-finetuned/final\"\n",
    "\n",
    "print(\"Loading base model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"Loading LoRA adapter...\")\n",
    "model = PeftModel.from_pretrained(model, adapter_path)\n",
    "model = model.merge_and_unload()  # Merge LoRA weights\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "class GenerateRequest(BaseModel):\n",
    "    prompt: str\n",
    "    max_tokens: int = 100\n",
    "    temperature: float = 0.7\n",
    "\n",
    "@app.post(\"/generate\")\n",
    "async def generate(request: GenerateRequest):\n",
    "    # Format prompt\n",
    "    formatted_prompt = f\"### Instruction: {request.prompt}\\n### Response:\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=request.max_tokens,\n",
    "            temperature=request.temperature,\n",
    "            do_sample=True,\n",
    "            top_p=0.9\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = response.split(\"### Response:\")[-1].strip()\n",
    "    \n",
    "    return {\"response\": response}\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    return {\"status\": \"healthy\", \"model\": \"fine-tuned-mistral\"}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\"\"\"\n",
    "\n",
    "# Deployment configuration\n",
    "deploy_config = TaskConfig(\n",
    "    name=\"deploy-finetuned-model\",\n",
    "    command=\"\"\"\n",
    "    # Assume model artifacts are uploaded\n",
    "    pip install transformers peft fastapi uvicorn accelerate\n",
    "    python serve_finetuned.py\n",
    "    \"\"\",\n",
    "    instance_type=\"a100\",\n",
    "    ports=[8000],\n",
    "    # upload_files would include model artifacts\n",
    "    max_price_per_hour=10.00,\n",
    "    max_run_time_hours=24\n",
    ")\n",
    "\n",
    "print(\"‚úì Deployment configuration created\")\n",
    "print(\"\\nTo deploy your fine-tuned model:\")\n",
    "print(\"1. Download model artifacts from fine-tuning task\")\n",
    "print(\"2. Upload with deployment task\")\n",
    "print(\"3. Access at http://<endpoint>:8000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cost Analysis\n",
    "\n",
    "Analyze fine-tuning costs and optimization strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning cost estimates\n",
    "cost_estimates = [\n",
    "    {\n",
    "        \"model\": \"Llama-2-7B\",\n",
    "        \"method\": \"LoRA\",\n",
    "        \"dataset_size\": \"1K samples\",\n",
    "        \"instance\": \"a100\",\n",
    "        \"time_hours\": 1,\n",
    "        \"notes\": \"Quick experimentation\"\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"Llama-2-7B\",\n",
    "        \"method\": \"LoRA\",\n",
    "        \"dataset_size\": \"10K samples\",\n",
    "        \"instance\": \"a100\",\n",
    "        \"time_hours\": 3,\n",
    "        \"notes\": \"Production fine-tuning\"\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"Llama-2-13B\",\n",
    "        \"method\": \"QLoRA\",\n",
    "        \"dataset_size\": \"10K samples\",\n",
    "        \"instance\": \"a100\",\n",
    "        \"time_hours\": 5,\n",
    "        \"notes\": \"Larger model with QLoRA\"\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"Llama-2-70B\",\n",
    "        \"method\": \"LoRA\",\n",
    "        \"dataset_size\": \"1K samples\",\n",
    "        \"instance\": \"4xa100\",\n",
    "        \"time_hours\": 2,\n",
    "        \"notes\": \"Multi-GPU required\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Create cost analysis table\n",
    "cost_df = pd.DataFrame(cost_estimates)\n",
    "\n",
    "print(\"üí∞ Fine-tuning Time Estimates\\n\")\n",
    "print(cost_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nüìä Cost Optimization Tips:\")\n",
    "print(\"1. Use QLoRA for models > 7B parameters\")\n",
    "print(\"2. Start with small dataset samples for validation\")\n",
    "print(\"3. Use gradient checkpointing to reduce memory\")\n",
    "print(\"4. Enable model sharding for very large models\")\n",
    "print(\"5. Monitor GPU utilization and adjust batch size\")\n",
    "print(\"\\nüí° Remember: Mithril uses dynamic pricing. Check current rates with 'flow instances'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned how to:\n",
    "\n",
    "1. **Fine-tune with LoRA** - Quick and efficient adaptation\n",
    "2. **Use QLoRA** - Memory-efficient fine-tuning for large models\n",
    "3. **Prepare custom datasets** - Format data for instruction tuning\n",
    "4. **Scale with multi-GPU** - Distributed fine-tuning strategies\n",
    "5. **Deploy fine-tuned models** - Serve your custom models\n",
    "6. **Optimize costs** - Choose the right approach for your budget\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- LoRA typically achieves 90-95% of full fine-tuning performance\n",
    "- QLoRA enables fine-tuning 70B models on a single A100\n",
    "- Start with small datasets to validate your approach\n",
    "- Always use validation sets to prevent overfitting\n",
    "- Monitor training metrics to catch issues early\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try fine-tuning on your own dataset\n",
    "- Experiment with different LoRA ranks (r=8, 16, 32, 64)\n",
    "- Explore advanced techniques like DPO and RLHF\n",
    "- Deploy your model with the [Inference Notebook](inference.ipynb)\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [PEFT Documentation](https://huggingface.co/docs/peft)\n",
    "- [TRL Documentation](https://huggingface.co/docs/trl)\n",
    "- [Flow SDK Examples](../../examples/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}