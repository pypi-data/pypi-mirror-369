{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow SDK Getting Started Notebook\n",
    "\n",
    "Interactive introduction to GPU computing with Flow SDK.\n",
    "\n",
    "## Prerequisites\n",
    "- Flow SDK installed: `pip install flow-compute`\n",
    "- API key from https://app.mithril.ai/account/apikeys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Flow SDK if needed\n",
    "!pip install flow-sdk --quiet\n",
    "\n",
    "# Import required libraries\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import flow\n",
    "from flow import TaskConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up authentication\n",
    "# Option 1: Set API key directly (not recommended for shared notebooks)\n",
    "# os.environ['FLOW_API_KEY'] = 'your-api-key'\n",
    "\n",
    "# Option 2: Load from secure location\n",
    "# with open(os.path.expanduser('~/.flow/credentials'), 'r') as f:\n",
    "#     os.environ['FLOW_API_KEY'] = f.read().strip()\n",
    "\n",
    "# Initialize Flow client\n",
    "flow_client = flow.Flow()\n",
    "print(\"‚úÖ Flow SDK initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GPU Validation\n",
    "\n",
    "Let's verify GPU access and explore available instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick GPU test\n",
    "validation_config = TaskConfig(\n",
    "    name=\"notebook-gpu-test\",\n",
    "    command=\"\"\"\n",
    "    nvidia-smi -L && echo \"---\" &&\n",
    "    python -c \"\n",
    "import torch\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
    "\"\n",
    "    \"\"\",\n",
    "    instance_type=\"h100-80gb.sxm.8x\",\n",
    "    region=\"us-central1-b\",\n",
    "    max_run_time_seconds=60,\n",
    "    max_price_per_hour=10.00,\n",
    ")\n",
    "\n",
    "print(\"üöÄ Running GPU validation...\")\n",
    "task = flow_client.run(validation_config, wait=True)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nüìã GPU Information:\")\n",
    "print(task.logs())\n",
    "print(f\"\\nüí∞ Test cost: ${task.total_cost:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available Mithril instance types\n",
    "instance_types = [\n",
    "    {\"name\": \"a100\", \"vram\": \"80GB\", \"gpus\": 1, \"use_case\": \"Training, fine-tuning, inference\"},\n",
    "    {\n",
    "        \"name\": \"2xa100\",\n",
    "        \"vram\": \"160GB\",\n",
    "        \"gpus\": 2,\n",
    "        \"use_case\": \"Larger models, distributed training\",\n",
    "    },\n",
    "    {\"name\": \"4xa100\", \"vram\": \"320GB\", \"gpus\": 4, \"use_case\": \"Large-scale distributed training\"},\n",
    "    {\"name\": \"8xa100\", \"vram\": \"640GB\", \"gpus\": 8, \"use_case\": \"Massive models and workloads\"},\n",
    "    {\"name\": \"h100\", \"vram\": \"640GB\", \"gpus\": 8, \"use_case\": \"Cutting-edge performance (8√ó H100)\"},\n",
    "]\n",
    "\n",
    "# Display as formatted table\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(instance_types)\n",
    "print(\"üí° Note: Mithril uses dynamic auction-based pricing.\")\n",
    "print(\"   Use 'flow instances' to see current spot prices.\\n\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Interactive GPU Selection\n",
    "\n",
    "Choose the right GPU for your workload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive instance selector\n",
    "from ipywidgets import Checkbox, Dropdown, FloatSlider, interact\n",
    "\n",
    "\n",
    "def recommend_instance(workload_type, model_size_gb, max_budget):\n",
    "    \"\"\"Recommend best instance for workload\"\"\"\n",
    "\n",
    "    # Calculate memory needs\n",
    "    memory_multiplier = {\"inference\": 1.2, \"training\": 3.0, \"fine-tuning\": 2.5}\n",
    "\n",
    "    memory_needed = model_size_gb * memory_multiplier[workload_type]\n",
    "\n",
    "    # Filter suitable instances\n",
    "    suitable = []\n",
    "    for inst in instance_types:\n",
    "        vram = float(inst[\"vram\"].replace(\"GB\", \"\"))\n",
    "\n",
    "        if vram >= memory_needed:\n",
    "            suitable.append({**inst, \"memory_headroom\": vram - memory_needed})\n",
    "\n",
    "    if suitable:\n",
    "        # Sort by least waste (smallest instance that fits)\n",
    "        suitable.sort(key=lambda x: x[\"memory_headroom\"])\n",
    "        best = suitable[0]\n",
    "        print(f\"üí° Recommended: {best['name']}\")\n",
    "        print(f\"   VRAM: {best['vram']} ({best['gpus']} GPU{'s' if best['gpus'] > 1 else ''})\")\n",
    "        print(f\"   Memory headroom: {best['memory_headroom']:.1f}GB\")\n",
    "        print(f\"   Use case: {best['use_case']}\")\n",
    "        print(\"\\nüìå Set your max price with --max-price to control costs\")\n",
    "    else:\n",
    "        print(\"‚ùå No suitable instances found. Try:\")\n",
    "        print(\"   - Using model quantization (8-bit or 4-bit)\")\n",
    "        print(\"   - Gradient checkpointing for training\")\n",
    "        print(\"   - Smaller batch sizes\")\n",
    "\n",
    "\n",
    "interact(\n",
    "    recommend_instance,\n",
    "    workload_type=Dropdown(options=[\"inference\", \"training\", \"fine-tuning\"], value=\"inference\"),\n",
    "    model_size_gb=FloatSlider(min=1, max=200, step=1, value=7, description=\"Model Size (GB)\"),\n",
    "    max_budget=FloatSlider(min=10, max=100, step=5, value=30, description=\"Max $/hour\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Running Your First GPU Task\n",
    "\n",
    "Let's run a simple matrix multiplication benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication benchmark\n",
    "benchmark_script = \"\"\"\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# GPU warmup\n",
    "device = torch.device(\"cuda\")\n",
    "_ = torch.randn(100, 100).to(device) @ torch.randn(100, 100).to(device)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Benchmark different sizes\n",
    "sizes = [1000, 2000, 4000, 8000]\n",
    "results = []\n",
    "\n",
    "for size in sizes:\n",
    "    A = torch.randn(size, size).to(device)\n",
    "    B = torch.randn(size, size).to(device)\n",
    "    \n",
    "    # Time the operation\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    \n",
    "    C = A @ B\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    # Calculate TFLOPS\n",
    "    flops = 2 * size**3  # Matrix multiplication FLOPs\n",
    "    tflops = flops / elapsed / 1e12\n",
    "    \n",
    "    results.append({\n",
    "        \"size\": size,\n",
    "        \"time\": elapsed,\n",
    "        \"tflops\": tflops\n",
    "    })\n",
    "    \n",
    "    print(f\"Size {size}x{size}: {elapsed:.3f}s ({tflops:.1f} TFLOPS)\")\n",
    "\n",
    "# Save results\n",
    "import json\n",
    "with open(\"/tmp/benchmark_results.json\", \"w\") as f:\n",
    "    json.dump(results, f)\n",
    "\"\"\"\n",
    "\n",
    "# Run benchmark\n",
    "benchmark_config = TaskConfig(\n",
    "    name=\"gpu-benchmark\",\n",
    "    command=f\"python -c '{benchmark_script}'\",\n",
    "    instance_type=\"h100-80gb.sxm.8x\",\n",
    "    region=\"us-central1-b\",\n",
    "    max_run_time_minutes=5,\n",
    "    max_price_per_hour=20.00,\n",
    "    output_artifacts=[\"/tmp/benchmark_results.json\"],\n",
    ")\n",
    "\n",
    "print(\"üèÉ Running GPU benchmark...\")\n",
    "benchmark_task = flow_client.run(benchmark_config, wait=True)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nüìä Benchmark Results:\")\n",
    "print(benchmark_task.logs())\n",
    "print(f\"\\nüí∞ Benchmark cost: ${benchmark_task.total_cost:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and visualize results\n",
    "if benchmark_task.status == \"completed\":\n",
    "    # Download results file\n",
    "    results_path = benchmark_task.download_artifact(\n",
    "        \"/tmp/benchmark_results.json\", \"./benchmark_results.json\"\n",
    "    )\n",
    "\n",
    "    # Load and visualize\n",
    "    with open(results_path) as f:\n",
    "        results = json.load(f)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    sizes = [r[\"size\"] for r in results]\n",
    "    tflops = [r[\"tflops\"] for r in results]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(sizes, tflops, color=\"#2E86AB\")\n",
    "    plt.xlabel(\"Matrix Size\")\n",
    "    plt.ylabel(\"Performance (TFLOPS)\")\n",
    "    plt.title(\"GPU Matrix Multiplication Performance\")\n",
    "    plt.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for i, (s, t) in enumerate(zip(sizes, tflops, strict=False)):\n",
    "        plt.text(s, t + 0.5, f\"{t:.1f}\", ha=\"center\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cost Estimation Tool\n",
    "\n",
    "Estimate costs for your workloads before running them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive cost calculator\n",
    "def calculate_costs(instance_type, hours_per_day, days, use_spot):\n",
    "    \"\"\"Calculate and display costs\"\"\"\n",
    "\n",
    "    # Find instance pricing\n",
    "    instance = next((i for i in instance_types if i[\"name\"] == instance_type), None)\n",
    "    if not instance:\n",
    "        print(\"Instance type not found\")\n",
    "        return\n",
    "\n",
    "    hourly_rate = instance[\"spot_price\"] if use_spot else instance[\"price\"]\n",
    "\n",
    "    # Calculate costs\n",
    "    daily_cost = hourly_rate * hours_per_day\n",
    "    total_cost = daily_cost * days\n",
    "    monthly_cost = hourly_rate * 24 * 30  # If running 24/7\n",
    "\n",
    "    # Display results\n",
    "    print(f\"üíª Instance: {instance_type} ({instance['vram']} VRAM)\")\n",
    "    print(f\"üíµ Hourly rate: ${hourly_rate:.2f}\")\n",
    "    print(f\"üìÖ Daily cost: ${daily_cost:.2f} ({hours_per_day}h/day)\")\n",
    "    print(f\"üìä Total cost: ${total_cost:.2f} ({days} days)\")\n",
    "    print(f\"üìÜ Monthly 24/7: ${monthly_cost:.2f}\")\n",
    "\n",
    "    if use_spot:\n",
    "        savings = (instance[\"price\"] - hourly_rate) * hours_per_day * days\n",
    "        print(f\"\\n‚ú® Spot savings: ${savings:.2f} (70% off)\")\n",
    "\n",
    "    # ROI calculation\n",
    "    print(\"\\nüìà ROI Considerations:\")\n",
    "    print(f\"   - Inference: ~${hourly_rate / 100:.4f} per 1K requests\")\n",
    "    print(f\"   - Training: ~${total_cost / 10:.2f} per epoch (estimated)\")\n",
    "\n",
    "\n",
    "# Create interactive widget\n",
    "from ipywidgets import interact\n",
    "\n",
    "interact(\n",
    "    calculate_costs,\n",
    "    instance_type=Dropdown(\n",
    "        options=[i[\"name\"] for i in instance_types], value=\"l40s\", description=\"Instance:\"\n",
    "    ),\n",
    "    hours_per_day=FloatSlider(min=1, max=24, step=1, value=8, description=\"Hours/day:\"),\n",
    "    days=FloatSlider(min=1, max=30, step=1, value=7, description=\"Days:\"),\n",
    "    use_spot=Checkbox(value=True, description=\"Use spot instances\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Task Management\n",
    "\n",
    "Monitor and manage your running tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List recent tasks\n",
    "def list_my_tasks(limit=5):\n",
    "    \"\"\"List recent tasks with details\"\"\"\n",
    "    tasks = flow_client.list_tasks(limit=limit)\n",
    "\n",
    "    task_data = []\n",
    "    for task in tasks:\n",
    "        task_data.append(\n",
    "            {\n",
    "                \"ID\": task.task_id[:8],\n",
    "                \"Name\": task.name,\n",
    "                \"Status\": task.status,\n",
    "                \"Instance\": task.instance_type,\n",
    "                \"Runtime\": f\"{task.runtime_seconds / 60:.1f}m\" if task.runtime_seconds else \"N/A\",\n",
    "                \"Cost\": f\"${task.total_cost:.3f}\" if task.total_cost else \"N/A\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df = pd.DataFrame(task_data)\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"üìã Recent Tasks:\")\n",
    "list_my_tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task monitoring function\n",
    "def monitor_task(task_id):\n",
    "    \"\"\"Monitor a running task\"\"\"\n",
    "    import time\n",
    "\n",
    "    from IPython.display import clear_output\n",
    "\n",
    "    while True:\n",
    "        task = flow_client.get_task(task_id)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        print(f\"üîÑ Task: {task.name} ({task.task_id[:8]})\")\n",
    "        print(f\"üìä Status: {task.status}\")\n",
    "        print(f\"‚è±Ô∏è  Runtime: {task.runtime_seconds / 60:.1f} minutes\")\n",
    "        print(f\"üí∞ Cost: ${task.total_cost:.4f}\")\n",
    "\n",
    "        if task.status in [\"completed\", \"failed\", \"cancelled\"]:\n",
    "            print(f\"\\n‚úÖ Task {task.status}!\")\n",
    "            break\n",
    "\n",
    "        time.sleep(5)\n",
    "\n",
    "\n",
    "# Example: monitor the last task\n",
    "# monitor_task(benchmark_task.task_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Best Practices Summary\n",
    "\n",
    "Key recommendations for using Flow SDK effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best practices checklist\n",
    "best_practices = [\n",
    "    {\n",
    "        \"category\": \"Cost Management\",\n",
    "        \"practices\": [\n",
    "            \"Always set max_price_per_hour\",\n",
    "            \"Use spot instances for fault-tolerant workloads\",\n",
    "            \"Set max_total_cost for budget control\",\n",
    "            \"Monitor costs with flow_client.get_spending()\",\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"Performance\",\n",
    "        \"practices\": [\n",
    "            \"Choose instance types based on VRAM needs\",\n",
    "            \"Use mixed precision (fp16) for 2x speedup\",\n",
    "            \"Enable gradient checkpointing for memory savings\",\n",
    "            \"Batch operations for better GPU utilization\",\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"Reliability\",\n",
    "        \"practices\": [\n",
    "            \"Implement checkpointing for long runs\",\n",
    "            \"Use retry policies for spot instances\",\n",
    "            \"Save outputs as artifacts\",\n",
    "            \"Set appropriate timeouts\",\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"Development\",\n",
    "        \"practices\": [\n",
    "            \"Test on smaller instances first\",\n",
    "            \"Use interactive notebooks for experimentation\",\n",
    "            \"Version control your TaskConfigs\",\n",
    "            \"Log metrics for analysis\",\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "for bp in best_practices:\n",
    "    print(f\"\\nüìå {bp['category']}:\")\n",
    "    for practice in bp[\"practices\"]:\n",
    "        print(f\"   ‚úì {practice}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Next Steps\n",
    "\n",
    "Ready to do more? Check out these resources:\n",
    "\n",
    "1. **[Inference Notebook](./inference.ipynb)** - Deploy model servers\n",
    "2. **[Training Notebook](./training.ipynb)** - Train models from scratch\n",
    "3. **[Fine-tuning Notebook](./fine-tuning.ipynb)** - Adapt pre-trained models\n",
    "4. **[Flow SDK Docs](https://docs.flow.ai)** - Complete documentation\n",
    "\n",
    "### Quick Links\n",
    "- [GPU Instance Types](../index.md#gpu-instances)\n",
    "- [Cost Estimates](../_shared/cost-estimates.md)\n",
    "- [API Reference](https://docs.flow.ai/api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your session info\n",
    "session_info = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"flow_version\": flow.__version__,\n",
    "    \"total_tasks_run\": len(list_my_tasks(100)),\n",
    "    \"estimated_total_cost\": sum(\n",
    "        [t.get(\"total_cost\", 0) for t in flow_client.list_tasks(limit=100)]\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(\"üìä Session Summary:\")\n",
    "print(json.dumps(session_info, indent=2))\n",
    "\n",
    "# Save for future reference\n",
    "with open(\"flow_session.json\", \"w\") as f:\n",
    "    json.dump(session_info, f, indent=2)\n",
    "\n",
    "print(\"\\n‚úÖ Session info saved to flow_session.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
