{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow SDK Inference Notebook\n",
    "\n",
    "Deploy and serve large language models using Flow SDK with vLLM for high-performance inference.\n",
    "\n",
    "This notebook covers:\n",
    "- Quick model deployment\n",
    "- OpenAI-compatible API serving\n",
    "- Batch inference processing\n",
    "- Performance optimization\n",
    "- Cost monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install and configure the Flow SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Flow SDK\n",
    "!pip install flow-sdk --upgrade\n",
    "\n",
    "# Import required libraries\n",
    "import flow\n",
    "from flow import TaskConfig\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Flow client\n",
    "flow_client = flow.Flow()\n",
    "\n",
    "# Check authentication\n",
    "print(\"‚úì Flow SDK initialized\")\n",
    "print(f\"API Endpoint: {flow_client.api_endpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Quick Model Deployment\n",
    "\n",
    "Deploy a model with vLLM in under 2 minutes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy Mistral-7B with vLLM\n",
    "inference_config = TaskConfig(\n",
    "    name=\"mistral-inference-server\",\n",
    "    command=\"\"\"\n",
    "    pip install vllm\n",
    "    \n",
    "    python -m vllm.entrypoints.openai.api_server \\\n",
    "        --model mistralai/Mistral-7B-Instruct-v0.1 \\\n",
    "        --host 0.0.0.0 \\\n",
    "        --port 8000 \\\n",
    "        --max-model-len 8192\n",
    "    \"\"\",\n",
    "    instance_type=\"a100\",  # Single A100 80GB\n",
    "    ports=[8000],\n",
    "    max_price_per_hour=10.00,  # Set your budget\n",
    "    max_run_time_hours=24  # 24 hour deployment\n",
    ")\n",
    "\n",
    "# Launch the server\n",
    "print(\"üöÄ Launching inference server...\")\n",
    "inference_task = flow_client.run(inference_config)\n",
    "print(f\"Task ID: {inference_task.task_id}\")\n",
    "print(f\"Status: {inference_task.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for server to be ready\n",
    "print(\"‚è≥ Waiting for server to start...\")\n",
    "\n",
    "while True:\n",
    "    task_info = flow_client.get_task(inference_task.task_id)\n",
    "    if task_info.status == \"running\":\n",
    "        print(\"‚úì Server is running!\")\n",
    "        break\n",
    "    elif task_info.status in [\"failed\", \"cancelled\"]:\n",
    "        print(f\"‚ùå Task {task_info.status}\")\n",
    "        print(task_info.logs())\n",
    "        break\n",
    "    time.sleep(10)\n",
    "\n",
    "# Get endpoint information\n",
    "if task_info.status == \"running\":\n",
    "    endpoint = task_info.endpoints[0]\n",
    "    print(f\"\\nüåê API Endpoint: {endpoint}\")\n",
    "    print(f\"üí∞ Current cost: ${task_info.total_cost:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test the Inference Server\n",
    "\n",
    "Let's test our deployed model with some sample requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a helper function for API calls\n",
    "def query_model(prompt: str, max_tokens: int = 100, temperature: float = 0.7):\n",
    "    \"\"\"Query the deployed model.\"\"\"\n",
    "    response = requests.post(\n",
    "        f\"{endpoint}/v1/completions\",\n",
    "        json={\n",
    "            \"model\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "    )\n",
    "    return response.json()\n",
    "\n",
    "# Test the model\n",
    "test_prompt = \"Explain quantum computing in simple terms:\"\n",
    "result = query_model(test_prompt, max_tokens=200)\n",
    "\n",
    "print(\"ü§ñ Model Response:\")\n",
    "print(\"-\" * 50)\n",
    "print(result['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Batch Inference\n",
    "\n",
    "Process multiple prompts efficiently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batch inference configuration\n",
    "batch_config = TaskConfig(\n",
    "    name=\"batch-inference-job\",\n",
    "    command=\"\"\"\n",
    "    pip install vllm pandas tqdm\n",
    "    \n",
    "    python - << 'EOF'\n",
    "import json\n",
    "import pandas as pd\n",
    "from vllm import LLM, SamplingParams\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Sample prompts for batch processing\n",
    "prompts = [\n",
    "    \"What are the benefits of renewable energy?\",\n",
    "    \"Explain machine learning to a 10-year-old.\",\n",
    "    \"What is the future of space exploration?\",\n",
    "    \"How does cryptocurrency work?\",\n",
    "    \"What are best practices for remote work?\"\n",
    "]\n",
    "\n",
    "# Initialize model\n",
    "print(\"Loading model...\")\n",
    "llm = LLM(\"mistralai/Mistral-7B-Instruct-v0.1\", max_model_len=8192)\n",
    "\n",
    "# Set generation parameters\n",
    "params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "# Run batch inference\n",
    "print(f\"Processing {len(prompts)} prompts...\")\n",
    "outputs = llm.generate(prompts, params)\n",
    "\n",
    "# Save results\n",
    "results = []\n",
    "for prompt, output in zip(prompts, outputs):\n",
    "    results.append({\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": output.outputs[0].text,\n",
    "        \"tokens\": len(output.outputs[0].token_ids)\n",
    "    })\n",
    "\n",
    "# Save to file\n",
    "with open(\"batch_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nProcessed {len(results)} prompts\")\n",
    "print(f\"Results saved to batch_results.json\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample result:\")\n",
    "print(f\"Prompt: {results[0]['prompt']}\")\n",
    "print(f\"Response: {results[0]['response'][:200]}...\")\n",
    "EOF\n",
    "    \"\"\",\n",
    "    instance_type=\"a100\",\n",
    "    max_price_per_hour=10.00,\n",
    "    max_run_time_hours=1,\n",
    "    download_patterns=[\"batch_results.json\"]\n",
    ")\n",
    "\n",
    "# Run batch job\n",
    "print(\"üöÄ Starting batch inference job...\")\n",
    "batch_task = flow_client.run(batch_config, wait=True)\n",
    "print(f\"\\n‚úì Batch job completed!\")\n",
    "print(f\"Total cost: ${batch_task.total_cost:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and display results\n",
    "results_path = flow_client.download(batch_task.task_id, \"batch_results.json\")\n",
    "\n",
    "with open(results_path, 'r') as f:\n",
    "    batch_results = json.load(f)\n",
    "\n",
    "# Display results in a nice format\n",
    "for i, result in enumerate(batch_results[:3]):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Prompt {i+1}: {result['prompt']}\")\n",
    "    print(f\"\\nResponse: {result['response'][:300]}...\")\n",
    "    print(f\"\\nTokens generated: {result['tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Model Serving\n",
    "\n",
    "Deploy multiple models with a load balancer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a multi-model server script\n",
    "multi_model_script = \"\"\"\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from vllm import LLM, SamplingParams\n",
    "import uvicorn\n",
    "import torch\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Model registry\n",
    "models = {\n",
    "    \"small\": {\n",
    "        \"name\": \"microsoft/phi-2\",\n",
    "        \"max_len\": 2048,\n",
    "        \"description\": \"Fast 2.7B parameter model\"\n",
    "    },\n",
    "    \"medium\": {\n",
    "        \"name\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "        \"max_len\": 8192,\n",
    "        \"description\": \"Balanced 7B parameter model\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Load models\n",
    "loaded_models = {}\n",
    "for size, config in models.items():\n",
    "    print(f\"Loading {size} model: {config['name']}\")\n",
    "    loaded_models[size] = LLM(config['name'], max_model_len=config['max_len'])\n",
    "\n",
    "class GenerateRequest(BaseModel):\n",
    "    prompt: str\n",
    "    model_size: str = \"medium\"\n",
    "    max_tokens: int = 100\n",
    "    temperature: float = 0.7\n",
    "\n",
    "@app.get(\"/models\")\n",
    "async def list_models():\n",
    "    return models\n",
    "\n",
    "@app.post(\"/generate\")\n",
    "async def generate(request: GenerateRequest):\n",
    "    if request.model_size not in loaded_models:\n",
    "        raise HTTPException(400, f\"Model size {request.model_size} not available\")\n",
    "    \n",
    "    # Generate response\n",
    "    params = SamplingParams(\n",
    "        temperature=request.temperature,\n",
    "        max_tokens=request.max_tokens\n",
    "    )\n",
    "    \n",
    "    outputs = loaded_models[request.model_size].generate([request.prompt], params)\n",
    "    \n",
    "    return {\n",
    "        \"model\": models[request.model_size][\"name\"],\n",
    "        \"response\": outputs[0].outputs[0].text,\n",
    "        \"tokens\": len(outputs[0].outputs[0].token_ids)\n",
    "    }\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    return {\"status\": \"healthy\", \"models_loaded\": list(loaded_models.keys())}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\"\"\"\n",
    "\n",
    "# Save the script\n",
    "with open(\"/tmp/multi_model_server.py\", \"w\") as f:\n",
    "    f.write(multi_model_script)\n",
    "\n",
    "print(\"‚úì Multi-model server script created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy multi-model server\n",
    "multi_model_config = TaskConfig(\n",
    "    name=\"multi-model-server\",\n",
    "    command=\"\"\"\n",
    "    pip install vllm fastapi uvicorn\n",
    "    python /workspace/multi_model_server.py\n",
    "    \"\"\",\n",
    "    instance_type=\"2xa100\",  # 2x A100 for multiple models\n",
    "    ports=[8000],\n",
    "    upload_files={\"/tmp/multi_model_server.py\": \"multi_model_server.py\"},\n",
    "    max_price_per_hour=20.00,\n",
    "    max_run_time_hours=12\n",
    ")\n",
    "\n",
    "print(\"üöÄ Deploying multi-model server...\")\n",
    "multi_task = flow_client.run(multi_model_config)\n",
    "print(f\"Task ID: {multi_task.task_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Optimization\n",
    "\n",
    "Optimize inference for better throughput and lower latency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark different configurations\n",
    "benchmark_config = TaskConfig(\n",
    "    name=\"inference-benchmark\",\n",
    "    command=\"\"\"\n",
    "    pip install vllm pandas matplotlib\n",
    "    \n",
    "    python - << 'EOF'\n",
    "import time\n",
    "import json\n",
    "from vllm import LLM, SamplingParams\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Test configurations\n",
    "configs = [\n",
    "    {\"tensor_parallel_size\": 1, \"max_num_seqs\": 256},\n",
    "    {\"tensor_parallel_size\": 1, \"max_num_seqs\": 512},\n",
    "    {\"tensor_parallel_size\": 1, \"max_num_seqs\": 1024},\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for config in configs:\n",
    "    print(f\"\\nTesting config: {config}\")\n",
    "    \n",
    "    # Initialize model with config\n",
    "    llm = LLM(\n",
    "        \"mistralai/Mistral-7B-v0.1\",\n",
    "        tensor_parallel_size=config[\"tensor_parallel_size\"],\n",
    "        max_num_seqs=config[\"max_num_seqs\"]\n",
    "    )\n",
    "    \n",
    "    # Create test prompts\n",
    "    test_prompts = [\"Generate a random sentence.\" for _ in range(100)]\n",
    "    \n",
    "    # Benchmark\n",
    "    start_time = time.time()\n",
    "    outputs = llm.generate(test_prompts, SamplingParams(max_tokens=50))\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    total_time = end_time - start_time\n",
    "    throughput = len(test_prompts) / total_time\n",
    "    avg_latency = total_time / len(test_prompts)\n",
    "    \n",
    "    result = {\n",
    "        \"config\": config,\n",
    "        \"throughput_req_per_sec\": throughput,\n",
    "        \"avg_latency_sec\": avg_latency,\n",
    "        \"total_time\": total_time\n",
    "    }\n",
    "    results.append(result)\n",
    "    \n",
    "    print(f\"Throughput: {throughput:.2f} req/s\")\n",
    "    print(f\"Avg latency: {avg_latency:.3f} s\")\n",
    "\n",
    "# Save results\n",
    "with open(\"benchmark_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "max_seqs = [r[\"config\"][\"max_num_seqs\"] for r in results]\n",
    "throughputs = [r[\"throughput_req_per_sec\"] for r in results]\n",
    "\n",
    "plt.bar(range(len(max_seqs)), throughputs)\n",
    "plt.xlabel(\"Max Sequences Configuration\")\n",
    "plt.ylabel(\"Throughput (req/s)\")\n",
    "plt.title(\"vLLM Throughput vs Configuration\")\n",
    "plt.xticks(range(len(max_seqs)), [str(s) for s in max_seqs])\n",
    "plt.savefig(\"benchmark_plot.png\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nBenchmark complete! Results saved.\")\n",
    "EOF\n",
    "    \"\"\",\n",
    "    instance_type=\"a100\",\n",
    "    max_price_per_hour=10.00,\n",
    "    max_run_time_hours=1,\n",
    "    download_patterns=[\"benchmark_results.json\", \"benchmark_plot.png\"]\n",
    ")\n",
    "\n",
    "print(\"‚ö° Running performance benchmark...\")\n",
    "benchmark_task = flow_client.run(benchmark_config, wait=True)\n",
    "print(\"‚úì Benchmark complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cost Monitoring & Optimization\n",
    "\n",
    "Monitor and optimize inference costs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current running tasks\n",
    "running_tasks = flow_client.list_tasks(status=\"running\")\n",
    "\n",
    "print(\"üí∞ Current Running Inference Tasks:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Task Name':<30} {'Instance':<15} {'Cost/Hour':<10} {'Total':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "total_hourly = 0\n",
    "for task in running_tasks:\n",
    "    if \"inference\" in task.name or \"server\" in task.name:\n",
    "        hourly_cost = getattr(task, 'cost_per_hour', 0)\n",
    "        total_cost = getattr(task, 'total_cost', 0)\n",
    "        total_hourly += hourly_cost\n",
    "        \n",
    "        print(f\"{task.name:<30} {task.instance_type:<15} ${hourly_cost:<9.2f} ${total_cost:<9.2f}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Total Hourly Cost:':<30} {'':<15} ${total_hourly:<9.2f}\")\n",
    "print(f\"\\nüìä Daily projection: ${total_hourly * 24:.2f}\")\n",
    "print(f\"üìä Monthly projection: ${total_hourly * 24 * 30:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost optimization recommendations\n",
    "def get_instance_recommendation(model_size_gb: float, concurrent_requests: int) -> str:\n",
    "    \"\"\"Recommend optimal instance type based on workload.\"\"\"\n",
    "    \n",
    "    # Model memory requirement (with overhead)\n",
    "    memory_needed = model_size_gb * 2.5\n",
    "    \n",
    "    if memory_needed <= 80 and concurrent_requests <= 50:\n",
    "        return \"a100\"  # Single GPU\n",
    "    elif memory_needed <= 160 or concurrent_requests <= 100:\n",
    "        return \"2xa100\"\n",
    "    elif memory_needed <= 320 or concurrent_requests <= 200:\n",
    "        return \"4xa100\"\n",
    "    else:\n",
    "        return \"8xa100\"\n",
    "\n",
    "# Example recommendations\n",
    "workloads = [\n",
    "    {\"model\": \"Llama-2-7B\", \"size_gb\": 13, \"requests\": 20},\n",
    "    {\"model\": \"Llama-2-13B\", \"size_gb\": 26, \"requests\": 50},\n",
    "    {\"model\": \"Llama-2-70B\", \"size_gb\": 140, \"requests\": 10},\n",
    "    {\"model\": \"Mixtral-8x7B\", \"size_gb\": 90, \"requests\": 30},\n",
    "]\n",
    "\n",
    "print(\"üéØ Instance Recommendations:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Model':<20} {'Size':<10} {'Requests/s':<15} {'Recommended':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for w in workloads:\n",
    "    rec = get_instance_recommendation(w['size_gb'], w['requests'])\n",
    "    print(f\"{w['model']:<20} {w['size_gb']:>3} GB      {w['requests']:<15} {rec:<15}\")\n",
    "\n",
    "print(\"\\nüí° Note: Mithril uses dynamic pricing. Use 'flow instances' to check current rates.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Production Deployment Pattern\n",
    "\n",
    "Complete production deployment with monitoring and auto-scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production deployment configuration\n",
    "production_config = TaskConfig(\n",
    "    name=\"production-inference-server\",\n",
    "    command=\"\"\"\n",
    "    # Install dependencies\n",
    "    pip install vllm prometheus-client\n",
    "    \n",
    "    # Create monitoring wrapper\n",
    "    cat > server_with_monitoring.py << 'SCRIPT'\n",
    "import os\n",
    "from prometheus_client import start_http_server, Counter, Histogram, Gauge\n",
    "import time\n",
    "import threading\n",
    "\n",
    "# Metrics\n",
    "request_count = Counter('inference_requests_total', 'Total inference requests')\n",
    "request_latency = Histogram('inference_request_latency_seconds', 'Request latency')\n",
    "active_requests = Gauge('inference_active_requests', 'Active requests')\n",
    "model_loaded = Gauge('model_loaded', 'Model load status')\n",
    "\n",
    "# Start metrics server\n",
    "start_http_server(9090)\n",
    "print(\"Metrics server started on port 9090\")\n",
    "\n",
    "# Start vLLM\n",
    "model_loaded.set(1)\n",
    "os.system(\"\"\"\n",
    "python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model mistralai/Mistral-7B-Instruct-v0.1 \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 8000 \\\n",
    "    --max-model-len 8192 \\\n",
    "    --gpu-memory-utilization 0.9\n",
    "\"\"\")\n",
    "SCRIPT\n",
    "    \n",
    "    # Run with monitoring\n",
    "    python server_with_monitoring.py\n",
    "    \"\"\",\n",
    "    instance_type=\"a100\",\n",
    "    ports=[8000, 9090],  # API and metrics ports\n",
    "    max_price_per_hour=15.00,\n",
    "    max_run_time_hours=168,  # 1 week\n",
    "    \n",
    "    # Auto-restart on failure\n",
    "    retry_on_failure=True,\n",
    "    max_retries=3,\n",
    "    \n",
    "    # Environment variables\n",
    "    environment={\n",
    "        \"VLLM_ATTENTION_BACKEND\": \"FLASHINFER\",\n",
    "        \"CUDA_VISIBLE_DEVICES\": \"0\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"üöÄ Deploying production inference server...\")\n",
    "prod_task = flow_client.run(production_config)\n",
    "print(f\"Production Task ID: {prod_task.task_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cleanup\n",
    "\n",
    "Don't forget to stop your inference servers when done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all running inference tasks\n",
    "inference_tasks = [\n",
    "    task for task in flow_client.list_tasks(status=\"running\") \n",
    "    if \"inference\" in task.name.lower() or \"server\" in task.name.lower()\n",
    "]\n",
    "\n",
    "print(f\"Found {len(inference_tasks)} running inference tasks:\")\n",
    "for task in inference_tasks:\n",
    "    print(f\"  - {task.name} (ID: {task.task_id})\")\n",
    "\n",
    "# Uncomment to stop all inference tasks\n",
    "# for task in inference_tasks:\n",
    "#     print(f\"Stopping {task.name}...\")\n",
    "#     flow_client.cancel(task.task_id)\n",
    "#     print(\"‚úì Stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned how to:\n",
    "\n",
    "1. **Deploy models quickly** with vLLM\n",
    "2. **Run batch inference** for processing large datasets\n",
    "3. **Serve multiple models** from a single endpoint\n",
    "4. **Optimize performance** with benchmarking\n",
    "5. **Monitor costs** and get instance recommendations\n",
    "6. **Deploy production-ready** inference servers\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Explore the [Training Notebook](training.ipynb) for model training\n",
    "- Check out [Fine-tuning Notebook](fine-tuning.ipynb) for customizing models\n",
    "- Read the [Production Guide](../../guides/production-inference.md) for scaling\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Start with single GPU (`a100`) and scale up as needed\n",
    "- Always set budget limits with `max_price_per_hour`\n",
    "- Use vLLM for best inference performance\n",
    "- Monitor costs and optimize instance selection\n",
    "- Mithril uses dynamic pricing - check current rates with `flow instances`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}