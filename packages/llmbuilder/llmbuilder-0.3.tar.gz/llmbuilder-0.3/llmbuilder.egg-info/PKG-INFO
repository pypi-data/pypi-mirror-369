Metadata-Version: 2.4
Name: llmbuilder
Version: 0.3
Summary: A comprehensive toolkit for building, training, and deploying language models
Home-page: https://github.com/Qubasehq/llmbuilder-package
Author: Qub‚ñ≥se
Author-email: Qubase Team <team@qubase.in>
Maintainer-email: Qubase Team <team@qubase.in>
License: Apache-2.0
Project-URL: Homepage, https://github.com/Qubasehq/llmbuilder-package
Project-URL: Documentation, https://qubasehq.github.io/llmbuilder-package/
Project-URL: Repository, https://github.com/Qubasehq/llmbuilder-package.git
Project-URL: Issues, https://github.com/Qubasehq/llmbuilder-package/issues
Project-URL: Changelog, https://github.com/Qubasehq/llmbuilder-package/blob/main/CHANGELOG.md
Keywords: machine learning,deep learning,natural language processing,transformers,language models,training,inference,tokenization,data processing
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: torch>=1.12.0
Requires-Dist: numpy>=1.21.0
Requires-Dist: click>=8.0.0
Requires-Dist: pyyaml>=6.0
Requires-Dist: tqdm>=4.64.0
Requires-Dist: requests>=2.28.0
Requires-Dist: packaging>=21.0
Provides-Extra: pdf
Requires-Dist: pymupdf>=1.23.0; extra == "pdf"
Requires-Dist: pytesseract>=0.3.10; extra == "pdf"
Requires-Dist: pillow>=9.0.0; extra == "pdf"
Provides-Extra: epub
Requires-Dist: ebooklib>=0.18; extra == "epub"
Provides-Extra: html
Requires-Dist: beautifulsoup4>=4.11.0; extra == "html"
Requires-Dist: lxml>=4.9.0; extra == "html"
Requires-Dist: html5lib>=1.1; extra == "html"
Provides-Extra: semantic
Requires-Dist: sentence-transformers>=2.2.0; extra == "semantic"
Requires-Dist: scikit-learn>=1.1.0; extra == "semantic"
Requires-Dist: faiss-cpu>=1.7.0; extra == "semantic"
Provides-Extra: conversion
Requires-Dist: gguf>=0.1.0; extra == "conversion"
Provides-Extra: all
Requires-Dist: llmbuilder[conversion,epub,html,pdf,semantic]; extra == "all"
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0.0; extra == "dev"
Requires-Dist: pytest-xdist>=3.0.0; extra == "dev"
Requires-Dist: pytest-mock>=3.10.0; extra == "dev"
Requires-Dist: black>=23.0.0; extra == "dev"
Requires-Dist: isort>=5.12.0; extra == "dev"
Requires-Dist: flake8>=6.0.0; extra == "dev"
Requires-Dist: flake8-docstrings>=1.7.0; extra == "dev"
Requires-Dist: flake8-bugbear>=23.0.0; extra == "dev"
Requires-Dist: flake8-comprehensions>=3.10.0; extra == "dev"
Requires-Dist: mypy>=1.0.0; extra == "dev"
Requires-Dist: pre-commit>=3.0.0; extra == "dev"
Requires-Dist: bandit>=1.7.0; extra == "dev"
Requires-Dist: pydocstyle>=6.3.0; extra == "dev"
Requires-Dist: coverage>=7.0.0; extra == "dev"
Requires-Dist: sphinx>=5.0.0; extra == "dev"
Requires-Dist: sphinx-rtd-theme>=1.2.0; extra == "dev"
Requires-Dist: mkdocs>=1.4.0; extra == "dev"
Requires-Dist: mkdocs-material>=9.0.0; extra == "dev"
Requires-Dist: mkdocs-mermaid2-plugin>=0.6.0; extra == "dev"
Provides-Extra: test
Requires-Dist: pytest>=7.0.0; extra == "test"
Requires-Dist: pytest-cov>=4.0.0; extra == "test"
Requires-Dist: pytest-xdist>=3.0.0; extra == "test"
Requires-Dist: pytest-mock>=3.10.0; extra == "test"
Requires-Dist: coverage>=7.0.0; extra == "test"
Provides-Extra: docs
Requires-Dist: mkdocs>=1.4.0; extra == "docs"
Requires-Dist: mkdocs-material>=9.0.0; extra == "docs"
Requires-Dist: mkdocs-mermaid2-plugin>=0.6.0; extra == "docs"
Requires-Dist: mkdocs-include-markdown-plugin>=4.0.0; extra == "docs"
Dynamic: author
Dynamic: home-page
Dynamic: requires-python

# ü§ñ LLMBuilder

[![Documentation](https://img.shields.io/badge/docs-mkdocs-blue)](https://qubasehq.github.io/llmbuilder-package/)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A comprehensive toolkit for building, training, fine-tuning, and deploying GPT-style language models with advanced data processing capabilities and CPU-friendly defaults.

## About LLMBuilder Framework

**LLMBuilder** is a production-ready framework for training and fine-tuning Large Language Models (LLMs) ‚Äî not a model itself. Designed for developers, researchers, and AI engineers, LLMBuilder provides a full pipeline to go from raw text data to deployable, optimized LLMs, all running locally on CPUs or GPUs.

### Complete Framework Structure

The full LLMBuilder framework includes:

```
LLMBuilder/
‚îú‚îÄ‚îÄ data/                   # Data directories
‚îÇ   ‚îú‚îÄ‚îÄ raw/               # Raw input files (.txt, .pdf, .docx)
‚îÇ   ‚îú‚îÄ‚îÄ cleaned/           # Processed text files
‚îÇ   ‚îî‚îÄ‚îÄ tokens/            # Tokenized datasets
‚îÇ   ‚îú‚îÄ‚îÄ download_data.py   # Script to download datasets
‚îÇ   ‚îî‚îÄ‚îÄ SOURCES.md         # Data sources documentation
‚îÇ
‚îú‚îÄ‚îÄ debug_scripts/         # Debugging utilities
‚îÇ   ‚îú‚îÄ‚îÄ debug_loader.py    # Data loading debugger
‚îÇ   ‚îú‚îÄ‚îÄ debug_training.py  # Training process debugger
‚îÇ   ‚îî‚îÄ‚îÄ debug_timestamps.py # Timing analysis
‚îÇ
‚îú‚îÄ‚îÄ eval/                  # Model evaluation
‚îÇ   ‚îî‚îÄ‚îÄ eval.py           # Evaluation scripts
‚îÇ
‚îú‚îÄ‚îÄ exports/               # Output directories
‚îÇ   ‚îú‚îÄ‚îÄ checkpoints/      # Training checkpoints
‚îÇ   ‚îú‚îÄ‚îÄ gguf/             # GGUF model exports
‚îÇ   ‚îú‚îÄ‚îÄ onnx/             # ONNX model exports
‚îÇ   ‚îî‚îÄ‚îÄ tokenizer/        # Saved tokenizer files
‚îÇ
‚îú‚îÄ‚îÄ finetune/             # Fine-tuning scripts
‚îÇ   ‚îú‚îÄ‚îÄ finetune.py      # Fine-tuning implementation
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py      # Package initialization
‚îÇ
‚îú‚îÄ‚îÄ logs/                 # Training and evaluation logs
‚îÇ
‚îú‚îÄ‚îÄ model/                # Model architecture
‚îÇ   ‚îî‚îÄ‚îÄ gpt_model.py     # GPT model implementation
‚îÇ
‚îú‚îÄ‚îÄ tools/                # Utility scripts
‚îÇ   ‚îú‚îÄ‚îÄ analyze_data.ps1  # PowerShell data analysis
‚îÇ   ‚îú‚îÄ‚îÄ analyze_data.sh   # Bash data analysis
‚îÇ   ‚îú‚îÄ‚îÄ download_hf_model.py # HuggingFace model downloader
‚îÇ   ‚îî‚îÄ‚îÄ export_gguf.py    # GGUF export utility
‚îÇ
‚îú‚îÄ‚îÄ training/             # Training pipeline
‚îÇ   ‚îú‚îÄ‚îÄ dataset.py       # Dataset handling
‚îÇ   ‚îú‚îÄ‚îÄ preprocess.py    # Data preprocessing
‚îÇ   ‚îú‚îÄ‚îÄ quantization.py  # Model quantization
‚îÇ   ‚îú‚îÄ‚îÄ train.py         # Main training script
‚îÇ   ‚îú‚îÄ‚îÄ train_tokenizer.py # Tokenizer training
‚îÇ   ‚îî‚îÄ‚îÄ utils.py         # Training utilities
‚îÇ
‚îú‚îÄ‚îÄ .gitignore           # Git ignore rules
‚îú‚îÄ‚îÄ config.json          # Main configuration
‚îú‚îÄ‚îÄ config_cpu_small.json # Small CPU config
‚îú‚îÄ‚îÄ config_gpu.json      # GPU configuration
‚îú‚îÄ‚îÄ inference.py         # Inference script
‚îú‚îÄ‚îÄ quantize_model.py    # Model quantization
‚îú‚îÄ‚îÄ README.md           # Documentation
‚îú‚îÄ‚îÄ requirements.txt    # Python dependencies
‚îú‚îÄ‚îÄ run.ps1            # PowerShell runner
‚îî‚îÄ‚îÄ run.sh             # Bash runner
```

üîó **Full Framework Repository**: [https://github.com/Qubasehq/llmbuilder](https://github.com/Qubasehq/llmbuilder)

> [!NOTE]
> **This is a separate framework** - The complete LLMBuilder framework shown above is **not related to this package**. It's a standalone, comprehensive framework available at the GitHub repository. This package (`llmbuilder_package`) provides the core modular toolkit, while the complete framework offers additional utilities, debugging tools, and production-ready scripts for comprehensive LLM development workflows.

## ‚ú® Key Features

### üîÑ Advanced Data Processing
- **Multi-Format Ingestion**: Process HTML, Markdown, EPUB, PDF, and text files with intelligent extraction
- **OCR Integration**: Automatic OCR fallback for scanned PDFs using Tesseract
- **Smart Deduplication**: Both exact and semantic duplicate detection with configurable similarity thresholds
- **Batch Processing**: Parallel processing with configurable worker threads and batch sizes

### üî§ Flexible Tokenization
- **Multiple Algorithms**: BPE, SentencePiece, Unigram, and WordPiece tokenizers
- **Custom Training**: Train tokenizers on your specific datasets with advanced configuration options
- **Validation Tools**: Built-in tokenizer testing and benchmarking utilities

### ‚ö° Model Conversion & Optimization
- **GGUF Pipeline**: Convert trained models to GGUF format for llama.cpp compatibility
- **Quantization Options**: Support for F32, F16, Q8_0, Q5_1, Q5_0, Q4_1, Q4_0 quantization levels
- **Batch Conversion**: Convert multiple models with different quantization levels simultaneously
- **Validation**: Automatic output validation and integrity checking

### ‚öôÔ∏è Configuration Management
- **Template System**: Pre-configured templates for different use cases (CPU, GPU, inference, etc.)
- **Validation**: Comprehensive configuration validation with detailed error reporting
- **Override Support**: Easy configuration customization with dot-notation overrides
- **CLI Integration**: Full command-line configuration management tools

### üñ•Ô∏è Production-Ready CLI
- **Complete Interface**: Full command-line interface for all operations
- **Interactive Modes**: Guided setup and configuration for new users
- **Progress Tracking**: Real-time progress reporting with detailed logging
- **Batch Operations**: Support for processing multiple files and models

### üß™ Quality Assurance
- **Extensive Testing**: 200+ unit and integration tests covering all functionality
- **Performance Tests**: Memory usage monitoring and performance benchmarking
- **CI/CD Pipeline**: Automated testing and validation on multiple platforms
- **Documentation**: Comprehensive documentation with examples and troubleshooting guides

## üöÄ Quick Start

### Installation

```bash
pip install llmbuilder
```

### Optional Dependencies

LLMBuilder works out of the box, but you can install additional dependencies for advanced features:

```bash
# Complete installation with all features
pip install llmbuilder[all]

# Or install specific feature sets:
pip install llmbuilder[pdf]        # PDF processing with OCR
pip install llmbuilder[epub]       # EPUB document processing
pip install llmbuilder[html]       # Advanced HTML processing
pip install llmbuilder[semantic]   # Semantic deduplication
pip install llmbuilder[conversion] # GGUF model conversion

# Manual installation of optional dependencies:
pip install pymupdf pytesseract    # PDF processing with OCR
pip install ebooklib               # EPUB processing  
pip install beautifulsoup4 lxml    # HTML processing
pip install sentence-transformers  # Semantic deduplication
```

### System Dependencies

**For PDF OCR Processing:**
```bash
# Windows: Download from https://github.com/UB-Mannheim/tesseract/wiki
# macOS:
brew install tesseract

# Ubuntu/Debian:
sudo apt-get install tesseract-ocr tesseract-ocr-eng

# Verify installation:
tesseract --version
```

**For GGUF Model Conversion:**
```bash
# Option 1: Install llama.cpp (recommended)
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp && make

# Option 2: Python package (alternative)
pip install llama-cpp-python

# Verify installation:
llmbuilder convert gguf --help
```

### 5-Minute Complete Pipeline

```python
import llmbuilder as lb

# 1. Create configuration from template
from llmbuilder.config.manager import create_config_from_template
config = create_config_from_template("basic_config", {
    "model": {"vocab_size": 16000},
    "data": {
        "ingestion": {"enable_ocr": True, "supported_formats": ["pdf", "html", "txt"]},
        "deduplication": {"similarity_threshold": 0.85}
    }
})

# 2. Process multi-format documents with advanced features
from llmbuilder.data.ingest import IngestionPipeline
pipeline = IngestionPipeline(config.data.ingestion)
pipeline.process_directory("./raw_data", "./processed_data")
print(f"Processed {pipeline.get_stats()['files_processed']} files")

# 3. Advanced deduplication (exact + semantic)
from llmbuilder.data.dedup import DeduplicationPipeline
dedup = DeduplicationPipeline(config.data.deduplication)
stats = dedup.process_file("./processed_data/combined.txt", "./clean_data/deduplicated.txt")
print(f"Removed {stats['duplicates_removed']} duplicates")

# 4. Train custom tokenizer with validation
from llmbuilder.tokenizer import TokenizerTrainer
trainer = TokenizerTrainer(config.tokenizer_training)
results = trainer.train("./clean_data/deduplicated.txt", "./tokenizers")
print(f"Trained tokenizer with {results['vocab_size']} tokens")

# 5. Build and train model
model = lb.build_model(config.model)
from llmbuilder.data import TextDataset
dataset = TextDataset("./clean_data/deduplicated.txt", block_size=config.model.max_seq_length)
training_results = lb.train_model(model, dataset, config.training)
print(f"Training completed. Final loss: {training_results['final_loss']:.4f}")

# 6. Convert to multiple GGUF formats
from llmbuilder.tools.convert_to_gguf import GGUFConverter
converter = GGUFConverter()
quantization_levels = ["Q8_0", "Q4_0", "Q4_1"]
for quant in quantization_levels:
    result = converter.convert_model(
        "./checkpoints/model.pt", 
        f"./exports/model_{quant.lower()}.gguf", 
        quant
    )
    if result.success:
        print(f"‚úÖ {quant}: {result.file_size_bytes / (1024*1024):.1f} MB")

# 7. Generate text with different sampling strategies
text_creative = lb.generate_text(
    model_path="./checkpoints/model.pt",
    tokenizer_path="./tokenizers",
    prompt="The future of AI is",
    max_new_tokens=100,
    temperature=0.8,  # More creative
    top_p=0.9
)

text_focused = lb.generate_text(
    model_path="./checkpoints/model.pt", 
    tokenizer_path="./tokenizers",
    prompt="The future of AI is",
    max_new_tokens=100,
    temperature=0.3,  # More focused
    top_k=40
)

print("Creative:", text_creative)
print("Focused:", text_focused)
```

## üìö Documentation

**Complete documentation is available at: [https://qubasehq.github.io/llmbuilder-package/](https://qubasehq.github.io/llmbuilder-package/)**

The documentation includes:
- üìñ **Getting Started Guide** - From installation to your first model
- üéØ **User Guides** - Comprehensive guides for all features  
- üñ•Ô∏è **CLI Reference** - Complete command-line interface documentation
- üêç **Python API** - Full API reference with examples
- üìã **Examples** - Working code examples for common tasks
- ‚ùì **FAQ** - Answers to frequently asked questions

## CLI Quickstart

### Getting Started
```bash
# Show help and available commands
llmbuilder --help

# Interactive welcome guide for new users
llmbuilder welcome

# Show package information and credits
llmbuilder info
```

### Data Processing Pipeline
```bash
# Multi-format document ingestion with OCR
llmbuilder data load -i ./documents -o ./processed.txt --format all --clean --enable-ocr

# Advanced deduplication (exact + semantic)
llmbuilder data deduplicate -i ./processed.txt -o ./clean.txt --method both --threshold 0.85

# Train custom tokenizer with validation
llmbuilder data tokenizer -i ./clean.txt -o ./tokenizers --algorithm sentencepiece --vocab-size 16000
```

### Configuration Management
```bash
# List available configuration templates
llmbuilder config templates

# Create configuration from template with overrides
llmbuilder config from-template advanced_processing_config -o my_config.json \
  --override model.vocab_size=24000 \
  --override training.batch_size=32

# Validate configuration with detailed reporting
llmbuilder config validate my_config.json --detailed

# Show comprehensive configuration summary
llmbuilder config summary my_config.json
```

### Model Training & Operations
```bash
# Train model with configuration file
llmbuilder train model --config my_config.json --data ./clean.txt --tokenizer ./tokenizers --output ./checkpoints

# Interactive text generation setup
llmbuilder generate text --setup

# Generate text with custom parameters
llmbuilder generate text -m ./checkpoints/model.pt -t ./tokenizers -p "Hello world" --temperature 0.8 --max-tokens 100
```

### GGUF Model Conversion
```bash
# Convert single model with validation
llmbuilder convert gguf ./checkpoints/model.pt -o ./exports/model.gguf -q Q8_0 --validate

# Convert with all quantization levels
llmbuilder convert gguf ./checkpoints/model.pt -o ./exports/model.gguf --all-quantizations

# Batch convert multiple models
llmbuilder convert batch -i ./models -o ./exports -q Q8_0 Q4_0 Q4_1 --pattern "*.pt"
```

### Advanced Operations
```bash
# Process large datasets with custom settings
llmbuilder data load -i ./large_docs -o ./processed.txt --batch-size 200 --workers 8 --format pdf,html

# Semantic deduplication with GPU acceleration
llmbuilder data deduplicate -i ./dataset.txt -o ./clean.txt --method semantic --use-gpu --batch-size 2000

# Train tokenizer with advanced options
llmbuilder data tokenizer -i ./corpus.txt -o ./tokenizers \
  --algorithm sentencepiece \
  --vocab-size 32000 \
  --character-coverage 0.9998 \
  --special-tokens "<pad>,<unk>,<s>,</s>,<mask>"
```

## Python API Quickstart

```python
import llmbuilder as lb

# Load a preset config and build a model
cfg = lb.load_config(preset="cpu_small")
model = lb.build_model(cfg.model)

# Train (example; see examples/train_tiny.py for a runnable script)
from llmbuilder.data import TextDataset
dataset = TextDataset("./data/clean.txt", block_size=cfg.model.max_seq_length)
results = lb.train_model(model, dataset, cfg.training)

# Generate text
text = lb.generate_text(
    model_path="./checkpoints/model.pt",
    tokenizer_path="./tokenizers",
    prompt="Hello world",
    max_new_tokens=50,
)
print(text)
```

## More
- Examples: see the `examples/` folder
  - `examples/generate_text.py`
  - `examples/train_tiny.py`
- Migration from older scripts: see `MIGRATION.md`

## For Developers and Advanced Users
- Python API quickstart:
  ```python
  import llmbuilder as lb
  cfg = lb.load_config(preset="cpu_small")
  model = lb.build_model(cfg.model)
  from llmbuilder.data import TextDataset
  dataset = TextDataset("./data/clean.txt", block_size=cfg.model.max_seq_length)
  results = lb.train_model(model, dataset, cfg.training)
  text = lb.generate_text(
      model_path="./checkpoints/model.pt",
      tokenizer_path="./tokenizers",
      prompt="Hello",
      max_new_tokens=64,
      temperature=0.8,
      top_k=50,
      top_p=0.9,
  )
  print(text)
  ```
- Config presets and legacy keys:
  - Use `lb.load_config(preset="cpu_small")` or `path="config.yaml"`.
  - Legacy flat keys like `n_layer`, `n_head`, `n_embd` are accepted and mapped internally.
- Useful CLI flags:
  - Training: `--epochs`, `--batch-size`, `--lr`, `--eval-interval`, `--save-interval` (see `llmbuilder train model --help`).
  - Generation: `--max-new-tokens`, `--temperature`, `--top-k`, `--top-p`, `--device` (see `llmbuilder generate text --help`).
- Environment knobs:
  - Enable slow tests: `set RUN_SLOW=1`
  - Enable performance tests: `set RUN_PERF=1`
- Performance tips:
  - Prefer CPU wheels for broad compatibility; use smaller seq length and batch size.
  - Checkpoints are saved under `checkpoints/`; consider periodic eval to monitor perplexity.

## üîß Troubleshooting

### Installation Issues

**Missing Optional Dependencies**
```bash
# Check what's installed
python -c "import llmbuilder; print('‚úÖ LLMBuilder installed')"

# Install missing dependencies
pip install pymupdf pytesseract ebooklib beautifulsoup4 lxml sentence-transformers

# Verify specific features
python -c "import pytesseract; print('‚úÖ OCR available')"
python -c "import sentence_transformers; print('‚úÖ Semantic deduplication available')"
```

**System Dependencies**
```bash
# Tesseract OCR (for PDF processing)
# Windows: Download from https://github.com/UB-Mannheim/tesseract/wiki
# macOS: brew install tesseract
# Ubuntu: sudo apt-get install tesseract-ocr tesseract-ocr-eng

# Verify Tesseract installation
tesseract --version
python -c "import pytesseract; pytesseract.get_tesseract_version()"
```

### Processing Issues

**PDF Processing Problems**
```bash
# Enable debug logging
export LLMBUILDER_LOG_LEVEL=DEBUG

# Test OCR functionality
llmbuilder data load -i test.pdf -o output.txt --enable-ocr --verbose

# Common fixes:
# 1. Install language packs: sudo apt-get install tesseract-ocr-eng
# 2. Set Tesseract path: export TESSDATA_PREFIX=/usr/share/tesseract-ocr/4.00/tessdata
# 3. Lower OCR threshold: --ocr-threshold 0.3
```

**Memory Issues with Large Datasets**
```bash
# Use configuration to optimize memory usage
llmbuilder config from-template cpu_optimized_config -o memory_config.json \
  --override data.ingestion.batch_size=50 \
  --override data.deduplication.batch_size=500 \
  --override data.deduplication.use_gpu_for_embeddings=false

# Process in smaller chunks
llmbuilder data load -i large_dataset/ -o processed.txt --batch-size 25 --workers 2
```

**Semantic Deduplication Performance**
```bash
# GPU issues - disable GPU acceleration
llmbuilder data deduplicate -i dataset.txt -o clean.txt --method semantic --no-gpu

# Slow processing - increase batch size
llmbuilder data deduplicate -i dataset.txt -o clean.txt --method semantic --batch-size 2000

# Memory issues - reduce embedding cache
llmbuilder config from-template basic_config -o config.json \
  --override data.deduplication.embedding_cache_size=5000
```

### GGUF Conversion Issues

**Missing llama.cpp**
```bash
# Install llama.cpp
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp && make

# Add to PATH or specify location
export PATH=$PATH:/path/to/llama.cpp

# Alternative: Use Python package
pip install llama-cpp-python

# Test conversion
llmbuilder convert gguf --help
```

**Conversion Failures**
```bash
# Check available conversion scripts
llmbuilder convert gguf model.pt -o test.gguf --verbose

# Try different quantization levels
llmbuilder convert gguf model.pt -o test.gguf -q F16  # Less compression
llmbuilder convert gguf model.pt -o test.gguf -q Q8_0 # Balanced

# Increase timeout for large models
llmbuilder config from-template basic_config -o config.json \
  --override gguf_conversion.conversion_timeout=7200
```

### Configuration Issues

**Validation Errors**
```bash
# Validate configuration with detailed output
llmbuilder config validate my_config.json --detailed

# Common fixes:
# 1. Vocab size mismatch - ensure model.vocab_size matches tokenizer_training.vocab_size
# 2. Sequence length issues - ensure data.max_length <= model.max_seq_length
# 3. Invalid quantization level - use: F32, F16, Q8_0, Q5_1, Q5_0, Q4_1, Q4_0
```

**Template Issues**
```bash
# List available templates
llmbuilder config templates

# Create from working template
llmbuilder config from-template basic_config -o working_config.json

# Validate before use
llmbuilder config validate working_config.json
```

### Performance Optimization

**Speed Up Processing**
```bash
# Use more workers for I/O bound tasks
llmbuilder data load -i docs/ -o processed.txt --workers 8

# Enable GPU for semantic operations
llmbuilder data deduplicate -i dataset.txt -o clean.txt --use-gpu --batch-size 2000

# Use faster HTML parser
llmbuilder config from-template basic_config -o config.json \
  --override data.ingestion.html_parser=lxml
```

**Reduce Memory Usage**
```bash
# Smaller batch sizes
llmbuilder data load -i docs/ -o processed.txt --batch-size 25

# Disable semantic deduplication for large datasets
llmbuilder data deduplicate -i dataset.txt -o clean.txt --method exact

# Use CPU-optimized configuration
llmbuilder config from-template cpu_optimized_config -o config.json
```

### Debug Mode

**Enable Verbose Logging**
```bash
# Set environment variable
export LLMBUILDER_LOG_LEVEL=DEBUG

# Or use CLI flag
llmbuilder data load -i docs/ -o processed.txt --verbose

# Check processing statistics
llmbuilder data load -i docs/ -o processed.txt --stats
```

### Getting Help

- üìñ **Documentation**: [https://qubasehq.github.io/llmbuilder-package/](https://qubasehq.github.io/llmbuilder-package/)
- üêõ **Issues**: [GitHub Issues](https://github.com/Qubasehq/llmbuilder-package/issues)
- üí¨ **Discussions**: [GitHub Discussions](https://github.com/Qubasehq/llmbuilder-package/discussions)

## Testing (developers)
- Fast tests: `python -m pytest -q tests`
- Slow tests: `set RUN_SLOW=1 && python -m pytest -q tests`
- Performance tests: `set RUN_PERF=1 && python -m pytest -q tests\performance`

## License
Apache-2.0 (or project license).
