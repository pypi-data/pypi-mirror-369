
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/advanced_training/plot_data_augmentation_search.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_advanced_training_plot_data_augmentation_search.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_advanced_training_plot_data_augmentation_search.py:

.. _bcic-iv-2a-best-data-aug:

Searching the best data augmentation on BCIC IV 2a Dataset
====================================================================================

This tutorial shows how to search data augmentations using braindecode.
Indeed, it is known that the best augmentation to use often dependent on the task
or phenomenon studied. Here we follow the methodology proposed in [1]_ on the
openly available BCI IV 2a Dataset.


.. topic:: Data Augmentation

    Data augmentation could be a step in training deep learning models.
    For decoding brain signals, recent studies have shown that artificially
    generating samples may increase the final performance of a deep learning model [1]_.
    Other studies have shown that data augmentation can be used to cast
    a self-supervised paradigm, presenting a more diverse
    view of the data, both with pretext tasks and contrastive learning [2]_.


Data augmentation and self-supervised learning approaches demand an intense comparison
to find the best fit with the data. This view is demonstrated in [1]_ and shows the
importance of selecting the right transformation and strength for different type of
task considered. Here, we use the augmentation module present in braindecode in
the context of trialwise decoding with the BCI IV 2a dataset.

.. contents:: This example covers:
   :local:
   :depth: 2

.. GENERATED FROM PYTHON SOURCE LINES 33-38

.. code-block:: Python


    # Authors: Bruno Aristimunha <a.bruno@ufabc.edu.br>
    #          Cédric Rommel <cedric.rommel@inria.fr>
    # License: BSD (3-clause)








.. GENERATED FROM PYTHON SOURCE LINES 39-49

Loading and preprocessing the dataset
-------------------------------------

Loading
~~~~~~~

First, we load the data. In this tutorial, we use the functionality of braindecode
to load BCI IV competition dataset 1. The dataset is available on the BNCI website.
There is 9 subjects recorded with 22 electrodes while doing a motor imagery task,
with 144 trials per class. We will load this dataset through the MOABB library.

.. GENERATED FROM PYTHON SOURCE LINES 49-58

.. code-block:: Python


    from skorch.callbacks import LRScheduler

    from braindecode import EEGClassifier
    from braindecode.datasets import MOABBDataset

    subject_id = 3
    dataset = MOABBDataset(dataset_name="BNCI2014001", subject_ids=[subject_id])





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    BNCI2014001 has been renamed to BNCI2014_001. BNCI2014001 will be removed in version 1.1.
    The dataset class name 'BNCI2014001' must be an abbreviation of its code 'BNCI2014-001'. See moabb.datasets.base.is_abbrev for more information.




.. GENERATED FROM PYTHON SOURCE LINES 59-64

Preprocessing
~~~~~~~~~~~~~

We apply a bandpass filter, from 4 to 38 Hz to focus motor imagery-related
brain activity

.. GENERATED FROM PYTHON SOURCE LINES 64-81

.. code-block:: Python


    from numpy import multiply

    from braindecode.preprocessing import (
        Preprocessor,
        exponential_moving_standardize,
        preprocess,
    )

    low_cut_hz = 4.0  # low cut frequency for filtering
    high_cut_hz = 38.0  # high cut frequency for filtering
    # Parameters for exponential moving standardization
    factor_new = 1e-3
    init_block_size = 1000
    # Factor to convert from V to uV
    factor = 1e6








.. GENERATED FROM PYTHON SOURCE LINES 82-86

In time series targets setup, targets variables are stored in mne.Raw object as channels
of type `misc`. Thus those channels have to be selected for further processing. However,
many mne functions ignore `misc` channels and perform operations only on data channels
(see `MNE's glossary on data channels <MNE-glossary-data-channels_>`_).

.. GENERATED FROM PYTHON SOURCE LINES 86-100

.. code-block:: Python


    preprocessors = [
        Preprocessor("pick_types", eeg=True, meg=False, stim=False),  # Keep EEG sensors
        Preprocessor(lambda data: multiply(data, factor)),  # Convert from V to uV
        Preprocessor("filter", l_freq=low_cut_hz, h_freq=high_cut_hz),  # Bandpass filter
        Preprocessor(
            exponential_moving_standardize,  # Exponential moving standardization
            factor_new=factor_new,
            init_block_size=init_block_size,
        ),
    ]

    preprocess(dataset, preprocessors, n_jobs=-1)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /Users/baristim/Projects/braindecode-1/braindecode/preprocessing/preprocess.py:71: UserWarning: Preprocessing choices with lambda functions cannot be saved.
      warn("Preprocessing choices with lambda functions cannot be saved.")

    <braindecode.datasets.moabb.MOABBDataset object at 0x31d98fc20>



.. GENERATED FROM PYTHON SOURCE LINES 101-107

Extracting windows
~~~~~~~~~~~~~~~~~~

Now we cut out compute windows, the inputs for the deep networks during
training. We use the braindecode function for this, provinding parameters
to define how trials should be used.

.. GENERATED FROM PYTHON SOURCE LINES 107-128

.. code-block:: Python



    from numpy import array
    from skorch.helper import SliceDataset

    from braindecode.preprocessing import create_windows_from_events

    trial_start_offset_seconds = -0.5
    # Extract sampling frequency, check that they are same in all datasets
    sfreq = dataset.datasets[0].raw.info["sfreq"]
    assert all([ds.raw.info["sfreq"] == sfreq for ds in dataset.datasets])
    # Calculate the trial start offset in samples.
    trial_start_offset_samples = int(trial_start_offset_seconds * sfreq)

    windows_dataset = create_windows_from_events(
        dataset,
        trial_start_offset_samples=trial_start_offset_samples,
        trial_stop_offset_samples=0,
        preload=True,
    )








.. GENERATED FROM PYTHON SOURCE LINES 129-132

Split dataset into train and valid
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Following the split defined in the BCI competition

.. GENERATED FROM PYTHON SOURCE LINES 132-138

.. code-block:: Python



    splitted = windows_dataset.split("session")
    train_set = splitted["0train"]  # Session train
    eval_set = splitted["1test"]  # Session evaluation








.. GENERATED FROM PYTHON SOURCE LINES 139-160

Defining a list of transforms
------------------------------

In this tutorial, we will use three categories of augmentations.
This categorization has been proposed by [1]_ to explain and aggregate
the several possibilities of augmentations in EEG, being them:

a) Frequency domain augmentations,
b) Time domain augmentations,
c) Spatial domain augmentations.

From this same paper, we selected the best augmentations in each type: ``FTSurrogate``,
``SmoothTimeMask``, ``ChannelsDropout``, respectively.

For each augmentation, we adjustable two values from a range for one parameter
inside the transformation.

It is important to remember that you can increase the range.
For that, we need to define three lists of transformations and range for the parameter
∆φmax in FTSurrogate where ∆φmax ∈ [0, 2π); for ∆t in SmoothTimeMask is ∆t ∈ [0, 2];
For the method ChannelsDropout, we analyse the parameter p_drop ∈ [0, 1].

.. GENERATED FROM PYTHON SOURCE LINES 160-184

.. code-block:: Python


    from numpy import linspace

    from braindecode.augmentation import ChannelsDropout, FTSurrogate, SmoothTimeMask

    seed = 20200220

    transforms_freq = [
        FTSurrogate(probability=0.5, phase_noise_magnitude=phase_freq, random_state=seed)
        for phase_freq in linspace(0, 1, 2)
    ]

    transforms_time = [
        SmoothTimeMask(
            probability=0.5, mask_len_samples=int(sfreq * second), random_state=seed
        )
        for second in linspace(0.1, 2, 2)
    ]

    transforms_spatial = [
        ChannelsDropout(probability=0.5, p_drop=prob, random_state=seed)
        for prob in linspace(0, 1, 2)
    ]








.. GENERATED FROM PYTHON SOURCE LINES 185-196

Training a model with data augmentation
---------------------------------------

Now that we know how to instantiate three list of ``Transforms``, it is time to learn how
to use them to train a model and try to search the best for the dataset.
Let's first create a model for search a parameter.

Create model
~~~~~~~~~~~~

The model to be trained is defined as usual.

.. GENERATED FROM PYTHON SOURCE LINES 196-207

.. code-block:: Python

    import torch

    from braindecode.models import ShallowFBCSPNet
    from braindecode.util import set_random_seeds

    cuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it
    device = "cuda" if cuda else "cpu"
    if cuda:
        torch.backends.cudnn.benchmark = True









.. GENERATED FROM PYTHON SOURCE LINES 208-214

Set random seed to be able to roughly reproduce results
Note that with cudnn benchmark set to True, GPU indeterminism
may still make results substantially different between runs.
To obtain more consistent results at the cost of increased computation time,
you can set ``cudnn_benchmark=False`` in ``set_random_seeds``
or remove ``torch.backends.cudnn.benchmark = True``

.. GENERATED FROM PYTHON SOURCE LINES 214-231

.. code-block:: Python


    seed = 20200220
    set_random_seeds(seed=seed, cuda=cuda)

    n_classes = 4
    classes = list(range(n_classes))
    # Extract number of chans and time steps from dataset
    n_channels = train_set[0][0].shape[0]
    n_times = train_set[0][0].shape[1]

    model = ShallowFBCSPNet(
        n_chans=n_channels,
        n_outputs=n_classes,
        n_times=n_times,
        final_conv_length="auto",
    )








.. GENERATED FROM PYTHON SOURCE LINES 232-238

Create an EEGClassifier with the desired augmentation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In order to train with data augmentation, a custom data loader can be
for the training. Multiple transforms can be passed to it and will be applied
sequentially to the batched data within the ``AugmentedDataLoader`` object.

.. GENERATED FROM PYTHON SOURCE LINES 238-245

.. code-block:: Python


    from braindecode.augmentation import AugmentedDataLoader

    # Send model to GPU
    if cuda:
        model.cuda()








.. GENERATED FROM PYTHON SOURCE LINES 246-249

The model is now trained as in the trial-wise example. The
``AugmentedDataLoader`` is used as the train iterator and the list of
transforms are passed as arguments.

.. GENERATED FROM PYTHON SOURCE LINES 249-274

.. code-block:: Python


    lr = 0.0625 * 0.01
    weight_decay = 0

    batch_size = 64
    n_epochs = 2

    clf = EEGClassifier(
        model,
        iterator_train=AugmentedDataLoader,  # This tells EEGClassifier to use a custom DataLoader
        iterator_train__transforms=[],  # This sets is handled by GridSearchCV
        criterion=torch.nn.CrossEntropyLoss,
        optimizer=torch.optim.AdamW,
        train_split=None,  # GridSearchCV will control the split and train/validation over the dataset
        optimizer__lr=lr,
        optimizer__weight_decay=weight_decay,
        batch_size=batch_size,
        callbacks=[
            "accuracy",
            ("lr_scheduler", LRScheduler("CosineAnnealingLR", T_max=n_epochs - 1)),
        ],
        device=device,
        classes=classes,
    )








.. GENERATED FROM PYTHON SOURCE LINES 275-278

To use the skorch framework, it is necessary to transform the windows
dataset using the module SliceData. Also, it is mandatory to eval the
generator of the training.

.. GENERATED FROM PYTHON SOURCE LINES 278-282

.. code-block:: Python


    train_X = SliceDataset(train_set, idx=0)
    train_y = array(list(SliceDataset(train_set, idx=1)))








.. GENERATED FROM PYTHON SOURCE LINES 283-285

Given the trialwise approach, here we use the KFold approach and
GridSearchCV.

.. GENERATED FROM PYTHON SOURCE LINES 285-312

.. code-block:: Python


    from sklearn.model_selection import GridSearchCV, KFold

    cv = KFold(n_splits=2, shuffle=True, random_state=seed)
    fit_params = {"epochs": n_epochs}

    transforms = transforms_freq + transforms_time + transforms_spatial

    param_grid = {
        "iterator_train__transforms": transforms,
    }

    clf.verbose = 0

    search = GridSearchCV(
        estimator=clf,
        param_grid=param_grid,
        cv=cv,
        return_train_score=True,
        scoring="accuracy",
        refit=True,
        verbose=1,
        error_score="raise",
    )

    search.fit(train_X, train_y, **fit_params)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Fitting 2 folds for each of 6 candidates, totalling 12 fits


.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <style>#sk-container-id-2 {
      /* Definition of color scheme common for light and dark mode */
      --sklearn-color-text: black;
      --sklearn-color-line: gray;
      /* Definition of color scheme for unfitted estimators */
      --sklearn-color-unfitted-level-0: #fff5e6;
      --sklearn-color-unfitted-level-1: #f6e4d2;
      --sklearn-color-unfitted-level-2: #ffe0b3;
      --sklearn-color-unfitted-level-3: chocolate;
      /* Definition of color scheme for fitted estimators */
      --sklearn-color-fitted-level-0: #f0f8ff;
      --sklearn-color-fitted-level-1: #d4ebff;
      --sklearn-color-fitted-level-2: #b3dbfd;
      --sklearn-color-fitted-level-3: cornflowerblue;

      /* Specific color for light theme */
      --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
      --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
      --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
      --sklearn-color-icon: #696969;

      @media (prefers-color-scheme: dark) {
        /* Redefinition of color scheme for dark theme */
        --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
        --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
        --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
        --sklearn-color-icon: #878787;
      }
    }

    #sk-container-id-2 {
      color: var(--sklearn-color-text);
    }

    #sk-container-id-2 pre {
      padding: 0;
    }

    #sk-container-id-2 input.sk-hidden--visually {
      border: 0;
      clip: rect(1px 1px 1px 1px);
      clip: rect(1px, 1px, 1px, 1px);
      height: 1px;
      margin: -1px;
      overflow: hidden;
      padding: 0;
      position: absolute;
      width: 1px;
    }

    #sk-container-id-2 div.sk-dashed-wrapped {
      border: 1px dashed var(--sklearn-color-line);
      margin: 0 0.4em 0.5em 0.4em;
      box-sizing: border-box;
      padding-bottom: 0.4em;
      background-color: var(--sklearn-color-background);
    }

    #sk-container-id-2 div.sk-container {
      /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
         but bootstrap.min.css set `[hidden] { display: none !important; }`
         so we also need the `!important` here to be able to override the
         default hidden behavior on the sphinx rendered scikit-learn.org.
         See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
      display: inline-block !important;
      position: relative;
    }

    #sk-container-id-2 div.sk-text-repr-fallback {
      display: none;
    }

    div.sk-parallel-item,
    div.sk-serial,
    div.sk-item {
      /* draw centered vertical line to link estimators */
      background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
      background-size: 2px 100%;
      background-repeat: no-repeat;
      background-position: center center;
    }

    /* Parallel-specific style estimator block */

    #sk-container-id-2 div.sk-parallel-item::after {
      content: "";
      width: 100%;
      border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
      flex-grow: 1;
    }

    #sk-container-id-2 div.sk-parallel {
      display: flex;
      align-items: stretch;
      justify-content: center;
      background-color: var(--sklearn-color-background);
      position: relative;
    }

    #sk-container-id-2 div.sk-parallel-item {
      display: flex;
      flex-direction: column;
    }

    #sk-container-id-2 div.sk-parallel-item:first-child::after {
      align-self: flex-end;
      width: 50%;
    }

    #sk-container-id-2 div.sk-parallel-item:last-child::after {
      align-self: flex-start;
      width: 50%;
    }

    #sk-container-id-2 div.sk-parallel-item:only-child::after {
      width: 0;
    }

    /* Serial-specific style estimator block */

    #sk-container-id-2 div.sk-serial {
      display: flex;
      flex-direction: column;
      align-items: center;
      background-color: var(--sklearn-color-background);
      padding-right: 1em;
      padding-left: 1em;
    }


    /* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
    clickable and can be expanded/collapsed.
    - Pipeline and ColumnTransformer use this feature and define the default style
    - Estimators will overwrite some part of the style using the `sk-estimator` class
    */

    /* Pipeline and ColumnTransformer style (default) */

    #sk-container-id-2 div.sk-toggleable {
      /* Default theme specific background. It is overwritten whether we have a
      specific estimator or a Pipeline/ColumnTransformer */
      background-color: var(--sklearn-color-background);
    }

    /* Toggleable label */
    #sk-container-id-2 label.sk-toggleable__label {
      cursor: pointer;
      display: block;
      width: 100%;
      margin-bottom: 0;
      padding: 0.5em;
      box-sizing: border-box;
      text-align: center;
    }

    #sk-container-id-2 label.sk-toggleable__label-arrow:before {
      /* Arrow on the left of the label */
      content: "▸";
      float: left;
      margin-right: 0.25em;
      color: var(--sklearn-color-icon);
    }

    #sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {
      color: var(--sklearn-color-text);
    }

    /* Toggleable content - dropdown */

    #sk-container-id-2 div.sk-toggleable__content {
      max-height: 0;
      max-width: 0;
      overflow: hidden;
      text-align: left;
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-0);
    }

    #sk-container-id-2 div.sk-toggleable__content.fitted {
      /* fitted */
      background-color: var(--sklearn-color-fitted-level-0);
    }

    #sk-container-id-2 div.sk-toggleable__content pre {
      margin: 0.2em;
      border-radius: 0.25em;
      color: var(--sklearn-color-text);
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-0);
    }

    #sk-container-id-2 div.sk-toggleable__content.fitted pre {
      /* unfitted */
      background-color: var(--sklearn-color-fitted-level-0);
    }

    #sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {
      /* Expand drop-down */
      max-height: 200px;
      max-width: 100%;
      overflow: auto;
    }

    #sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
      content: "▾";
    }

    /* Pipeline/ColumnTransformer-specific style */

    #sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
      color: var(--sklearn-color-text);
      background-color: var(--sklearn-color-unfitted-level-2);
    }

    #sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
      background-color: var(--sklearn-color-fitted-level-2);
    }

    /* Estimator-specific style */

    /* Colorize estimator box */
    #sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-2);
    }

    #sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
      /* fitted */
      background-color: var(--sklearn-color-fitted-level-2);
    }

    #sk-container-id-2 div.sk-label label.sk-toggleable__label,
    #sk-container-id-2 div.sk-label label {
      /* The background is the default theme color */
      color: var(--sklearn-color-text-on-default-background);
    }

    /* On hover, darken the color of the background */
    #sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {
      color: var(--sklearn-color-text);
      background-color: var(--sklearn-color-unfitted-level-2);
    }

    /* Label box, darken color on hover, fitted */
    #sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
      color: var(--sklearn-color-text);
      background-color: var(--sklearn-color-fitted-level-2);
    }

    /* Estimator label */

    #sk-container-id-2 div.sk-label label {
      font-family: monospace;
      font-weight: bold;
      display: inline-block;
      line-height: 1.2em;
    }

    #sk-container-id-2 div.sk-label-container {
      text-align: center;
    }

    /* Estimator-specific */
    #sk-container-id-2 div.sk-estimator {
      font-family: monospace;
      border: 1px dotted var(--sklearn-color-border-box);
      border-radius: 0.25em;
      box-sizing: border-box;
      margin-bottom: 0.5em;
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-0);
    }

    #sk-container-id-2 div.sk-estimator.fitted {
      /* fitted */
      background-color: var(--sklearn-color-fitted-level-0);
    }

    /* on hover */
    #sk-container-id-2 div.sk-estimator:hover {
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-2);
    }

    #sk-container-id-2 div.sk-estimator.fitted:hover {
      /* fitted */
      background-color: var(--sklearn-color-fitted-level-2);
    }

    /* Specification for estimator info (e.g. "i" and "?") */

    /* Common style for "i" and "?" */

    .sk-estimator-doc-link,
    a:link.sk-estimator-doc-link,
    a:visited.sk-estimator-doc-link {
      float: right;
      font-size: smaller;
      line-height: 1em;
      font-family: monospace;
      background-color: var(--sklearn-color-background);
      border-radius: 1em;
      height: 1em;
      width: 1em;
      text-decoration: none !important;
      margin-left: 1ex;
      /* unfitted */
      border: var(--sklearn-color-unfitted-level-1) 1pt solid;
      color: var(--sklearn-color-unfitted-level-1);
    }

    .sk-estimator-doc-link.fitted,
    a:link.sk-estimator-doc-link.fitted,
    a:visited.sk-estimator-doc-link.fitted {
      /* fitted */
      border: var(--sklearn-color-fitted-level-1) 1pt solid;
      color: var(--sklearn-color-fitted-level-1);
    }

    /* On hover */
    div.sk-estimator:hover .sk-estimator-doc-link:hover,
    .sk-estimator-doc-link:hover,
    div.sk-label-container:hover .sk-estimator-doc-link:hover,
    .sk-estimator-doc-link:hover {
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-3);
      color: var(--sklearn-color-background);
      text-decoration: none;
    }

    div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
    .sk-estimator-doc-link.fitted:hover,
    div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
    .sk-estimator-doc-link.fitted:hover {
      /* fitted */
      background-color: var(--sklearn-color-fitted-level-3);
      color: var(--sklearn-color-background);
      text-decoration: none;
    }

    /* Span, style for the box shown on hovering the info icon */
    .sk-estimator-doc-link span {
      display: none;
      z-index: 9999;
      position: relative;
      font-weight: normal;
      right: .2ex;
      padding: .5ex;
      margin: .5ex;
      width: min-content;
      min-width: 20ex;
      max-width: 50ex;
      color: var(--sklearn-color-text);
      box-shadow: 2pt 2pt 4pt #999;
      /* unfitted */
      background: var(--sklearn-color-unfitted-level-0);
      border: .5pt solid var(--sklearn-color-unfitted-level-3);
    }

    .sk-estimator-doc-link.fitted span {
      /* fitted */
      background: var(--sklearn-color-fitted-level-0);
      border: var(--sklearn-color-fitted-level-3);
    }

    .sk-estimator-doc-link:hover span {
      display: block;
    }

    /* "?"-specific style due to the `<a>` HTML tag */

    #sk-container-id-2 a.estimator_doc_link {
      float: right;
      font-size: 1rem;
      line-height: 1em;
      font-family: monospace;
      background-color: var(--sklearn-color-background);
      border-radius: 1rem;
      height: 1rem;
      width: 1rem;
      text-decoration: none;
      /* unfitted */
      color: var(--sklearn-color-unfitted-level-1);
      border: var(--sklearn-color-unfitted-level-1) 1pt solid;
    }

    #sk-container-id-2 a.estimator_doc_link.fitted {
      /* fitted */
      border: var(--sklearn-color-fitted-level-1) 1pt solid;
      color: var(--sklearn-color-fitted-level-1);
    }

    /* On hover */
    #sk-container-id-2 a.estimator_doc_link:hover {
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-3);
      color: var(--sklearn-color-background);
      text-decoration: none;
    }

    #sk-container-id-2 a.estimator_doc_link.fitted:hover {
      /* fitted */
      background-color: var(--sklearn-color-fitted-level-3);
    }
    </style><div id="sk-container-id-2" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>GridSearchCV(cv=KFold(n_splits=2, random_state=20200220, shuffle=True),
                 error_score=&#x27;raise&#x27;,
                 estimator=EEGClassifier(_params_to_validate={&#x27;iterator_train__transforms&#x27;, &#x27;optimizer__weight_decay&#x27;, &#x27;iterator_train__shuffle&#x27;, &#x27;optimizer__lr&#x27;, &#x27;iterator_train__drop_last&#x27;}, batch_size=64, callbacks=[&#x27;accuracy&#x27;, (&#x27;lr_scheduler&#x27;, &lt;skorch.callbacks.lr_scheduler.LRSched...
    ), optimizer=&lt;class &#x27;torch.optim.adamw.AdamW&#x27;&gt;, optimizer__lr=0.000625, optimizer__weight_decay=0, predict_nonlinearity=&#x27;auto&#x27;, torch_load_kwargs=None, train_split=None, use_caching=&#x27;auto&#x27;, verbose=0, warm_start=False),
                 param_grid={&#x27;iterator_train__transforms&#x27;: [FTSurrogate(),
                                                            FTSurrogate(),
                                                            SmoothTimeMask(),
                                                            SmoothTimeMask(),
                                                            ChannelsDropout(),
                                                            ChannelsDropout()]},
                 return_train_score=True, scoring=&#x27;accuracy&#x27;, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox" ><label for="sk-estimator-id-2" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;&nbsp;GridSearchCV<a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.GridSearchCV.html">?<span>Documentation for GridSearchCV</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>GridSearchCV(cv=KFold(n_splits=2, random_state=20200220, shuffle=True),
                 error_score=&#x27;raise&#x27;,
                 estimator=EEGClassifier(_params_to_validate={&#x27;iterator_train__transforms&#x27;, &#x27;optimizer__weight_decay&#x27;, &#x27;iterator_train__shuffle&#x27;, &#x27;optimizer__lr&#x27;, &#x27;iterator_train__drop_last&#x27;}, batch_size=64, callbacks=[&#x27;accuracy&#x27;, (&#x27;lr_scheduler&#x27;, &lt;skorch.callbacks.lr_scheduler.LRSched...
    ), optimizer=&lt;class &#x27;torch.optim.adamw.AdamW&#x27;&gt;, optimizer__lr=0.000625, optimizer__weight_decay=0, predict_nonlinearity=&#x27;auto&#x27;, torch_load_kwargs=None, train_split=None, use_caching=&#x27;auto&#x27;, verbose=0, warm_start=False),
                 param_grid={&#x27;iterator_train__transforms&#x27;: [FTSurrogate(),
                                                            FTSurrogate(),
                                                            SmoothTimeMask(),
                                                            SmoothTimeMask(),
                                                            ChannelsDropout(),
                                                            ChannelsDropout()]},
                 return_train_score=True, scoring=&#x27;accuracy&#x27;, verbose=1)</pre></div> </div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-3" type="checkbox" ><label for="sk-estimator-id-3" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">best_estimator_: EEGClassifier</label><div class="sk-toggleable__content fitted"><pre>&lt;class &#x27;braindecode.classifier.EEGClassifier&#x27;&gt;[initialized](
      module_==================================================================================================================================================
      Layer (type (var_name):depth-idx)             Input Shape               Output Shape              Param #                   Kernel Shape
      =================================================================================================================================================
      ShallowFBCSPNet (ShallowFBCSPNet)             [1, 22, 1125]             [1, 4]                    --                        --
      ├─Ensure4d (ensuredims): 1-1                  [1, 22, 1125]             [1, 22, 1125, 1]          --                        --
      ├─Rearrange (dimshuffle): 1-2                 [1, 22, 1125, 1]          [1, 1, 1125, 22]          --                        --
      ├─CombinedConv (conv_time_spat): 1-3          [1, 1, 1125, 22]          [1, 40, 1101, 1]          36,240                    --
      ├─BatchNorm2d (bnorm): 1-4                    [1, 40, 1101, 1]          [1, 40, 1101, 1]          80                        --
      ├─Expression (conv_nonlin_exp): 1-5           [1, 40, 1101, 1]          [1, 40, 1101, 1]          --                        --
      ├─AvgPool2d (pool): 1-6                       [1, 40, 1101, 1]          [1, 40, 69, 1]            --                        [75, 1]
      ├─SafeLog (pool_nonlin_exp): 1-7              [1, 40, 69, 1]            [1, 40, 69, 1]            --                        --
      ├─Dropout (drop): 1-8                         [1, 40, 69, 1]            [1, 40, 69, 1]            --                        --
      ├─Sequential (final_layer): 1-9               [1, 40, 69, 1]            [1, 4]                    --                        --
      │    └─Conv2d (conv_classifier): 2-1          [1, 40, 69, 1]            [1, 4, 1, 1]              11,044                    [69, 1]
      │    └─SqueezeFinalOutput (squeeze): 2-2      [1, 4, 1, 1]              [1, 4]                    --                        --
      │    │    └─Rearrange (squeeze): 3-1          [1, 4, 1, 1]              [1, 4, 1]                 --                        --
      =================================================================================================================================================
      Total params: 47,364
      Trainable params: 47,364
      Non-trainable params: 0
      Total mult-adds (Units.MEGABYTES): 0.01
      =================================================================================================================================================
      Input size (MB): 0.10
      Forward/backward pass size (MB): 0.35
      Params size (MB): 0.04
      Estimated Total Size (MB): 0.50
      =================================================================================================================================================,
    )</pre></div> </div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-4" type="checkbox" ><label for="sk-estimator-id-4" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">EEGClassifier</label><div class="sk-toggleable__content fitted"><pre>&lt;class &#x27;braindecode.classifier.EEGClassifier&#x27;&gt;[initialized](
      module_==================================================================================================================================================
      Layer (type (var_name):depth-idx)             Input Shape               Output Shape              Param #                   Kernel Shape
      =================================================================================================================================================
      ShallowFBCSPNet (ShallowFBCSPNet)             [1, 22, 1125]             [1, 4]                    --                        --
      ├─Ensure4d (ensuredims): 1-1                  [1, 22, 1125]             [1, 22, 1125, 1]          --                        --
      ├─Rearrange (dimshuffle): 1-2                 [1, 22, 1125, 1]          [1, 1, 1125, 22]          --                        --
      ├─CombinedConv (conv_time_spat): 1-3          [1, 1, 1125, 22]          [1, 40, 1101, 1]          36,240                    --
      ├─BatchNorm2d (bnorm): 1-4                    [1, 40, 1101, 1]          [1, 40, 1101, 1]          80                        --
      ├─Expression (conv_nonlin_exp): 1-5           [1, 40, 1101, 1]          [1, 40, 1101, 1]          --                        --
      ├─AvgPool2d (pool): 1-6                       [1, 40, 1101, 1]          [1, 40, 69, 1]            --                        [75, 1]
      ├─SafeLog (pool_nonlin_exp): 1-7              [1, 40, 69, 1]            [1, 40, 69, 1]            --                        --
      ├─Dropout (drop): 1-8                         [1, 40, 69, 1]            [1, 40, 69, 1]            --                        --
      ├─Sequential (final_layer): 1-9               [1, 40, 69, 1]            [1, 4]                    --                        --
      │    └─Conv2d (conv_classifier): 2-1          [1, 40, 69, 1]            [1, 4, 1, 1]              11,044                    [69, 1]
      │    └─SqueezeFinalOutput (squeeze): 2-2      [1, 4, 1, 1]              [1, 4]                    --                        --
      │    │    └─Rearrange (squeeze): 3-1          [1, 4, 1, 1]              [1, 4, 1]                 --                        --
      =================================================================================================================================================
      Total params: 47,364
      Trainable params: 47,364
      Non-trainable params: 0
      Total mult-adds (Units.MEGABYTES): 0.01
      =================================================================================================================================================
      Input size (MB): 0.10
      Forward/backward pass size (MB): 0.35
      Params size (MB): 0.04
      Estimated Total Size (MB): 0.50
      =================================================================================================================================================,
    )</pre></div> </div></div></div></div></div></div></div></div></div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 313-318

Analysing the best fit
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Next, just perform an analysis of the best fit, and the parameters,
remembering the order that was adjusted.

.. GENERATED FROM PYTHON SOURCE LINES 318-341

.. code-block:: Python


    import numpy as np
    import pandas as pd

    search_results = pd.DataFrame(search.cv_results_)

    best_run = search_results[search_results["rank_test_score"] == 1].squeeze()
    best_aug = best_run["params"]
    validation_score = np.around(best_run["mean_test_score"] * 100, 2).mean()
    training_score = np.around(best_run["mean_train_score"] * 100, 2).mean()

    report_message = (
        "Best augmentation is saved in best_aug which gave a mean validation accuracy"
        + "of {}% (train accuracy of {}%).".format(validation_score, training_score)
    )

    print(report_message)

    eval_X = SliceDataset(eval_set, idx=0)
    eval_y = SliceDataset(eval_set, idx=1)
    score = search.score(eval_X, eval_y)
    print(f"Eval accuracy is {score * 100:.2f}%.")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Best augmentation is saved in best_aug which gave a mean validation accuracyof 25.0% (train accuracy of 25.0%).
    Eval accuracy is 25.00%.




.. GENERATED FROM PYTHON SOURCE LINES 342-344

Plot results
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. GENERATED FROM PYTHON SOURCE LINES 344-361

.. code-block:: Python


    import matplotlib.pyplot as plt

    fig, ax = plt.subplots()
    search_results.plot.bar(
        x="param_iterator_train__transforms",
        y="mean_train_score",
        yerr="std_train_score",
        rot=45,
        color=["C0", "C0", "C1", "C1", "C2", "C2"],
        legend=None,
        ax=ax,
    )
    ax.set_xlabel("Data augmentation strategy")
    ax.set_ylim(0.2, 0.32)
    plt.tight_layout()




.. image-sg:: /auto_examples/advanced_training/images/sphx_glr_plot_data_augmentation_search_001.png
   :alt: plot data augmentation search
   :srcset: /auto_examples/advanced_training/images/sphx_glr_plot_data_augmentation_search_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 362-374

References
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


.. [1] Rommel, C., Paillard, J., Moreau, T., & Gramfort, A. (2022)
       Data augmentation for learning predictive models on EEG:
       a systematic comparison. https://arxiv.org/abs/2206.14483
.. [2] Banville, H., Chehab, O., Hyvärinen, A., Engemann, D. A., & Gramfort, A. (2021).
       Uncovering the structure of clinical EEG signals with self-supervised learning.
       Journal of Neural Engineering, 18(4), 046020.

.. include:: /links.inc


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 35.394 seconds)

**Estimated memory usage:**  509 MB


.. _sphx_glr_download_auto_examples_advanced_training_plot_data_augmentation_search.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_data_augmentation_search.ipynb <plot_data_augmentation_search.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_data_augmentation_search.py <plot_data_augmentation_search.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_data_augmentation_search.zip <plot_data_augmentation_search.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
