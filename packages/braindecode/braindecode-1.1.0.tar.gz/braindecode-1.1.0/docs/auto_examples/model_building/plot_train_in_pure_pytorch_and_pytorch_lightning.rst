
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/model_building/plot_train_in_pure_pytorch_and_pytorch_lightning.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_model_building_plot_train_in_pure_pytorch_and_pytorch_lightning.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_model_building_plot_train_in_pure_pytorch_and_pytorch_lightning.py:

.. _train-model-in-pytorch:

Training a Braindecode model in PyTorch
=======================================

This tutorial shows you how to train a Braindecode model with PyTorch. The data
preparation and model instantiation steps are identical to that of the tutorial
:ref:`train-test-tune-model`.

We will use the BCIC IV 2a dataset as a showcase example.

The methods shown can be applied to any standard supervised trial-based decoding setting.
This tutorial will include additional parts of code like loading and preprocessing,
defining a model, and other details which are not exclusive to this page (compare
:ref:`bcic-iv-2a-moabb-trial`). Therefore we
will not further elaborate on these parts and you can feel free to skip them.

The goal of this tutorial is to present braindecode in the PyTorch perceptive.

.. contents:: This example covers:
   :local:
   :depth: 2

.. GENERATED FROM PYTHON SOURCE LINES 27-52

Why should I care about model evaluation?
-----------------------------------------
Short answer: To produce reliable results!

In machine learning, we usually follow the scheme of splitting the
data into two parts, training and testing sets. It sounds like a
simple division, right? But the story does not end here.

While developing a ML model you usually have to adjust and tune
hyperparameters of your model or pipeline (e.g., number of layers,
learning rate, number of epochs). Deep learning models usually have
many free parameters; they could be considered complex models with
many degrees of freedom. If you kept using the test dataset to
evaluate your adjustmentyou would run into data leakage.

This means that if you use the test set to adjust the hyperparameters
of your model, the model implicitly learns or memorizes the test set.
Therefore, the trained model is no longer independent of the test set
(even though it was never used for training explicitly!).
If you perform any hyperparameter tuning, you need a third split,
the so-called validation set.

This tutorial shows the three basic schemes for training and evaluating
the model as well as two methods to tune your hyperparameters.


.. GENERATED FROM PYTHON SOURCE LINES 54-62

.. warning::
   You might recognize that the accuracy gets better throughout
   the experiments of this tutorial. The reason behind that is that
   we always use the same model with the same parameters in every
   segment to keep the tutorial short and readable. If you do your
   own experiments you always have to reinitialize the model before
   training.


.. GENERATED FROM PYTHON SOURCE LINES 64-67

Loading, preprocessing, defining a model, etc.
----------------------------------------------


.. GENERATED FROM PYTHON SOURCE LINES 70-73

Loading the Dataset Structure
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Here, we have a data structure with equal behavior to the Pytorch Dataset.

.. GENERATED FROM PYTHON SOURCE LINES 73-79

.. code-block:: Python


    from braindecode.datasets import MOABBDataset

    subject_id = 3
    dataset = MOABBDataset(dataset_name="BNCI2014_001", subject_ids=[subject_id])








.. GENERATED FROM PYTHON SOURCE LINES 80-83

Preprocessing, the offline transformation of the raw dataset
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


.. GENERATED FROM PYTHON SOURCE LINES 83-115

.. code-block:: Python


    import numpy as np

    from braindecode.preprocessing import (
        Preprocessor,
        exponential_moving_standardize,
        preprocess,
    )

    low_cut_hz = 4.0  # low cut frequency for filtering
    high_cut_hz = 38.0  # high cut frequency for filtering
    # Parameters for exponential moving standardization
    factor_new = 1e-3
    init_block_size = 1000

    transforms = [
        Preprocessor("pick_types", eeg=True, meg=False, stim=False),  # Keep EEG sensors
        Preprocessor(
            lambda data, factor: np.multiply(data, factor),  # Convert from V to uV
            factor=1e6,
        ),
        Preprocessor("filter", l_freq=low_cut_hz, h_freq=high_cut_hz),  # Bandpass filter
        Preprocessor(
            exponential_moving_standardize,  # Exponential moving standardization
            factor_new=factor_new,
            init_block_size=init_block_size,
        ),
    ]

    # Transform the data
    preprocess(dataset, transforms, n_jobs=-1)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /Users/baristim/Projects/braindecode-1/braindecode/preprocessing/preprocess.py:71: UserWarning: Preprocessing choices with lambda functions cannot be saved.
      warn("Preprocessing choices with lambda functions cannot be saved.")

    <braindecode.datasets.moabb.MOABBDataset object at 0x31a795400>



.. GENERATED FROM PYTHON SOURCE LINES 116-119

Cut Compute Windows
~~~~~~~~~~~~~~~~~~~


.. GENERATED FROM PYTHON SOURCE LINES 119-138

.. code-block:: Python


    from braindecode.preprocessing import create_windows_from_events

    trial_start_offset_seconds = -0.5
    # Extract sampling frequency, check that they are same in all datasets
    sfreq = dataset.datasets[0].raw.info["sfreq"]
    assert all([ds.raw.info["sfreq"] == sfreq for ds in dataset.datasets])
    # Calculate the trial start offset in samples.
    trial_start_offset_samples = int(trial_start_offset_seconds * sfreq)

    # Create windows using braindecode function for this. It needs parameters to define how
    # trials should be used.
    windows_dataset = create_windows_from_events(
        dataset,
        trial_start_offset_samples=trial_start_offset_samples,
        trial_stop_offset_samples=0,
        preload=True,
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
    Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
    Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
    Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
    Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
    Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
    Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
    Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
    Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
    Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
    Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
    Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']




.. GENERATED FROM PYTHON SOURCE LINES 139-142

Create Pytorch model
~~~~~~~~~~~~~~~~~~~~


.. GENERATED FROM PYTHON SOURCE LINES 142-177

.. code-block:: Python


    import torch

    from braindecode.models import ShallowFBCSPNet
    from braindecode.util import set_random_seeds

    cuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it
    device = "cuda" if cuda else "cpu"
    if cuda:
        torch.backends.cudnn.benchmark = True
    seed = 20200220
    set_random_seeds(seed=seed, cuda=cuda)

    n_classes = 4
    classes = list(range(n_classes))
    # Extract number of chans and time steps from dataset
    n_chans = windows_dataset[0][0].shape[0]
    n_times = windows_dataset[0][0].shape[1]

    # The ShallowFBCSPNet is a `nn.Sequential` model

    model = ShallowFBCSPNet(
        n_chans=n_chans,
        n_outputs=n_classes,
        n_times=n_times,
        final_conv_length="auto",
    )

    # Display torchinfo table describing the model
    print(model)

    # Send model to GPU
    if cuda:
        model.cuda()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    =================================================================================================================================================
    Layer (type (var_name):depth-idx)             Input Shape               Output Shape              Param #                   Kernel Shape
    =================================================================================================================================================
    ShallowFBCSPNet (ShallowFBCSPNet)             [1, 22, 1125]             [1, 4]                    --                        --
    ├─Ensure4d (ensuredims): 1-1                  [1, 22, 1125]             [1, 22, 1125, 1]          --                        --
    ├─Rearrange (dimshuffle): 1-2                 [1, 22, 1125, 1]          [1, 1, 1125, 22]          --                        --
    ├─CombinedConv (conv_time_spat): 1-3          [1, 1, 1125, 22]          [1, 40, 1101, 1]          36,240                    --
    ├─BatchNorm2d (bnorm): 1-4                    [1, 40, 1101, 1]          [1, 40, 1101, 1]          80                        --
    ├─Expression (conv_nonlin_exp): 1-5           [1, 40, 1101, 1]          [1, 40, 1101, 1]          --                        --
    ├─AvgPool2d (pool): 1-6                       [1, 40, 1101, 1]          [1, 40, 69, 1]            --                        [75, 1]
    ├─SafeLog (pool_nonlin_exp): 1-7              [1, 40, 69, 1]            [1, 40, 69, 1]            --                        --
    ├─Dropout (drop): 1-8                         [1, 40, 69, 1]            [1, 40, 69, 1]            --                        --
    ├─Sequential (final_layer): 1-9               [1, 40, 69, 1]            [1, 4]                    --                        --
    │    └─Conv2d (conv_classifier): 2-1          [1, 40, 69, 1]            [1, 4, 1, 1]              11,044                    [69, 1]
    │    └─SqueezeFinalOutput (squeeze): 2-2      [1, 4, 1, 1]              [1, 4]                    --                        --
    │    │    └─Rearrange (squeeze): 3-1          [1, 4, 1, 1]              [1, 4, 1]                 --                        --
    =================================================================================================================================================
    Total params: 47,364
    Trainable params: 47,364
    Non-trainable params: 0
    Total mult-adds (Units.MEGABYTES): 0.01
    =================================================================================================================================================
    Input size (MB): 0.10
    Forward/backward pass size (MB): 0.35
    Params size (MB): 0.04
    Estimated Total Size (MB): 0.50
    =================================================================================================================================================




.. GENERATED FROM PYTHON SOURCE LINES 178-181

How to train and evaluate your model
------------------------------------


.. GENERATED FROM PYTHON SOURCE LINES 183-186

Split dataset into train and test
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


.. GENERATED FROM PYTHON SOURCE LINES 188-199

We can easily split the dataset using additional info stored in the
description attribute, in this case the ``session`` column. We
select ``0train`` for training and ``1test`` for testing.
For other datasets, you might have to choose another column.

.. note::
   No matter which of the three schemes you use, this initial
   two-fold split into train_set and test_set always remains the same.
   Remember that you are not allowed to use the test_set during any
   stage of training or tuning.


.. GENERATED FROM PYTHON SOURCE LINES 199-204

.. code-block:: Python


    splitted = windows_dataset.split("session")
    train_set = splitted["0train"]  # Session train
    test_set = splitted["1test"]  # Session evaluation








.. GENERATED FROM PYTHON SOURCE LINES 205-210

Option 1: Pure PyTorch training loop
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. image:: https://upload.wikimedia.org/wikipedia/commons/9/96/Pytorch_logo.png
   :alt: Pytorch logo

.. GENERATED FROM PYTHON SOURCE LINES 213-231

``model`` is an instance of :class:`torch.nn.Module`,
and can as such be trained using PyTorch optimization capabilities.
The following training scheme is simple as the dataset is only
split into two distinct sets (``train_set`` and ``test_set``).
This scheme uses no separate validation split and should only be
used for the final evaluation of the (previously!) found
hyperparameters configuration.

.. warning::
   If you make any use of the ``test_set`` during training
   (e.g. by using EarlyStopping) there will be data leakage
   which will make the reported generalization capability/decoding
   performance of your model less credible.

.. warning::
   The parameter values showcased here for optimizing the network are
   chosen to make this tutorial fast to run and build. Real-world values
   would be higher, especially when it comes to n_epochs.

.. GENERATED FROM PYTHON SOURCE LINES 231-241

.. code-block:: Python


    from torch.nn import Module
    from torch.optim.lr_scheduler import LRScheduler
    from torch.utils.data import DataLoader

    lr = 0.0625 * 0.01
    weight_decay = 0
    batch_size = 64
    n_epochs = 2








.. GENERATED FROM PYTHON SOURCE LINES 242-245

The following method runs one training epoch over the dataloader for the
given model. It needs a loss function, optimization algorithm, and
learning rate updating callback.

.. GENERATED FROM PYTHON SOURCE LINES 245-293

.. code-block:: Python

    from tqdm import tqdm

    # Define a method for training one epoch


    def train_one_epoch(
        dataloader: DataLoader,
        model: Module,
        loss_fn,
        optimizer,
        scheduler: LRScheduler,
        epoch: int,
        device,
        print_batch_stats=True,
    ):
        model.train()  # Set the model to training mode
        train_loss, correct = 0.0, 0.0

        progress_bar = tqdm(
            enumerate(dataloader), total=len(dataloader), disable=not print_batch_stats
        )

        for batch_idx, (X, y, _) in progress_bar:
            X, y = X.to(device), y.to(device)
            optimizer.zero_grad()
            pred = model(X)
            loss = loss_fn(pred, y)
            loss.backward()
            optimizer.step()  # update the model weights
            optimizer.zero_grad()

            train_loss += loss.item()
            correct += (pred.argmax(1) == y).sum().item()

            if print_batch_stats:
                progress_bar.set_description(
                    f"Epoch {epoch}/{n_epochs}, "
                    f"Batch {batch_idx + 1}/{len(dataloader)}, "
                    f"Loss: {loss.item():.6f}"
                )

        # Update the learning rate
        scheduler.step()

        correct /= len(dataloader.dataset)
        return train_loss / len(dataloader), correct









.. GENERATED FROM PYTHON SOURCE LINES 294-296

Very similarly, the evaluation function loops over the entire dataloader
and accumulate the metrics, but doesn't update the model weights.

.. GENERATED FROM PYTHON SOURCE LINES 296-365

.. code-block:: Python



    @torch.no_grad()
    def test_model(dataloader: DataLoader, model: Module, loss_fn, print_batch_stats=True):
        size = len(dataloader.dataset)
        n_batches = len(dataloader)
        model.eval()  # Switch to evaluation mode
        test_loss, correct = 0.0, 0.0

        if print_batch_stats:
            progress_bar = tqdm(enumerate(dataloader), total=len(dataloader))
        else:
            progress_bar = enumerate(dataloader)

        for batch_idx, (X, y, _) in progress_bar:
            X, y = X.to(device), y.to(device)
            pred = model(X)
            batch_loss = loss_fn(pred, y).item()

            test_loss += batch_loss
            correct += (pred.argmax(1) == y).type(torch.float).sum().item()

            if print_batch_stats:
                progress_bar.set_description(
                    f"Batch {batch_idx + 1}/{len(dataloader)}, Loss: {batch_loss:.6f}"
                )

        test_loss /= n_batches
        correct /= size

        print(f"Test Accuracy: {100 * correct:.1f}%, Test Loss: {test_loss:.6f}\n")
        return test_loss, correct


    # Define the optimization
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs - 1)
    # Define the loss function
    # We used the NNLoss function, which expects log probabilities as input
    # (which is the case for our model output)
    loss_fn = torch.nn.CrossEntropyLoss()

    # train_set and test_set are instances of torch Datasets, and can seamlessly be
    # wrapped in data loaders.
    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_set, batch_size=batch_size)

    for epoch in range(1, n_epochs + 1):
        print(f"Epoch {epoch}/{n_epochs}: ", end="")

        train_loss, train_accuracy = train_one_epoch(
            train_loader,
            model,
            loss_fn,
            optimizer,
            scheduler,
            epoch,
            device,
        )

        test_loss, test_accuracy = test_model(test_loader, model, loss_fn)

        print(
            f"Train Accuracy: {100 * train_accuracy:.2f}%, "
            f"Average Train Loss: {train_loss:.6f}, "
            f"Test Accuracy: {100 * test_accuracy:.1f}%, "
            f"Average Test Loss: {test_loss:.6f}\n"
        )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Epoch 1/2:       0%|          | 0/5 [00:00<?, ?it/s]    Epoch 1/2, Batch 1/5, Loss: 1.596890:   0%|          | 0/5 [00:00<?, ?it/s]    Epoch 1/2, Batch 2/5, Loss: 1.547558:   0%|          | 0/5 [00:00<?, ?it/s]    Epoch 1/2, Batch 2/5, Loss: 1.547558:  40%|████      | 2/5 [00:00<00:00, 14.03it/s]    Epoch 1/2, Batch 3/5, Loss: 1.804696:  40%|████      | 2/5 [00:00<00:00, 14.03it/s]    Epoch 1/2, Batch 4/5, Loss: 1.587122:  40%|████      | 2/5 [00:00<00:00, 14.03it/s]    Epoch 1/2, Batch 4/5, Loss: 1.587122:  80%|████████  | 4/5 [00:00<00:00, 14.08it/s]    Epoch 1/2, Batch 5/5, Loss: 1.724403:  80%|████████  | 4/5 [00:00<00:00, 14.08it/s]    Epoch 1/2, Batch 5/5, Loss: 1.724403: 100%|██████████| 5/5 [00:00<00:00, 15.24it/s]
      0%|          | 0/5 [00:00<?, ?it/s]    Batch 1/5, Loss: 5.938163:   0%|          | 0/5 [00:00<?, ?it/s]    Batch 2/5, Loss: 6.778563:   0%|          | 0/5 [00:00<?, ?it/s]    Batch 3/5, Loss: 6.089133:   0%|          | 0/5 [00:00<?, ?it/s]    Batch 4/5, Loss: 6.277502:   0%|          | 0/5 [00:00<?, ?it/s]    Batch 4/5, Loss: 6.277502:  80%|████████  | 4/5 [00:00<00:00, 32.72it/s]    Batch 5/5, Loss: 6.813708:  80%|████████  | 4/5 [00:00<00:00, 32.72it/s]    Batch 5/5, Loss: 6.813708: 100%|██████████| 5/5 [00:00<00:00, 35.59it/s]
    Test Accuracy: 25.0%, Test Loss: 6.379414

    Train Accuracy: 24.65%, Average Train Loss: 1.652134, Test Accuracy: 25.0%, Average Test Loss: 6.379414

    Epoch 2/2:       0%|          | 0/5 [00:00<?, ?it/s]    Epoch 2/2, Batch 1/5, Loss: 1.500624:   0%|          | 0/5 [00:00<?, ?it/s]    Epoch 2/2, Batch 2/5, Loss: 1.243209:   0%|          | 0/5 [00:00<?, ?it/s]    Epoch 2/2, Batch 2/5, Loss: 1.243209:  40%|████      | 2/5 [00:00<00:00, 14.70it/s]    Epoch 2/2, Batch 3/5, Loss: 1.386828:  40%|████      | 2/5 [00:00<00:00, 14.70it/s]    Epoch 2/2, Batch 4/5, Loss: 1.110624:  40%|████      | 2/5 [00:00<00:00, 14.70it/s]    Epoch 2/2, Batch 4/5, Loss: 1.110624:  80%|████████  | 4/5 [00:00<00:00, 14.70it/s]    Epoch 2/2, Batch 5/5, Loss: 1.063915:  80%|████████  | 4/5 [00:00<00:00, 14.70it/s]    Epoch 2/2, Batch 5/5, Loss: 1.063915: 100%|██████████| 5/5 [00:00<00:00, 16.13it/s]
      0%|          | 0/5 [00:00<?, ?it/s]    Batch 1/5, Loss: 4.891396:   0%|          | 0/5 [00:00<?, ?it/s]    Batch 2/5, Loss: 5.591525:   0%|          | 0/5 [00:00<?, ?it/s]    Batch 3/5, Loss: 5.007288:   0%|          | 0/5 [00:00<?, ?it/s]    Batch 4/5, Loss: 5.203502:   0%|          | 0/5 [00:00<?, ?it/s]    Batch 4/5, Loss: 5.203502:  80%|████████  | 4/5 [00:00<00:00, 32.38it/s]    Batch 5/5, Loss: 5.639622:  80%|████████  | 4/5 [00:00<00:00, 32.38it/s]    Batch 5/5, Loss: 5.639622: 100%|██████████| 5/5 [00:00<00:00, 35.00it/s]
    Test Accuracy: 25.0%, Test Loss: 5.266667

    Train Accuracy: 39.58%, Average Train Loss: 1.261040, Test Accuracy: 25.0%, Average Test Loss: 5.266667





.. GENERATED FROM PYTHON SOURCE LINES 366-370

Option 2: Train it with PyTorch Lightning
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
.. image:: https://upload.wikimedia.org/wikipedia/commons/e/e6/Lightning_Logo_v2.png
   :alt: Pytorch Lightning logo

.. GENERATED FROM PYTHON SOURCE LINES 372-374

Alternatively, `<lightning_>`_ provides a nice interface around torch modules
which integrates the previous logic.

.. GENERATED FROM PYTHON SOURCE LINES 374-421

.. code-block:: Python



    import lightning as L
    from torchmetrics.functional import accuracy


    class LitModule(L.LightningModule):
        def __init__(self, module):
            super().__init__()
            self.module = module
            self.loss = torch.nn.CrossEntropyLoss()

        def training_step(self, batch, batch_idx):
            x, y, _ = batch
            y_hat = self.module(x)
            loss = self.loss(y_hat, y)
            self.log("train_loss", loss)
            return loss

        def test_step(self, batch, batch_idx):
            x, y, _ = batch
            y_hat = self.module(x)
            loss = self.loss(y_hat, y)
            acc = accuracy(y_hat, y, "multiclass", num_classes=4)
            metrics = {"test_acc": acc, "test_loss": loss}
            self.log_dict(metrics)
            return metrics

        def configure_optimizers(self):
            optimizer = torch.optim.AdamW(
                model.parameters(), lr=lr, weight_decay=weight_decay
            )
            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                optimizer, T_max=n_epochs - 1
            )
            return [optimizer], [scheduler]


    # Creating the trainer with max_epochs=2 for demonstration purposes
    trainer = L.Trainer(max_epochs=n_epochs)
    # Create and train the LightningModule
    lit_model = LitModule(model)
    trainer.fit(lit_model, train_loader)

    # After training, you can test the model using the test DataLoader
    trainer.test(dataloaders=test_loader)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
    GPU available: True (mps), used: True
    TPU available: False, using: 0 TPU cores
    HPU available: False, using: 0 HPUs
    /Users/baristim/miniforge3/envs/braindecode-official/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default

      | Name   | Type             | Params | Mode 
    ----------------------------------------------------
    0 | module | ShallowFBCSPNet  | 47.4 K | eval 
    1 | loss   | CrossEntropyLoss | 0      | train
    ----------------------------------------------------
    47.4 K    Trainable params
    0         Non-trainable params
    47.4 K    Total params
    0.189     Total estimated model params size (MB)
    1         Modules in train mode
    15        Modules in eval mode
    /Users/baristim/miniforge3/envs/braindecode-official/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.
    /Users/baristim/miniforge3/envs/braindecode-official/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
    Training: |          | 0/? [00:00<?, ?it/s]    Training:   0%|          | 0/5 [00:00<?, ?it/s]    Epoch 0:   0%|          | 0/5 [00:00<?, ?it/s]     Epoch 0:  20%|██        | 1/5 [00:01<00:07,  0.52it/s]    Epoch 0:  20%|██        | 1/5 [00:01<00:07,  0.52it/s, v_num=0]    Epoch 0:  40%|████      | 2/5 [00:01<00:02,  1.04it/s, v_num=0]    Epoch 0:  40%|████      | 2/5 [00:01<00:02,  1.04it/s, v_num=0]    Epoch 0:  60%|██████    | 3/5 [00:02<00:01,  1.36it/s, v_num=0]    Epoch 0:  60%|██████    | 3/5 [00:02<00:01,  1.36it/s, v_num=0]    Epoch 0:  80%|████████  | 4/5 [00:02<00:00,  1.62it/s, v_num=0]    Epoch 0:  80%|████████  | 4/5 [00:02<00:00,  1.62it/s, v_num=0]    Epoch 0: 100%|██████████| 5/5 [00:02<00:00,  1.80it/s, v_num=0]    Epoch 0: 100%|██████████| 5/5 [00:02<00:00,  1.80it/s, v_num=0]    Epoch 0: 100%|██████████| 5/5 [00:02<00:00,  1.80it/s, v_num=0]    Epoch 0:   0%|          | 0/5 [00:00<?, ?it/s, v_num=0]            Epoch 1:   0%|          | 0/5 [00:00<?, ?it/s, v_num=0]    Epoch 1:  20%|██        | 1/5 [00:00<00:00, 136.57it/s, v_num=0]    Epoch 1:  20%|██        | 1/5 [00:00<00:00, 134.97it/s, v_num=0]    Epoch 1:  40%|████      | 2/5 [00:00<00:00,  7.19it/s, v_num=0]     Epoch 1:  40%|████      | 2/5 [00:00<00:00,  7.19it/s, v_num=0]    Epoch 1:  60%|██████    | 3/5 [00:00<00:00,  5.47it/s, v_num=0]    Epoch 1:  60%|██████    | 3/5 [00:00<00:00,  5.47it/s, v_num=0]    Epoch 1:  80%|████████  | 4/5 [00:00<00:00,  4.88it/s, v_num=0]    Epoch 1:  80%|████████  | 4/5 [00:00<00:00,  4.88it/s, v_num=0]    Epoch 1: 100%|██████████| 5/5 [00:01<00:00,  4.59it/s, v_num=0]    Epoch 1: 100%|██████████| 5/5 [00:01<00:00,  4.59it/s, v_num=0]    Epoch 1: 100%|██████████| 5/5 [00:01<00:00,  4.59it/s, v_num=0]`Trainer.fit` stopped: `max_epochs=2` reached.
    Epoch 1: 100%|██████████| 5/5 [00:01<00:00,  4.07it/s, v_num=0]
    /Users/baristim/miniforge3/envs/braindecode-official/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py:149: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.
    Restoring states from the checkpoint path at /Users/baristim/Projects/braindecode-1/examples/model_building/lightning_logs/version_0/checkpoints/epoch=1-step=10.ckpt
    Loaded model weights from the checkpoint at /Users/baristim/Projects/braindecode-1/examples/model_building/lightning_logs/version_0/checkpoints/epoch=1-step=10.ckpt
    /Users/baristim/miniforge3/envs/braindecode-official/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.
    Testing: |          | 0/? [00:00<?, ?it/s]    Testing:   0%|          | 0/5 [00:00<?, ?it/s]    Testing DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]    Testing DataLoader 0:  20%|██        | 1/5 [00:00<00:00,  4.77it/s]    Testing DataLoader 0:  40%|████      | 2/5 [00:00<00:00,  5.16it/s]    Testing DataLoader 0:  60%|██████    | 3/5 [00:00<00:00,  5.28it/s]    Testing DataLoader 0:  80%|████████  | 4/5 [00:00<00:00,  5.37it/s]    Testing DataLoader 0: 100%|██████████| 5/5 [00:00<00:00,  5.98it/s]    Testing DataLoader 0: 100%|██████████| 5/5 [00:00<00:00,  5.96it/s]
    ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
    ┃        Test metric        ┃       DataLoader 0        ┃
    ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
    │         test_acc          │    0.2951388955116272     │
    │         test_loss         │     2.889047861099243     │
    └───────────────────────────┴───────────────────────────┘

    [{'test_acc': 0.2951388955116272, 'test_loss': 2.889047861099243}]



.. GENERATED FROM PYTHON SOURCE LINES 422-423

.. include:: /links.inc


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 17.874 seconds)

**Estimated memory usage:**  576 MB


.. _sphx_glr_download_auto_examples_model_building_plot_train_in_pure_pytorch_and_pytorch_lightning.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_train_in_pure_pytorch_and_pytorch_lightning.ipynb <plot_train_in_pure_pytorch_and_pytorch_lightning.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_train_in_pure_pytorch_and_pytorch_lightning.py <plot_train_in_pure_pytorch_and_pytorch_lightning.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_train_in_pure_pytorch_and_pytorch_lightning.zip <plot_train_in_pure_pytorch_and_pytorch_lightning.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
