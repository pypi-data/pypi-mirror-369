from __future__ import annotations

import hashlib
import logging
import os
import random
import time
from dataclasses import dataclass
from enum import Enum
from functools import cached_property
from typing import Literal, NamedTuple, cast, overload

from cachetools import TTLCache
from datasets import Dataset
from huggingface_hub import get_safetensors_metadata
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer
from torch import Tensor
from transformers import AutoConfig
from transformers.configuration_utils import PretrainedConfig
from transformers.tokenization_utils_base import PreTrainedTokenizerBase

from ..utils.batching import (
    batch_by_token_length,
    estimate_token_count_from_chars,
    pre_truncate_text,
)
from ..utils.dataset import parse_dataset
from ..utils.fs import dir_context, is_using_blob_storage
from ..utils.progress import OnLogCallback, OnProgressCallback
from ..utils.pydantic import InputType, InputTypeList, Timeseries, Vector
from .embedding_finetuning import (
    EmbeddingTrainingArguments,
    finetune_for_classification,
    finetune_with_batch_triplet_loss,
)
from .embedding_timeseries import (
    TimeseriesEmbeddingGenerator,
    TimeseriesEmbeddingTrainingArguments,
    TS2VecConfig,
)

CACHE_TTL = 2 * 60 * 60  # 2h
CACHE_SIZE = 25000


class EmbeddingModelContext(NamedTuple):
    embeddings: Tensor
    hash: str


class EmbeddingFinetuningMethod(str, Enum):
    CLASSIFICATION = "classification"
    """Fine tune embeddings by adding a logistic regression head and training to predict labels"""

    BATCH_TRIPLET_LOSS = "batch_triplet_loss"
    """Fine tune embeddings via contrastive triplet loss based on batches"""

    # TODO: bring these back once they properly support on progress and can be safely exposed via Lighthouse
    # TRIPLETS = "triplets"
    # """Fine tune embeddings via contrastive triplet loss based on labels"""

    # CLASS_PROXY_LOSS = "class_proxy_loss"
    # """Fine tune embeddings via proxy-based loss, where each class is represented by a learnable proxy."""


@dataclass(frozen=True)
class _Config:
    """Schema for relevant parsed values from the AutoConfig of an embedding model"""

    name: str
    """Either the name of a HuggingFace model or a path to a local saved model"""

    max_seq_length: int
    """The maximum sequence length to use with this model"""

    embedding_dim: int
    """The dimension of the embeddings generated by this model"""

    query_prompt: str | None
    """Optional prompt prefix to use for queries with this model"""

    document_prompt: str | None
    """Optional prompt prefix to use for documents with this model"""

    transductive_context_length: int | None
    """The number of documents to use for the transductive context of contextual embedding models"""

    model_type: str
    """The type of model from the AutoConfig"""

    supports_instructions: bool
    """Whether this model supports instruction-following for embedding generation"""


class FinetunedEmbeddingModelName(BaseModel):
    """A class that represents a finetuned embedding model name.
    This class follows a similar structure to PretrainedEmbeddingModelName but for finetuned models.
    """

    name: str
    storage_path: str

    @property
    def value(self) -> str:
        """The name/value of the finetuned model"""
        return self.name

    @property
    def path(self) -> str:
        """The path to the finetuned model"""
        return self.storage_path

    @property
    def finetuned(self) -> bool:
        """Whether this is a finetuned model. Always returns True."""
        return True


class PretrainedEmbeddingModelName(Enum):
    # This enum is exposed over the lighthouse API, only add models that should appear on the API
    """Names of pretrained embedding models that are supported by OrcaCloud"""

    CLIP_BASE = "CLIP_BASE"
    """[CLIP-L14](https://huggingface.co/sentence-transformers/clip-ViT-L-14) embedding model"""

    GTE_BASE = "GTE_BASE"
    """[Alibaba GTE-Base v1.5](https://huggingface.co/Alibaba-NLP/gte-base-en-v1.5) embedding model"""

    CDE_SMALL = "CDE_SMALL"
    """[CDE-Small](https://huggingface.co/jxm/cde-small-v1) embedding model"""

    DISTILBERT = "DISTILBERT"
    """[DistilBERT](https://huggingface.co/distilbert-base-uncased) embedding model for testing"""

    GTE_SMALL = "GTE_SMALL"
    """[GTE-Small](https://huggingface.co/Supabase/gte-small) embedding model"""

    MXBAI_LARGE = "MXBAI_LARGE"
    """[MXBAI-Large](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1) embedding model"""

    E5_LARGE = "E5_LARGE"
    """[E5-Large-Instruct](https://huggingface.co/intfloat/multilingual-e5-large-instruct) embedding model"""

    QWEN2_1_5B = "QWEN2_1_5B"
    """[Qwen2-1.5B-Instruct](https://huggingface.co/Alibaba-NLP/gte-Qwen2-1.5B-instruct) embedding model"""

    BGE_BASE = "BGE_BASE"
    """[BGE-Base-EN-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5) embedding model"""

    GIST_LARGE = "GIST_LARGE"
    """[GIST-Large](https://huggingface.co/avsolatorio/GIST-large-Embedding-v0) embedding model"""

    @property
    def path(self) -> str:
        # this enum is exposed on the api and thus the names and values have to match, so we add
        # an additional path property that contains the huggingface name
        return {
            "CLIP_BASE": "OrcaDB/clip-ViT-L-14",
            "GTE_BASE": "OrcaDB/gte-base-en-v1.5",
            "CDE_SMALL": "OrcaDB/cde-small-v1",
            "DISTILBERT": "distilbert-base-uncased",
            "GTE_SMALL": "OrcaDB/gte-small",
            "MXBAI_LARGE": "OrcaDB/mxbai-large",
            "E5_LARGE": "OrcaDB/e5-large",
            "QWEN2_1_5B": "OrcaDB/qwen2-1.5b",
            "BGE_BASE": "OrcaDB/bge-base",
            "GIST_LARGE": "OrcaDB/gist-large",
        }[self.value]

    @property
    def finetuned(self) -> bool:
        """Whether this is a finetuned model. Always returns False."""
        return False


class EmbeddingModelMeta(type):
    __instances: dict[str, EmbeddingModel] = {}

    def __getattr__(cls, name: str) -> EmbeddingModel:
        if name in PretrainedEmbeddingModelName.__members__:
            if name not in cls.__instances:
                cls.__instances[name] = EmbeddingModel(PretrainedEmbeddingModelName[name].path)
            return cls.__instances[name]
        else:
            raise AttributeError(f"'{cls.__name__}' object has no attribute '{name}'")


class EmbeddingModel(metaclass=EmbeddingModelMeta):
    """Embedding models for use with memorysets"""

    @staticmethod
    def load_config(
        path: str,
        trust_remote_code: bool = False,
        max_seq_length_override: int | None = None,
        query_prompt_override: str | None = None,
        document_prompt_override: str | None = None,
    ) -> _Config:
        # Convert model name to path if needed
        path = EmbeddingModel._get_model_path(path)

        # TODO: separate timeseries and sentence embedding models into separate classes. For now,
        # we overload path to allow "ts2vec" as a special path that loads a sorta pretrained
        # timeseries embedding model with a default config.
        #
        # To fully customize ts2vec model configs, instantiate `TimeseriesEmbeddingGenerator`
        # directly and save it to disk. Then instantiate the `EmbeddingModel` with the path which
        # will just work because the `TS2VecConfig` is a registered `PretrainedConfig` that
        # can be loaded just like any other HF transformer based model.

        if path == "ts2vec":
            config: PretrainedConfig = (
                TS2VecConfig(max_seq_length=max_seq_length_override)
                if max_seq_length_override is not None
                else TS2VecConfig()
            )
        else:
            with dir_context(path, "read") as location:
                config: PretrainedConfig = AutoConfig.from_pretrained(location, trust_remote_code=trust_remote_code)
        embedding_dim = getattr(config, "embedding_dim", None)
        if embedding_dim is None:
            embedding_dim = getattr(config, "embedding_size", None)
        if embedding_dim is None:
            embedding_dim = getattr(config, "projection_dim", None)
        if embedding_dim is None:
            embedding_dim = getattr(config, "hidden_size", None)
        if embedding_dim is None:
            embedding_dim = getattr(config, "d_model", None)
        if embedding_dim is None:
            embedding_dim = getattr(config, "n_embed", None)
        if embedding_dim is None:
            raise ValueError(f"Could not determine embedding dimension from {config}")
        max_seq_length = getattr(config, "max_seq_length", None)
        if max_seq_length is None and hasattr(config, "text_config"):
            max_seq_length = getattr(config.text_config, "max_position_embeddings", None)
        if max_seq_length is None:
            max_seq_length = getattr(config, "max_position_embeddings", None)
        if max_seq_length is None:
            max_seq_length = getattr(config, "n_positions", None)
        if max_seq_length is None:
            raise ValueError(f"Could not determine max sequence length from {config}")

        # Get prompts from config
        query_prompt = getattr(config, "query_prompt", None)
        document_prompt = getattr(config, "document_prompt", None)

        # Override with user-provided prompts if specified
        if query_prompt_override is not None:
            query_prompt = query_prompt_override
        if document_prompt_override is not None:
            document_prompt = document_prompt_override

        transductive_context_length = getattr(config, "transductive_corpus_size", None)

        # Get supports_instructions from config
        supports_instructions = getattr(config, "supports_instructions", False)

        return _Config(
            name=path,
            embedding_dim=embedding_dim,
            max_seq_length=max_seq_length,
            query_prompt=query_prompt,
            document_prompt=document_prompt,
            transductive_context_length=transductive_context_length,
            model_type=config.model_type or type(config).__name__,
            supports_instructions=supports_instructions,
        )

    @staticmethod
    def _get_cache_key(
        value: InputType,
        context: EmbeddingModelContext | None,
        prompt: str | None = None,
    ) -> str:
        # Generate cache key based on content, context, and prompt
        if isinstance(value, str):
            input_hash = hashlib.md5(value.encode("utf-8")).hexdigest()
        else:
            input_hash = hashlib.md5(value.tobytes()).hexdigest()
        if context is not None:
            input_hash += f"_{context.hash}"
        if prompt is not None:
            prompt_hash = hashlib.md5(prompt.encode("utf-8")).hexdigest()
            input_hash += f"_{prompt_hash}"
        return input_hash

    _instances: dict[tuple, EmbeddingModel] = {}

    @staticmethod
    def _get_model_path(model_name: str) -> str:
        """
        Convert a model name from PretrainedEmbeddingModelName enum to its corresponding path.
        If the input is not a model name, return it unchanged.

        Args:
            model_name: Either a model name from PretrainedEmbeddingModelName enum (e.g. "E5_LARGE")
                       or a direct model path (e.g. "OrcaDB/e5-large")

        Returns:
            The corresponding model path
        """
        if model_name in PretrainedEmbeddingModelName.__members__:
            return PretrainedEmbeddingModelName[model_name].path
        return model_name

    def __new__(
        cls,
        path: str,
        *,
        trust_remote_code: bool = False,
        max_seq_length_override: int | None = None,
        cache_size: int = CACHE_SIZE,
        cache_ttl: int = CACHE_TTL,
    ):
        # Convert model name to path if needed
        path = cls._get_model_path(path)

        cache_key = (
            path,
            max_seq_length_override,
            cache_size,
            cache_ttl,
        )
        if cache_key not in cls._instances:
            cls._instances[cache_key] = super().__new__(cls)
        return cls._instances[cache_key]

    def __init__(
        self,
        path: str,
        *,
        trust_remote_code: bool = False,
        max_seq_length_override: int | None = None,
        cache_size: int = CACHE_SIZE,
        cache_ttl: int = CACHE_TTL,
    ):
        """
        Initialize an embedding model

        Warning:
            Only the models that are available as class properties like `EmbeddingModel.CLIP_BASE` as
            well as fine-tuned versions of them are guaranteed to work.

        Note:
            This class will return singleton instances of the embedding model, i.e. calling it with
            the same arguments will return the same instance.

        Args:
            path: the name of the embedding model or a path to a local saved model
            trust_remote_code: whether to trust remote model implementation code from sources other
                than HuggingFace and Orca's organizations, don't st this to true in a server environment
            max_seq_length_override: optional override for the maximum sequence length (in tokens) that the
                model will accept, if `None` the model's default max sequence length will be used
            cache_size: the size of the cache to use for the embedding model
            cache_ttl: the time to live for the cache
        """
        self.path = path
        self.from_disk = is_using_blob_storage(self.path) or os.path.isdir(self.path)
        self.trust_remote_code = trust_remote_code or self.path.startswith("OrcaDB/") or self.from_disk
        self.config = self.load_config(
            path,
            trust_remote_code=self.trust_remote_code,
            max_seq_length_override=max_seq_length_override,
        )

        if (
            max_seq_length_override
            and self.config.max_seq_length
            and max_seq_length_override > self.config.max_seq_length
        ):
            raise ValueError(
                f"Max sequence length {max_seq_length_override} is greater than what the model supports: {self.config.max_seq_length}"
            )

        self.max_seq_length_override = max_seq_length_override
        self._cache: TTLCache[str, Vector] = TTLCache(maxsize=cache_size, ttl=cache_ttl)

    @property
    def uses_context(self) -> bool:
        return self.config.transductive_context_length is not None

    @property
    def supports_instructions(self) -> bool:
        """Whether this model supports instruction-following for embedding generation"""
        return self.config.supports_instructions

    @property
    def embedding_dim(self) -> int:
        return self.config.embedding_dim

    @property
    def max_seq_length(self) -> int:
        return self.max_seq_length_override or self.config.max_seq_length

    @cached_property
    def num_params(self) -> int:
        """The number of parameters in the model"""
        metadata = get_safetensors_metadata(self.path)
        return sum(metadata.parameter_count.values())

    def __repr__(self) -> str:
        return f"EmbeddingModel({self.path}, embedding_dim={self.embedding_dim}, max_seq_length={self.max_seq_length})"

    __embedder: SentenceTransformer | TimeseriesEmbeddingGenerator | None = None

    @property
    def _embedder(self) -> SentenceTransformer | TimeseriesEmbeddingGenerator:
        if self.__embedder is None:
            with dir_context(self.path, "read", recursive=True) as location:
                # TODO: separate timeseries and sentence embedding models into separate classes
                if self.config.model_type == "ts2vec":
                    self.__embedder = (
                        TimeseriesEmbeddingGenerator(max_seq_length=self.max_seq_length_override)
                        if self.path == "ts2vec"
                        else TimeseriesEmbeddingGenerator.from_pretrained(location)
                    )
                else:
                    # For local directories, we need to use the absolute path to avoid HuggingFace's repo_id validation
                    if os.path.isdir(location):
                        self.__embedder = SentenceTransformer(
                            os.path.abspath(location), trust_remote_code=self.trust_remote_code
                        )
                    else:
                        # Convert model name to path if needed
                        model_path = self._get_model_path(location)
                        self.__embedder = SentenceTransformer(model_path, trust_remote_code=self.trust_remote_code)
                    if self.max_seq_length_override is not None:
                        self.__embedder.max_seq_length = self.max_seq_length_override
        return self.__embedder

    def compute_context(self, values: InputTypeList) -> EmbeddingModelContext:
        """
        Compute the context for contextual embedding models like [CDE](https://huggingface.co/jxm/cde-small-v1)

        Args:
            values: the values of the corpus to construct the context from

        Returns:
            The context tensor for the embedding model that can be passed to the `embed` method
        """
        if self.config.transductive_context_length is None or not isinstance(self._embedder, SentenceTransformer):
            raise ValueError("Context can only be computed for contextual embedding models")

        start_time = time.perf_counter()

        if len(values) > self.config.transductive_context_length:
            sampled_values = random.sample(values, k=self.config.transductive_context_length)
        else:
            sampled_values = random.choices(values, k=self.config.transductive_context_length)

        document_prompt = self.config.document_prompt
        logging.info(
            f"Starting to compute embedding context with {len(sampled_values)} values for model {self.path}. Prompt: '{document_prompt}'"
        )

        embeddings = self._embedder.encode(
            sampled_values,  # type: ignore -- types are wrong, image is accepted here
            prompt=document_prompt,
            convert_to_tensor=True,
            show_progress_bar=False,
        )

        end_time = time.perf_counter()
        logging.info(f"Time taken to compute context for {self.path}: {end_time - start_time:.2f} seconds")

        return EmbeddingModelContext(
            embeddings=embeddings, hash=hashlib.md5(embeddings.cpu().numpy().tobytes(order="C")).hexdigest()
        )

    def _embed(
        self,
        values: InputTypeList,
        *,
        show_progress_bar: bool,
        batch_size: int,
        context_embeddings: Tensor | None,
        prompt: str | None = None,
    ) -> list[Vector]:
        if isinstance(self._embedder, TimeseriesEmbeddingGenerator):
            return self._embedder.encode(cast(list[Timeseries], values), batch_size=batch_size)
        else:
            return list(
                self._embedder.encode(
                    values,  # type: ignore -- types are wrong, image is accepted here
                    show_progress_bar=show_progress_bar,
                    normalize_embeddings=True,
                    batch_size=batch_size,
                    prompt=prompt,
                    dataset_embeddings=context_embeddings,
                )
            )

    @overload
    def embed(
        self,
        values: InputTypeList,
        *,
        show_progress_bar: bool = False,
        batch_size: int = 32,
        context: EmbeddingModelContext | None = None,
        use_cache: bool = True,
        value_kind: Literal["query", "document"] = "query",
        prompt: str | None = None,
    ) -> list[Vector]:
        pass

    @overload
    def embed(
        self,
        values: InputType,
        *,
        show_progress_bar: bool = False,
        batch_size: int = 32,
        context: EmbeddingModelContext | None = None,
        use_cache: bool = True,
        value_kind: Literal["query", "document"] = "query",
        prompt: str | None = None,
    ) -> Vector:
        pass

    def embed(
        self,
        values: InputTypeList | InputType,
        *,
        show_progress_bar: bool = False,
        batch_size: int = 32,
        context: EmbeddingModelContext | None = None,
        use_cache: bool = True,
        value_kind: Literal["query", "document"] = "query",
        prompt: str | None = None,
    ) -> list[Vector] | Vector:
        """
        Generate embeddings for the given input

        Args:
            values: the value or values to encode
            show_progress_bar: whether to show a progress bar
            batch_size: size of the batches to use
            context: optional context tensor to use for contextual embedding models
            value_kind: kind of values to embed, either "query" or "document" to determine potential
                prompts for the model, this is usually just used by memoryset internally
            prompt: override for the default prompt, if this is specified, the value_kind is ignored

        Returns:
            list of embeddings or a single embedding if a scalar was given
        """
        scalar_value = not isinstance(values, list)
        if scalar_value:
            values = [values]
        elif len(values) == 0:
            return []

        if prompt is None:
            if value_kind == "document":
                prompt = self.config.document_prompt
            else:
                prompt = self.config.query_prompt

        # Pre-truncate string inputs once to prevent memory explosion during tokenization
        processed_values = []
        for value in values:
            if isinstance(value, str):
                # Apply pre-truncation based on the model's max sequence length
                # Include space for the prompt if provided
                effective_max_length = self.max_seq_length
                if prompt:
                    # Estimate prompt tokens and reduce available space
                    prompt_token_estimate = estimate_token_count_from_chars(prompt)
                    effective_max_length = max(1, self.max_seq_length - prompt_token_estimate)

                truncated_value = pre_truncate_text(value, effective_max_length)
                processed_values.append(truncated_value)
            else:
                # Non-string values (Images, Timeseries) pass through unchanged
                processed_values.append(value)

        # Use processed_values for the rest of the method
        values = processed_values

        # if caching is disabled, we can just embed the values directly
        if not use_cache:
            return self._embed(
                values,
                show_progress_bar=show_progress_bar,
                batch_size=batch_size,
                context_embeddings=context.embeddings if context is not None else None,
                prompt=prompt,
            )

        # otherwise we first resolve cache hits and collect the new values to embed
        new_values: InputTypeList = []
        new_values_indices: list[int] = []
        new_values_cache_keys: list[str] = []
        all_results: list[tuple[int, Vector]] = []  # (index, result)
        for i, value in enumerate(values):
            cache_key = self._get_cache_key(value, context, prompt)
            result = self._cache.get(cache_key, None)
            if result is not None:
                all_results.append((i, result))
            else:
                new_values.append(value)
                new_values_indices.append(i)
                new_values_cache_keys.append(cache_key)

        # if everything was cached, we can just return the cached results
        if len(new_values) == 0:
            res = [r for _, r in all_results]
            return res[0] if scalar_value else res
        # otherwise, generate embeddings for the new values
        new_results = self._embed(
            new_values,
            show_progress_bar=show_progress_bar,
            batch_size=batch_size,
            context_embeddings=context.embeddings if context is not None else None,
            prompt=prompt,
        )

        # if nothing was cached, we can just store the new results and return them
        if len(all_results) == 0:
            for cache_key, result in zip(new_values_cache_keys, new_results):
                self._cache[cache_key] = result
            return new_results[0] if scalar_value else new_results

        # otherwise, store the new results and return the combined results in given order
        for i, cache_key, result in zip(new_values_indices, new_values_cache_keys, new_results):
            self._cache[cache_key] = result
            all_results.append((i, result))
        all_results.sort(key=lambda x: x[0])
        res = [r for _, r in all_results]
        return res[0] if scalar_value else res

    def smart_batch[T](
        self,
        items: list[T],
        *,
        batch_size: int = 32,
        value_kind: Literal["query", "document"] = "query",
        prompt: str | None = None,
        value_prop: str = "value",
    ) -> list[list[T]]:
        """
        Batch items by token length using dynamic batch sizes to optimize memory usage.

        Groups items by similar token lengths and uses adaptive batch sizing to prevent
        batch poisoning from very long texts that can cause memory spikes during embedding.

        Args:
            items: List of items to batch
            batch_size: Base batch size for medium-length texts
            value_kind: Kind of values to embed, either "query" or "document" to determine potential
                prompts for the model, this is usually just used by memoryset internally
            prompt: Optional prompt to use for the model, if this is specified, the value_kind is ignored
            value_prop: Property name to use for value

        Returns:
            List of batches, where each batch is a list of items grouped by similar token lengths
        """
        # if the items are not strings, use standard batching
        if not isinstance(getattr(items[0], value_prop), str):
            return [items[i : i + batch_size] for i in range(0, len(items), batch_size)]

        if prompt is None:
            if value_kind == "document":
                prompt = self.config.document_prompt
            else:
                prompt = self.config.query_prompt
        return batch_by_token_length(
            items,
            base_batch_size=batch_size,
            max_tokens_per_batch=self.max_seq_length * batch_size,
            prompt=prompt or "",
            value_prop=value_prop,
        )

    @property
    def tokenizer(self) -> PreTrainedTokenizerBase:
        assert not isinstance(self._embedder, TimeseriesEmbeddingGenerator)
        return self._embedder.tokenizer

    def tokenize(self, value: str) -> list[str]:
        """
        Tokenize a value

        Args:
            value: Value to tokenize

        Returns:
            The tokenized value
        """
        return self.tokenizer.tokenize(value)

    def finetune(
        self,
        save_dir: str,
        train_dataset: Dataset,
        eval_dataset: Dataset | None = None,
        value_column: str = "value",
        label_column: str = "label",
        training_args: (
            EmbeddingTrainingArguments | TimeseriesEmbeddingTrainingArguments | None
        ) = None,  # filled with method specific defaults
        method: EmbeddingFinetuningMethod | str = EmbeddingFinetuningMethod.CLASSIFICATION,
        on_progress: OnProgressCallback | None = None,
        on_log: OnLogCallback | None = None,
        seed: int = 42,
    ) -> EmbeddingModel:
        """
        Finetune the embedding model on a given dataset

        Args:
            save_dir: The directory to save the finetuned model to.
            train_dataset: The data to finetune on.
            eval_dataset: The data to evaluate the finetuned model on, if this is `None` a 10%
                holdout from the training data will be used.
            training_args: The training arguments to use for the finetuning. If this is `None`
                sensible defaults will be used based on the finetuning method.
            method: The method to use for finetuning, "triplets" uses a contrastive triplet loss to
                pull embeddings together and push them apart based on labels. "classification" adds
                a logistic regression head to the model and fine-tunes it for classification.
            on_progress: Callback to report progress
            on_log: Callback to report logs
            seed: Random seed to use for reproducibility

        Returns:
            New finetuned embedding model
        """
        if self.path == "OrcaDB/clip-ViT-L-14-768" or self.path == "cde-small-v1-768":
            raise ValueError(f"Finetuning the {self.path} model is not supported")
        # TODO: separate timeseries and sentence embedding models into separate classes
        if self.path == "ts2vec":
            model = TimeseriesEmbeddingGenerator(max_seq_length=self.max_seq_length_override)
            if training_args is not None:
                assert isinstance(training_args, TimeseriesEmbeddingTrainingArguments)
            model.train(
                save_dir=save_dir,
                train_dataset=train_dataset,
                eval_dataset=eval_dataset,
                value_column=value_column,
                training_args=training_args,
                on_progress=on_progress,
                on_log=on_log,
            )
            return EmbeddingModel(save_dir)

        if is_using_blob_storage(self.path):
            raise ValueError("Finetuning models is currently not supported for models in blob storage")

        train_dataset = parse_dataset(train_dataset, value_column=value_column, label_column=label_column)
        if eval_dataset is not None:
            eval_dataset = parse_dataset(eval_dataset, value_column=value_column, label_column=label_column)
        else:
            try:
                split_dataset = train_dataset.train_test_split(test_size=0.1, stratify_by_column="label", seed=seed)
            except ValueError as e:
                if "should be greater or equal to the number of classes" in str(e):
                    logging.info(
                        f"The dataset does not contain enough samples to stratify by label column {label_column}, using random split instead"
                    )
                    split_dataset = train_dataset.train_test_split(test_size=0.1, seed=seed)
                else:
                    raise e
            train_dataset = split_dataset["train"]
            eval_dataset = split_dataset["test"]

        match method:
            case EmbeddingFinetuningMethod.CLASSIFICATION:
                training_args = training_args or EmbeddingTrainingArguments.for_classification()
                finetune_fn = finetune_for_classification
            case EmbeddingFinetuningMethod.BATCH_TRIPLET_LOSS:
                training_args = training_args or EmbeddingTrainingArguments.for_triplet_loss()
                finetune_fn = finetune_with_batch_triplet_loss
            # TODO: bring these back once they properly support on progress and can be safely exposed via Lighthouse
            # case EmbeddingFinetuningMethod.TRIPLETS:
            #     training_args = training_args or EmbeddingTrainingArguments.for_triplet_loss()
            #     finetune_fn = finetune_with_triplets
            # case EmbeddingFinetuningMethod.PROXY:
            #     training_args = training_args or EmbeddingTrainingArguments.for_proxy_loss()
            #     finetune_fn = finetune_with_proxy_loss
            case _:
                raise ValueError(f"Invalid finetuning method: {method}")
        assert isinstance(training_args, EmbeddingTrainingArguments)

        if training_args.max_seq_length is None and self.max_seq_length_override is not None:
            # if the model has a custom max sequence length, use it for finetuning
            training_args.max_seq_length = self.max_seq_length_override

        with dir_context(save_dir, "write", recursive=True) as location:
            finetune_fn(
                base_model_name=self.path,
                output_dir=location,
                train_dataset=train_dataset,
                eval_dataset=eval_dataset,
                training_args=training_args,
                on_progress=on_progress,
                on_log=on_log,
            )

        # return a new embedding model by loading it from the saved directory
        return EmbeddingModel(
            save_dir,
            trust_remote_code=self.trust_remote_code,
            max_seq_length_override=self.max_seq_length_override,
        )
