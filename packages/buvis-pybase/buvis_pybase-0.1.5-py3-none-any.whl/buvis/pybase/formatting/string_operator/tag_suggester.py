"""Tag suggestion module using NLP and zero-shot classification."""

from __future__ import annotations

import string
from collections import Counter
from pathlib import Path
from typing import List, Optional, Tuple

import nltk
import torch
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tag import pos_tag
from nltk.tokenize import word_tokenize
from rake_nltk import Rake
from transformers import pipeline

MIN_KEYWORD_LENGTH = 2


class NLTKPreprocessor:
    """Handles NLTK-related preprocessing tasks."""

    @staticmethod
    def download_nltk_dependencies() -> None:
        """Download required NLTK data packages."""
        dependencies = [
            "punkt",
            "averaged_perceptron_tagger",
            "averaged_perceptron_tagger_eng",
            "stopwords",
            "wordnet",
            "omw-1.4",
            "punkt_tab",
        ]
        for dependency in dependencies:
            nltk.download(dependency, quiet=True)


class KeywordExtractor:
    """Extracts keywords from text using various methods."""

    def __init__(self: KeywordExtractor) -> None:
        self.lemmatizer = WordNetLemmatizer()
        self.rake = Rake()

    def extract_by_frequency(
        self: KeywordExtractor,
        text: str,
        count: int = 20,
    ) -> list[str]:
        """Extract keywords based on frequency of nouns."""
        tokens = word_tokenize(text.lower())
        stop_words = set(stopwords.words("english"))

        filtered_tokens = [
            word
            for word in tokens
            if word not in stop_words and word not in string.punctuation
        ]

        tagged = pos_tag(filtered_tokens)
        nouns = [word for word, pos in tagged if pos.startswith("NN")]

        return [word for word, _ in Counter(nouns).most_common(count)]

    def extract_by_rake(self: KeywordExtractor, text: str) -> list[str]:
        """Extract keywords using RAKE algorithm."""
        self.rake.extract_keywords_from_text(text)
        keywords = self.rake.get_ranked_phrases()

        lemmatized = [
            " ".join(self.lemmatizer.lemmatize(word.lower()) for word in kw.split())
            for kw in keywords
        ]

        unique_keywords = {kw for kw in lemmatized if len(kw) >= MIN_KEYWORD_LENGTH}

        return [
            kw
            for kw in unique_keywords
            if kw.replace(" ", "").isalpha()
            and not any(kw != other and kw in other for other in unique_keywords)
        ]


class TagSuggester:
    """Suggests tags using zero-shot classification."""

    def __init__(
        self: TagSuggester,
        model_name: str = "facebook/bart-large-mnli",
        limit_score: float = 0.75,
    ) -> None:
        self.model_name = model_name
        self.limit_score = limit_score
        self.device = "mps" if torch.backends.mps.is_available() else 0
        self.classifier = pipeline(
            "zero-shot-classification",
            model=model_name,
            device=self.device,
        )
        self.used_tags_path = Path.home() / ".config" / "buvis" / "candidate_tags"

    def _get_used_tags(self: TagSuggester) -> list[str]:
        """Retrieve previously used tags from file."""
        return (
            self.used_tags_path.read_text().split("\n")
            if self.used_tags_path.is_file()
            else []
        )

    def _classify_candidates(
        self: TagSuggester,
        text: str,
        candidates: list[str],
    ) -> list[tuple[str, float]]:
        """Classify candidates using zero-shot classification."""
        if not candidates:
            return []

        result = self.classifier(
            text,
            candidate_labels=candidates,
            multi_label=True,
        )
        return list(zip(result["labels"], result["scores"]))

    def suggest(
        self: TagSuggester,
        text: str,
        candidate_tags: list[str] | None = None,
    ) -> list[str]:
        """Suggest tags for given text."""
        NLTKPreprocessor.download_nltk_dependencies()

        if not candidate_tags:
            extractor = KeywordExtractor()
            candidate_tags = list(
                set(
                    extractor.extract_by_rake(text)
                    + extractor.extract_by_frequency(text),
                ),
            )

        used_tags = self._get_used_tags()
        top_candidates = [
            tag for tag in candidate_tags if tag.replace(" ", "-") in used_tags
        ]
        bottom_candidates = [
            tag for tag in candidate_tags if tag.replace(" ", "-") not in used_tags
        ]

        # Classify and combine results
        top_scores = [
            (label, score + self.limit_score)
            for label, score in self._classify_candidates(text, top_candidates)
        ]
        bottom_scores = self._classify_candidates(text, bottom_candidates)

        all_scores = top_scores + bottom_scores

        return [
            label.replace(" ", "-")
            for label, _ in sorted(
                [
                    (label, score)
                    for label, score in all_scores
                    if score > self.limit_score
                ],
                key=lambda x: x[1],
                reverse=True,
            )
        ]
