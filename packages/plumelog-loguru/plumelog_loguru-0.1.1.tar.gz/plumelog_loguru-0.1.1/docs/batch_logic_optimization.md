# 批量处理逻辑优化说明

## 问题描述

原来的逻辑是"必须等待一个时间间隔才会判断数量"，即：
1. 先等待 `batch_interval_seconds` 时间
2. 如果超时，获取第一条日志
3. 然后尽可能收集更多日志直到达到 `batch_size`
4. 发送批次

这种方式的问题是：即使已经有足够的日志可以发送，也必须等待超时才能开始处理。

## 新的逻辑

现在实现了"达到任一条件就立即上报"的机制：

### 双触发条件
1. **数量触发**：当积累的日志达到 `batch_size` 时，立即发送
2. **时间触发**：当距离上次发送超过 `batch_interval_seconds` 时，发送当前积累的日志

### 具体工作流程

```
开始 -> 初始化空批次 -> 记录最后发送时间

循环:
  1. 计算距离上次发送的时间
  2. 如果已有日志且超时 -> 立即发送当前批次
  3. 否则，尝试从队列获取日志（带剩余超时时间）
  4. 获取到日志后，快速收集更多日志（非阻塞）
  5. 如果达到批量大小 -> 立即发送
  6. 如果超时但有积累的日志 -> 发送当前批次
  7. 继续循环
```

### 优势

1. **响应更快**：达到批量大小立即发送，不需要等待时间间隔
2. **资源利用更好**：减少内存中的日志积累时间
3. **更符合常见实践**：大多数批处理系统都采用这种双条件触发机制
4. **灵活性更高**：可以通过调整 `batch_size` 和 `batch_interval_seconds` 来平衡延迟和吞吐量

### 配置示例

```python
# 高频场景：快速发送，小批次
config = PlumelogSettings(
    batch_size=10,           # 较小批次
    batch_interval_seconds=1.0  # 短时间间隔
)

# 高吞吐场景：大批次，较长间隔
config = PlumelogSettings(
    batch_size=1000,         # 大批次
    batch_interval_seconds=5.0  # 较长时间间隔
)
```

## 测试场景

1. **快速连续日志**：应该在达到批量大小时立即发送
2. **稀疏日志**：应该在时间间隔到达时发送积累的日志  
3. **混合模式**：在不同时间点触发不同的发送条件
