# Model Fingerprint Comparison and Analysis


## Experimentation and Analysis

Broader experiments involving multiple models, fingerprinting methods, and similarity metrics are orchestrated using the `scripts/run_experiment.py` module. This script serves as the primary entry point for generating fingerprints, computing similarity matrices, and persisting their results. It references the global registry and so model IDs in config files refer to IDs in the global registry. 

It can be executed as a command-line tool with various configuration options. For example, to run an experiment based on a configuration file, you would use:

```bash
uv run python -m scripts.run_experiment \
  --config path/to/config.json \
  --use-existing
```

Below is an example of a configuration that could be used to generate the fingerprint type that currently exists in the global registry. When used with the command above, this config generates the fingerprint for models with ids 27, 35, and 193 if they don't already exist, and computes pairwise l1-similarity among the three pairs from the set of three models.

{
    "models": ["27","35","193"],
    "methods": ["input_output"],
    "method_args": {
      "input_output": {
        "n0": 40,
        "target_tokens_path": "helper_data/io_tokens/target_tokens.txt",
        "probe_tokens_path": "helper_data/io_tokens/probe_tokens.txt"
      }
    },
    "metrics": ["l1"]
  }

The `--use-existing` flag tells the script to first check if a fingerprint of the specified type already exists in the registry for a given model. If it does, that fingerprint will be used directly, saving computational resources and time by avoiding regeneration. This is beneficial when you have already generated fingerprints and want to re-run the similarity computation phase with different metrics or analyze a subset of models without re-processing all of them. If the flag is not used, or if a fingerprint doesn't exist, it will be generated as part of the experiment.

Note that only admins may use the `--write-to-registry` flag, which writes a fingerprint to the global registry.


## Model Fingerprint Metrics Analysis
The `scripts/compute_similarity_metrics.py` module provides tools to analyze and visualize the similarity between model fingerprints, grouping models by their families and types to generate statistical metrics.

## Overview

The metrics analysis module provides three main capabilities:

1. **Intrafamily Analysis**: Analyzes similarity between models within the same family, computing statistics (min, max, mean, variance) and generating interactive visualization plots.

2. **Interfamily Analysis**: Analyzes similarity between models of different families, computing statistics for each pair of model families.

3. **Type Pair Analysis**: Analyzes similarity between models of different types within the same family (e.g., Base vs. Quantized, Base vs. Fine-tuned), then groups these pairs by their type combination (e.g., all "Base-Distilled" pairs from different families).


## Usage

### 1. Prepare Model Information

Create a JSON file that provides family and type information for each model that was fingerprinted. The file should contain an array of objects with the following format. The `model_id` field must match the output model id that `scripts/run_experiment.py` uses in its results. 

```json
[
  {
    "model_id": "1",
    "family": "Llama 3",
    "model_type": "Base",
    "display_name": "Llama 3 pretrained"
  },
  {
    "model_id": "2",
    "family": "Llama 3",
    "model_type": "Chat",
    "display_name": "Llama 3 chat"
  },
  ...
]
```


### 2. Run Fingerprinting Experiment

First, run the fingerprinting experiment using the existing `scripts/run_experiment.py` tool, which will generate two JSON files:
- A fingerprints file containing the model fingerprints
- A similarity matrices file containing the computed similarity matrices

### 3. Analyze the Results

Use the `scripts/compute_similarity_metrics.py` tool to analyze the similarity matrices:

```bash
uv run python -m scripts.compute_similarity_metrics \
  --results-file path/to/similarity_matrices.json \
  --model-info path/to/model_info.json \
  --method input_output \
  --metric cosine
```

#### Arguments

- `--results-file`: Path to the similarity matrices JSON file generated by `scripts/run_experiment.py`
- `--model-info`: Path to your model information JSON file
- `--method`: Fingerprinting method to analyze (default: `input_output`)
- `--metric`: Similarity metric to analyze (default: `l1`)
- `--output-dir`: Directory to save outputs

## Outputs

The tool generates several outputs:

1. **JSON Report**: A comprehensive report containing all statistics.

2. **Interactive HTML Visualizations**: Scalable, interactive visualizations of similarity matrices and inter/intra family similarity scores.

3. **Dashboard**: A single HTML dashboard that combines certain visualizations and statistics, with the ability to explore the data regardless of the number of models.

4. **Consolidated Text Reports**: Reports showing model pairs with family and type information, sorted by similarity score and grouped by category (intrafamily, interfamily, type pair).

5. **Console Summary**: Printed summary of key metrics, ordered by mean similarity scores.

## Example Output

### Interactive Similarity Matrices

The similarity matrix visualizations support interactive zooming and panning and hover tooltips showing exact similarity values


### Bar Charts for Mean Similarity

The interactive bar charts display quartile similarity scores for:
- Intrafamily comparisons
- Interfamily comparisons


### Intrafamily Stats Example

```
INTRAFAMILY SIMILARITY (sorted by mean similarity):
Family               Min        Max        Mean       Variance   # Pairs
----------------------------------------------------------------------
Llama                0.9820     0.9973     0.9892     0.0001     6
Phi                  0.9610     0.9780     0.9695     0.0008     3
Gemma                0.9540     0.9820     0.9680     0.0002     6
Mistral              0.9320     0.9670     0.9495     0.0004     10
```

### Interfamily Stats Example

```
INTERFAMILY SIMILARITY (sorted by mean similarity):
Family Pair                   Min        Max        Mean       Variance   # Pairs
--------------------------------------------------------------------------------
Llama vs Phi                 0.8750     0.9120     0.8935     0.0003     12
Llama vs Mistral             0.8420     0.8930     0.8675     0.0005     20
Phi vs Gemma                 0.8150     0.8720     0.8435     0.0007     8
```

### Type Pair Stats Example

```
TYPE PAIR SIMILARITY (sorted by mean similarity):
Type Pair                               Min        Max        Mean       Variance   # Pairs
------------------------------------------------------------------------------------------
Base-Chat                              0.8830     0.9120     0.8975     0.0002     8
Base-Quantized                         0.8420     0.8930     0.8675     0.0005     12
Chat-Quantized                         0.8150     0.8720     0.8435     0.0007     8
```

### Detailed Pair Information

The tool also generates detailed reports showing every model pair with their family and type information, sorted by similarity score. This helps identify which specific models are most similar within or across families and types.

Example:
```
llama-2-7b (Llama/Base) <-> llama-2-13b (Llama/Base): 0.9973
llama-2-7b-chat (Llama/Chat) <-> llama-2-13b-chat (Llama/Chat): 0.9950
llama-2-7b (Llama/Base) <-> llama-2-7b-chat (Llama/Chat): 0.9830
```

## Type Pair Analysis Explained

The type pair analysis works as follows:

1. For each family, it identifies all pairs of different model types (e.g., Base-Chat, Base-Quantized)
2. It then groups these pairs by their type combination across different families
3. For each type combination (e.g., "Base-Chat"), it computes statistics across all families that have this type pair

This allows you to compare how similar, for example, Base models are to Chat models across different model families. This helps answer questions like:

- Are Base-Chat pairs more similar than Base-Quantized pairs across all model families?
- Is there a consistent pattern in how different model types relate to each other?

For example, if we have these models:
- Model A (Llama/Base)
- Model B (Llama/Chat)
- Model C (Phi/Base)
- Model D (Phi/Chat)
- Model E (Gemma/Base)
- Model F (Gemma/Quantized)

The type pair analysis would create these groupings:
- Group 1 "Base-Chat": Contains pairs (A,B) and (C,D)
- Group 2 "Base-Quantized": Contains pair (E,F)

The statistics for the "Base-Chat" group would be computed across the similarity scores of the (A,B) and (C,D) pairs.
