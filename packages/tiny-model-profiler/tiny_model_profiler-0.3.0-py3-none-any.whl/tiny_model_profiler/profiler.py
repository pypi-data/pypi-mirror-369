# -*- coding: utf-8 -*-
"""setup

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gyc6JPeaqQ4RN-NMTqLCthpFlfrq6_4S
"""

import torch
from torchinfo import summary

def profile(model, input_size=(1, 3, 224, 224)):
    """
    Profiles a PyTorch model: parameters, FLOPs, memory usage.

    Args:
        model: torch.nn.Module
        input_size: tuple, e.g. (batch, channels, height, width)
    """
    # Parameter count
    params = sum(p.numel() for p in model.parameters())

    # Memory estimation (float32 = 4 bytes)
    bytes_per_param = 4
    memory_mb = (params * bytes_per_param) / (1024 ** 2)

    # FLOPs estimation using torchinfo
    try:
        info = summary(model, input_size=input_size, verbose=0)
        flops = getattr(info, "total_mult_adds", None)
        if flops is not None:
            # torchinfo returns multiply-adds, FLOPs is usually counted as 2x MACs
            flops = flops * 2
            flops_str = f"{flops / 1e9:.2f} GFLOPs"
        else:
            flops_str = "N/A"
    except Exception as e:
        flops_str = f"Error estimating FLOPs: {e}"

    print(f"Parameters: {params:,}")
    print(f"Estimated Memory: {memory_mb:.2f} MB")
    print(f"FLOPs: {flops_str}")