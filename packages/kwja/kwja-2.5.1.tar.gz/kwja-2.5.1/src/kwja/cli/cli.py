import logging
import os
import sys
from abc import ABC
from collections.abc import Iterator
from enum import Enum
from pathlib import Path
from tempfile import NamedTemporaryFile
from typing import Annotated, Optional, TextIO
from unicodedata import normalize

import hydra
import lightning as L
import torch
import typer
from lightning.pytorch.trainer.states import TrainerFn
from rhoknp import Document, Sentence
from rhoknp.utils.reader import chunk_by_document, chunk_by_sentence

import kwja
from kwja.cli.config import CLIConfig, Device, ModelSize, get_kwja_config_file
from kwja.cli.utils import download_checkpoint, prepare_device
from kwja.datamodule.datamodule import DataModule
from kwja.datamodule.datasets.utils import add_doc_ids, add_sent_ids
from kwja.modules import CharModule, Seq2SeqModule, TypoModule, WordModule
from kwja.utils.constants import TRANSLATION_TABLE
from kwja.utils.logging_util import filter_logs

filter_logs(environment="production")
os.environ["TOKENIZERS_PARALLELISM"] = "false"
logging.basicConfig(format="")

logger = logging.getLogger("kwja_cli")
logger.setLevel(logging.INFO)
app = typer.Typer(pretty_exceptions_show_locals=False)


class InputFormat(str, Enum):
    RAW = "raw"
    JUMANPP = "jumanpp"
    KNP = "knp"


class BaseModuleProcessor(ABC):
    input_format: InputFormat

    def __init__(self, config: CLIConfig, batch_size: int) -> None:
        self.config: CLIConfig = config
        self.device = prepare_device(config.device)
        self.accelerator = self.device.type
        self.model_size: ModelSize = config.model_size
        self.batch_size: int = batch_size
        self.destination = Path(NamedTemporaryFile().name)
        self.module: Optional[L.LightningModule] = None
        self.trainer: Optional[L.Trainer] = None

    def load(self, **writer_kwargs) -> None:
        self.module = self._load_module()
        if self.config.torch_compile is True:
            self.module = torch.compile(self.module)
        self.module.hparams.datamodule.batch_size = self.batch_size  # type: ignore[union-attr]
        self.module.hparams.datamodule.num_workers = self.config.num_workers  # type: ignore[union-attr]

        # TODO: remove this after new checkpoints are released
        self._rename_modules_for_backward_compatibility()

        self.trainer = L.Trainer(
            logger=False,
            callbacks=[
                hydra.utils.instantiate(
                    self.module.hparams.callbacks.prediction_writer,  # type: ignore[union-attr]
                    destination=self.destination,
                    **writer_kwargs,
                ),
                hydra.utils.instantiate(self.module.hparams.callbacks.progress_bar),  # type: ignore[union-attr]
            ],
            accelerator=self.accelerator,
            devices=1,
        )

    def _load_module(self) -> L.LightningModule:
        raise NotImplementedError

    def _rename_modules_for_backward_compatibility(self) -> None:
        assert self.module is not None, "Module is not loaded"
        progress_bar_target = self.module.hparams.callbacks.progress_bar._target_
        if progress_bar_target.startswith("pytorch_lightning"):
            self.module.hparams.callbacks.progress_bar._target_ = progress_bar_target.replace(
                "pytorch_lightning", "lightning.pytorch"
            )
        prediction_writer_target = self.module.hparams.callbacks.prediction_writer._target_
        if prediction_writer_target.startswith("pytorch_lightning"):
            self.module.hparams.callbacks.prediction_writer._target_ = prediction_writer_target.replace(
                "pytorch_lightning", "lightning.pytorch"
            )

    def delete_module_and_trainer(self) -> None:
        del self.module, self.trainer

    def apply_module(self, input_file: Path) -> None:
        datamodule = self._load_datamodule(input_file)
        assert self.trainer is not None
        self.trainer.predict(model=self.module, dataloaders=[datamodule.predict_dataloader()], return_predictions=False)

    def _load_datamodule(self, input_file: Path) -> DataModule:
        raise NotImplementedError

    def export_prediction(self) -> str:
        raise NotImplementedError


class TypoModuleProcessor(BaseModuleProcessor):
    input_format = InputFormat.RAW

    def _load_module(self) -> L.LightningModule:
        logger.info("Loading typo module")
        checkpoint_path: Path = download_checkpoint(module="typo", model_size=self.model_size)
        return TypoModule.fast_load_from_checkpoint(
            checkpoint_path, map_location=self.device, accelerator=self.accelerator
        )

    def _load_datamodule(self, input_file: Path) -> DataModule:
        assert self.module is not None
        self.module.hparams.datamodule.predict.raw_text_file = input_file
        datamodule = DataModule(cfg=self.module.hparams.datamodule)
        datamodule.setup(stage=TrainerFn.PREDICTING)
        return datamodule

    def export_prediction(self) -> str:
        output_text = ""
        for line in self.destination.read_text().strip().split("\n"):
            if line.startswith("# D-ID:"):
                pass
            else:
                output_text += line + "\n"
        return output_text


class CharModuleProcessor(BaseModuleProcessor):
    input_format = InputFormat.RAW

    def _load_module(self) -> L.LightningModule:
        logger.info("Loading char module")
        checkpoint_path: Path = download_checkpoint(module="char", model_size=self.model_size)
        return CharModule.fast_load_from_checkpoint(
            checkpoint_path, map_location=self.device, accelerator=self.accelerator
        )

    def _load_datamodule(self, input_file: Path) -> DataModule:
        assert self.module is not None
        self.module.hparams.datamodule.predict.raw_text_file = input_file
        datamodule = DataModule(cfg=self.module.hparams.datamodule)
        datamodule.setup(stage=TrainerFn.PREDICTING)
        return datamodule

    def export_prediction(self) -> str:
        export_text = ""
        with self.destination.open() as f:
            for juman_text in chunk_by_sentence(f):
                sentence = Sentence.from_jumanpp(juman_text)
                if sentence.comment != "":
                    export_text += sentence.comment + "\n"
                export_text += " ".join(m.text for m in sentence.morphemes) + "\n"
        return export_text


class Seq2SeqModuleProcessor(BaseModuleProcessor):
    input_format = InputFormat.JUMANPP

    def _load_module(self) -> L.LightningModule:
        logger.info("Loading seq2seq module")
        checkpoint_path: Path = download_checkpoint(module="seq2seq", model_size=self.model_size)
        return Seq2SeqModule.fast_load_from_checkpoint(
            checkpoint_path, map_location=self.device, accelerator=self.accelerator
        )

    def _load_datamodule(self, input_file: Path) -> DataModule:
        assert self.module is not None
        self.module.hparams.datamodule.predict.juman_file = input_file
        datamodule = DataModule(cfg=self.module.hparams.datamodule)
        datamodule.setup(stage=TrainerFn.PREDICTING)
        return datamodule

    def export_prediction(self) -> str:
        return self.destination.read_text()


class WordModuleProcessor(BaseModuleProcessor):
    input_format = InputFormat.JUMANPP

    def __init__(self, config: CLIConfig, batch_size: int, from_seq2seq: bool) -> None:
        super().__init__(config, batch_size)
        self.from_seq2seq = from_seq2seq

    def load(self) -> None:  # type: ignore[override]
        super().load(preserve_reading_lemma_canon=self.from_seq2seq)

    def _load_module(self) -> L.LightningModule:
        logger.info("Loading word module")
        checkpoint_path: Path = download_checkpoint(module="word", model_size=self.model_size)
        return WordModule.fast_load_from_checkpoint(
            checkpoint_path, map_location=self.device, accelerator=self.accelerator
        )

    def _load_datamodule(self, input_file: Path) -> DataModule:
        assert self.module is not None
        self.module.hparams.datamodule.predict.juman_file = input_file
        datamodule = DataModule(cfg=self.module.hparams.datamodule)
        datamodule.setup(stage=TrainerFn.PREDICTING)
        return datamodule

    def export_prediction(self) -> str:
        return self.destination.read_text()


class CLIProcessor:
    def __init__(self, config: CLIConfig, tasks: list[str]) -> None:
        self.initial_destination = Path(NamedTemporaryFile(delete=False).name)
        self._task2processors: dict[str, BaseModuleProcessor] = {
            "typo": TypoModuleProcessor(config, config.typo_batch_size),
            "char": CharModuleProcessor(config, config.char_batch_size),
            "seq2seq": Seq2SeqModuleProcessor(config, config.seq2seq_batch_size),
            "word": WordModuleProcessor(
                config,
                config.word_batch_size,
                from_seq2seq="seq2seq" in tasks,
            ),
        }
        self.processors: list[BaseModuleProcessor] = [self._task2processors[task] for task in tasks]

    def load_all_modules(self) -> None:
        for processor in self.processors:
            processor.load()

    def refresh(self) -> None:
        self.initial_destination.unlink(missing_ok=True)
        for processor in self._task2processors.values():
            processor.destination.unlink(missing_ok=True)

    def run(self, input_documents: list[Document], interactive: bool = False) -> str:
        input_documents = [document for document in input_documents if document.text != ""]
        if len(input_documents) == 0:
            return ""
        add_doc_ids(input_documents)
        add_sent_ids(input_documents)
        if self.processors[0].input_format == InputFormat.RAW:
            output_text = ""
            for input_document in input_documents:
                output_text += f"# D-ID:{input_document.doc_id}\n"
                output_text += normalize_text(input_document.text) + "\nEOD\n"
            self.initial_destination.write_text(output_text)
        elif self.processors[0].input_format == InputFormat.JUMANPP:
            self.initial_destination.write_text("".join(document.to_jumanpp() + "\n" for document in input_documents))
        else:
            raise AssertionError  # unreachable

        input_file = self.initial_destination
        for processor in self.processors:
            if processor.module is None or processor.trainer is None:
                processor.load()
            processor.apply_module(input_file)
            input_file = processor.destination
            if interactive is False:
                processor.delete_module_and_trainer()
        return self.processors[-1].export_prediction()


def normalize_text(text: str) -> str:
    # Tokenizers (BertJapaneseTokenizer, DebertaV2Tokenizer, etc.) apply NFKC normalization internally, so
    # there may be inconsistency in number of characters if not applying NFKC normalization in advance
    normalized = normalize("NFKC", text)
    # escape several symbols and delete control characters
    normalized = normalized.translate(TRANSLATION_TABLE)
    # if normalized != text:
    #     typer.echo(f"apply normalization ({text} -> {normalized})", err=True)
    return normalized


def _chunk_by_document(f: TextIO, input_format: InputFormat) -> Iterator[str]:
    if input_format in (InputFormat.JUMANPP, InputFormat.KNP):
        yield from chunk_by_document(f)
    elif input_format == InputFormat.RAW:
        buff: str = ""
        for line in f:
            if line.strip() == "EOD":
                yield buff.rstrip()
                buff = ""
            else:
                buff += line
        if buff.rstrip() != "":
            yield buff.rstrip()
    else:
        raise AssertionError  # unreachable


def _load_document_from_text(text: str, input_format: InputFormat) -> Document:
    if input_format == InputFormat.RAW:
        return Document.from_raw_text(text)
    elif input_format == InputFormat.JUMANPP:
        return Document.from_jumanpp(text)
    elif input_format == InputFormat.KNP:
        return Document.from_knp(text)
    else:
        raise AssertionError  # unreachable


def _version_callback(value: bool) -> None:
    if value is True:
        print(f"KWJA {kwja.__version__}")
        raise typer.Exit


def _tasks_callback(value: str) -> str:
    """sort and validate specified tasks"""
    values: list[str] = [v for v in value.split(",") if v]
    tasks: list[str] = []
    for candidate_task in ("typo", "char", "seq2seq", "word"):
        if candidate_task in values:
            tasks.append(candidate_task)
            values.remove(candidate_task)
    if len(values) == 1:
        raise typer.BadParameter(f"invalid task is specified: {values[0]!r}")
    if len(values) > 1:
        raise typer.BadParameter(f"invalid tasks are specified: {', '.join(repr(v) for v in values)}")
    if len(tasks) == 0:
        raise typer.BadParameter("task must be specified")
    return ",".join(tasks)


@app.command()
def main(
    text: Annotated[Optional[str], typer.Option(help="Text to be analyzed.")] = None,
    filename: list[Path] = typer.Option([], dir_okay=False, help="Files to be analyzed."),
    model_size: Annotated[
        Optional[ModelSize], typer.Option(case_sensitive=False, help="Model size to be used.")
    ] = None,
    device: Annotated[Optional[Device], typer.Option(case_sensitive=False, help="Device to be used.")] = None,
    typo_batch_size: Annotated[Optional[int], typer.Option(help="Batch size for typo module.")] = None,
    char_batch_size: Annotated[Optional[int], typer.Option(help="Batch size for char module.")] = None,
    seq2seq_batch_size: Annotated[Optional[int], typer.Option(help="Batch size for seq2seq module.")] = None,
    word_batch_size: Annotated[Optional[int], typer.Option(help="Batch size for word module.")] = None,
    tasks: Annotated[str, typer.Option(callback=_tasks_callback, help="Tasks to be performed.")] = "char,word",
    _: Annotated[
        Optional[bool],
        typer.Option("--version", callback=_version_callback, is_eager=True, help="Show version and exit."),
    ] = None,
    config_file: Annotated[Optional[Path], typer.Option(help="Path to KWJA config file.")] = None,
    input_format: Annotated[InputFormat, typer.Option(case_sensitive=False, help="Input format.")] = InputFormat.RAW,
) -> None:
    # validate task combination
    specified_tasks: list[str] = tasks.split(",")
    valid_task_combinations: set[tuple[str, ...]] = {
        ("typo",),
        ("typo", "char"),
        ("typo", "char", "seq2seq"),
        ("typo", "char", "word"),
        ("typo", "char", "seq2seq", "word"),
        ("char",),
        ("char", "seq2seq"),
        ("char", "word"),
        ("char", "seq2seq", "word"),
    }
    if input_format in (InputFormat.JUMANPP, InputFormat.KNP):
        valid_task_combinations |= {
            ("seq2seq",),
            ("seq2seq", "word"),
            ("word",),
        }
    if tuple(specified_tasks) not in valid_task_combinations:
        raise typer.BadParameter(
            "task combination is invalid. "
            f"Please specify one of {', '.join(repr(','.join(ts)) for ts in valid_task_combinations)}."
        )
    if input_format in (InputFormat.JUMANPP, InputFormat.KNP):
        if specified_tasks[0] in ("typo", "char"):
            logger.warning("WARNING: with typo or char task, your input text will be treated as raw text.")
        elif specified_tasks[0] in ("seq2seq", "word"):
            logger.warning("WARNING: with seq2seq or word task, your input text will be treated as a word sequence.")

    input_documents: Optional[list[Document]] = None
    if text is not None and len(filename) > 0:
        logger.error("ERROR: Please provide text or filename, not both")
        raise typer.Abort
    elif text is not None:
        input_documents = [_load_document_from_text(text, input_format)]
    elif len(filename) > 0:
        input_documents = []
        for path in filename:
            if path.exists() is False:
                logger.error(f"ERROR: {path} does not exist")
                raise typer.Abort
            with path.open() as f:
                for document_text in _chunk_by_document(f, input_format):
                    input_documents.append(_load_document_from_text(document_text, input_format))
    else:
        pass  # interactive mode

    if config_file is None:
        config_file = get_kwja_config_file()
    if config_file.exists():
        config = CLIConfig.from_yaml(config_file)
    else:
        config = CLIConfig()
    if model_size is not None:
        config.model_size = model_size
    if device is not None:
        config.device = device
    if typo_batch_size is not None:
        config.typo_batch_size = typo_batch_size
    if char_batch_size is not None:
        config.char_batch_size = char_batch_size
    if seq2seq_batch_size is not None:
        config.seq2seq_batch_size = seq2seq_batch_size
    if word_batch_size is not None:
        config.word_batch_size = word_batch_size

    processor = CLIProcessor(config, specified_tasks)

    # Batch mode
    if input_documents is not None:
        output: str = processor.run(input_documents)
        print(output or "EOD\n", end="")
        processor.refresh()
        raise typer.Exit

    # Interactive mode
    processor.load_all_modules()
    print('Type "EOD" in a new line to finish the input.', file=sys.stderr)
    input_text = ""
    while True:
        try:
            input_ = input()
        except EOFError:
            break
        if input_ == "EOD":
            processor.refresh()
            input_document: Document = _load_document_from_text(input_text, input_format)
            output = processor.run([input_document], interactive=True)
            print(output, end="")
            if specified_tasks != ["typo"] or output == "":
                print("EOD")  # To indicate the end of the output.
            input_text = ""
        else:
            input_text += input_ + "\n"


if __name__ == "__main__":
    typer.run(main)
