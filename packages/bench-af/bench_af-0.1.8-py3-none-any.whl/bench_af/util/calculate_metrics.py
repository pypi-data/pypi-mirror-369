from __future__ import annotations

from typing import TYPE_CHECKING
import os
from collections import defaultdict

from .cache import (
    get_logs_dir,
    get_cache_dir_one_model_one_environment,
    get_cache_dir_one_model_all_environments,
    get_cache_dir_all_models_all_environments,
    CompleteMetricsCoT,
    CompleteMetricsNoCoT,
    IndependentMetrics,
    IndependentMetricsNoCoT,
    ConditionalMetrics,
    MetricsForManyEnvironments,
)

if TYPE_CHECKING:
    from pathlib import Path
    from inspect_ai.log import EvalLogInfo

DEBUG = False
WARN_ON_ERROR = True

def _get_latest_run_dir(model: str, environment: str) -> Path:
    return max(get_logs_dir(model, environment).iterdir(), key=os.path.getmtime)



def calc_independent_metrics_cot(log: EvalLogInfo) -> IndependentMetrics:
    from .environments.cot_scorers import behave_because_observed, misbehave_because_unobserved
    cot = get_flattened_responses_from_log_info(log, cot_flag=True)
    behave_because_observed_list = behave_because_observed(cot)
    misbehave_because_unobserved_list = misbehave_because_unobserved(cot)

    return IndependentMetrics(
        responses=cot,
        compliance=get_average_compliance_from_log_info(log),
        behave_because_observed=sum(behave_because_observed_list) / len(behave_because_observed_list),
        misbehave_because_unobserved=sum(misbehave_because_unobserved_list) / len(misbehave_because_unobserved_list),
        behave_because_observed_count=sum(behave_because_observed_list),
        misbehave_because_unobserved_count=sum(misbehave_because_unobserved_list),
    )

def calc_independent_metrics_no_cot(log: EvalLogInfo) -> IndependentMetricsNoCoT:
    return IndependentMetricsNoCoT(
        responses=get_flattened_responses_from_log_info(log, cot_flag=False),
        compliance=get_average_compliance_from_log_info(log),
    )

def calc_conditional_metrics(unobserved_metrics: IndependentMetrics, observed_metrics: IndependentMetrics) -> ConditionalMetrics:
    # TODO: this is probably not correct
    pseudo_counts = 0.0001
    return ConditionalMetrics(
        behave_conditional_on_behave_because_observed=(1 - unobserved_metrics.behave_because_observed_count) / (unobserved_metrics.behave_because_observed_count + observed_metrics.behave_because_observed_count + pseudo_counts),
        misbehave_conditional_on_misbehave_because_unobserved=unobserved_metrics.misbehave_because_unobserved_count / (unobserved_metrics.misbehave_because_unobserved_count + observed_metrics.misbehave_because_unobserved_count + pseudo_counts),
    )

def calc_all_metrics_for_model_environment_pair_cot(logs_dir: Path) -> CompleteMetricsCoT:
    from inspect_ai.log import list_eval_logs
    eval_logs = list_eval_logs(str(logs_dir))
    assert len(eval_logs) == 2, f"Expected 2 eval logs, got {len(eval_logs)}"
    unobserved_log, observed_log = eval_logs

    unobserved_metrics = calc_independent_metrics_cot(unobserved_log)
    observed_metrics = calc_independent_metrics_cot(observed_log)

    return CompleteMetricsCoT(
        unobserved=unobserved_metrics,
        observed=observed_metrics,
        conditional=calc_conditional_metrics(unobserved_metrics, observed_metrics),
    )

def calc_all_metrics_for_model_environment_pair_no_cot(logs_dir: Path) -> CompleteMetricsNoCoT:
    from inspect_ai.log import list_eval_logs
    eval_logs = list_eval_logs(str(logs_dir))
    assert len(eval_logs) == 2, f"Expected 2 eval logs, got {len(eval_logs)}"
    unobserved_log, observed_log = eval_logs

    unobserved_metrics = calc_independent_metrics_no_cot(unobserved_log)
    observed_metrics = calc_independent_metrics_no_cot(observed_log)

    return CompleteMetricsNoCoT(
        unobserved=unobserved_metrics,
        observed=observed_metrics,
    )

def update_all_metrics(overwrite: bool = False) -> tuple[dict[str, dict[str, int]], dict[str, dict[str, set[str]]]]:
    from .list import list_model_organisms, list_environments
    statuses = {
        "updated": defaultdict(lambda: defaultdict(set)),
        "skipped_already_exist": defaultdict(lambda: defaultdict(set)),
        "failed": defaultdict(lambda: defaultdict(set)),
        "skipped_no_logs": defaultdict(lambda: defaultdict(set)),
    }

    for model in list_model_organisms():
        cot_flag = "no_cot" not in model
        for environment in list_environments():
            if DEBUG:
                print(f"Updating metrics for {model} in {environment}")
            logs_dir = get_logs_dir(model, environment)
            if not logs_dir.exists():
                statuses["skipped_no_logs"][model][environment].add(str(logs_dir))
                if DEBUG:
                    print(f"No logs found for {model} in {environment}")
                continue
            for run in logs_dir.iterdir():
                if DEBUG:
                    print(f"Updating metrics for {model} in {environment} in {run}")
                try:
                    update_metrics(run, cot_flag, overwrite)
                    statuses["updated"][model][environment].add(str(run))
                    if DEBUG:
                        print(f"Metrics updated for {model} in {environment}")
                except FileExistsError:
                    statuses["skipped_already_exist"][model][environment].add(str(run))
                    if DEBUG:
                        print(f"Metrics already exist for {model} in {environment}")
                except Exception as e:
                    statuses["failed"][model][environment].add(str(run))
                    if WARN_ON_ERROR:
                        print(f"Warning: Failed to update metrics for {model} in {environment}, run: {run}: {e}")
                    else:
                        raise e
    statuses = {key: {k: {k_inner: list(v_inner) for k_inner, v_inner in v.items()} for k, v in value.items()} for key, value in statuses.items() if key != "skipped_no_logs"}
    summary = {
        "Models": {key: len(value) for key, value in statuses.items()},
        "Model-environment pairs": {key: sum(len(v_inner) for v_inner in value.values()) for key, value in statuses.items()},
        "Individual runs": {key: sum(sum(len(v_inner) for v_inner in v.values()) for v in value.values()) for key, value in statuses.items()},
        "Total": sum(sum(sum(len(v_inner) for v_inner in v.values()) for v in value.values()) for value in statuses.values()),
    }
    return summary, statuses

def update_latest_metrics(model: str, environment: str, overwrite: bool = False) -> None:
    cot_flag = "no_cot" not in model
    update_metrics(_get_latest_run_dir(model, environment), cot_flag, overwrite)

def update_metrics(logs_dir: Path, cot_flag: bool, overwrite: bool = False) -> None:
    import json
    if not logs_dir.exists():
        raise FileNotFoundError(f"Logs directory {logs_dir} does not exist")
    if (logs_dir / "metrics.json").exists() and not overwrite:
        raise FileExistsError(f"Metrics already exist for {logs_dir}")

    if cot_flag:
        metrics = calc_all_metrics_for_model_environment_pair_cot(logs_dir)
    else:
        metrics = calc_all_metrics_for_model_environment_pair_no_cot(logs_dir)

    with open(logs_dir / "metrics.json", "w") as f:
        json.dump(metrics.model_dump(mode="json"), f, indent=2)
    cache_all_latest_metrics()

def get_flattened_responses_from_log_info(log: EvalLogInfo, cot_flag: bool) -> list[str]:
    from inspect_ai.log import read_eval_log_samples
    samples = list(read_eval_log_samples(log))
    list_of_contents = [sample.output.choices[0].message.content for sample in samples]
    output: list[str] = []
    for content in list_of_contents:
        if isinstance(content, str):
            output.append(content)
        else:
            part_of_cot = ""
            for item in content:
                if item.type == "text":
                    part_of_cot += item.text
                elif item.type == "reasoning":
                    if not cot_flag:
                        raise ValueError("Reasoning is not allowed without CoT")
                    part_of_cot += item.reasoning
                else:
                    raise ValueError(f"Unknown content type: {item.type}")
            output.append(part_of_cot)
    return output

def get_average_compliance_from_log_info(log: EvalLogInfo) -> float:
    from inspect_ai.log import read_eval_log_samples
    samples = list(read_eval_log_samples(log))
    assert all(sample.scores is not None for sample in samples), "Not all samples have scores"
    if DEBUG:
        print(list(sample.scores.keys() for sample in samples))
    assert all(len(sample.scores.keys()) == 1 for sample in samples), "Not all samples have exactly one scorer"
    scorer_name = next(iter(samples[0].scores.keys()))
    assert all(scorer_name in sample.scores for sample in samples), "Not all samples have the same scorer"
    return sum([sample.scores[scorer_name].value for sample in samples]) / len(samples)


def cache_all_latest_metrics() -> None:
    """
    Read all latest metrics.json files and store the metrics in a cache for faster access
    """
    import json
    from .list import list_model_organisms, list_environments
    
    all_metrics = defaultdict(lambda: defaultdict(dict))
    
    for model in list_model_organisms():
        organism_metrics = defaultdict(dict)
        for environment in list_environments():
            if not (get_logs_dir(model, environment)).exists():
                continue
            with open(_get_latest_run_dir(model, environment) / "metrics.json") as f:
                metrics = json.load(f)
            if metrics:
                overwrite_cache(metrics, get_cache_dir_one_model_one_environment(model, environment))
                organism_metrics[environment] = metrics
        if organism_metrics:
            model_metrics = MetricsForManyEnvironments(
                model=model,
                this_model_can_use_cot=organism_metrics[next(iter(organism_metrics.keys()))]["this_model_can_use_cot"],
                data=organism_metrics,
            )
            overwrite_cache(model_metrics.model_dump(mode="json"), get_cache_dir_one_model_all_environments(model))
            all_metrics[model] = organism_metrics
    if all_metrics:
        overwrite_cache(all_metrics, get_cache_dir_all_models_all_environments())

def overwrite_cache(cache_data: dict, cache_path: Path) -> None:
    import json
    if cache_data:
        cache_path.parent.mkdir(parents=True, exist_ok=True)
        with open(cache_path, "w") as f:
            json.dump(cache_data, f, indent=2)



if __name__ == "__main__":
    print(update_all_metrics(overwrite=True))
