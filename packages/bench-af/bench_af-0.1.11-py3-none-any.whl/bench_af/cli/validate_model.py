from __future__ import annotations
from typing import TYPE_CHECKING


if TYPE_CHECKING:
    from pathlib import Path
    from typing import Any
    from inspect_ai.log import EvalLog
    from bench_af import Environment, ModelOrganism


def validate_model_organism(
    *,
    model_organism: ModelOrganism,
    environment: Environment,
    raw_eval_logs: bool = True,
    eval_config: dict[str, Any],
    results_dir: Path,
    head: int = 50,
) -> list[EvalLog] | str:
    """
    Validate a model organism on an environment.
    """
    from inspect_ai import eval

    # Check if environment is supported by model organism
    if environment.name not in model_organism.supported_environments:
        print(f"Warning: Environment {environment.name} is not supported by model organism {model_organism.name}")

    # Get the observed task from the environment for validation
    observed_task = environment.get_observed_task(head=head)
    unobserved_task = environment.get_unobserved_task(head=head)
    tools = environment.get_tools()
    model_organism.update_solver(tools=tools)

    eval_logs = eval(
        tasks=[observed_task, unobserved_task],
        model=model_organism.model,
        solver=model_organism.solver,
        tags=["validate_model_organism"],
        log_dir=str(results_dir / "logs"),
        log_format="json",
        display="plain",
        **eval_config,
        max_connections = 20,
    )

    return eval_logs
