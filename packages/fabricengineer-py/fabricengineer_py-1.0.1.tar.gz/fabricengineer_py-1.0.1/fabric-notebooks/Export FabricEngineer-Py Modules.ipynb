{"cells":[{"cell_type":"markdown","source":["# Export fabricengineer-py Modules\n","\n","This notebook retrieves the fabricengineer-py module source code directly from GitHub, allowing you to work without relying on slow-loading Fabric environments in notebooks."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cd9e391f-c2b1-40ab-8e7e-946e7811eeef"},{"cell_type":"markdown","source":["## Parameters"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"449bfe4d-d9a6-4da8-89ce-2a97a9f5466a"},{"cell_type":"code","source":["IS_DEV: bool = True\n","VERSION: str = None\n","LAKEHOUSE_CODE_ABFSS = None\n","MODULES: str = None  # transform.mlv | transform.silver.insertonly | transform.silver.scd2"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"85ef6a09-31a9-4c32-8746-9d93bec4ad32","normalized_state":"finished","queued_time":"2025-08-14T10:08:06.8957008Z","session_start_time":"2025-08-14T10:08:06.8966968Z","execution_start_time":"2025-08-14T10:08:17.9191491Z","execution_finish_time":"2025-08-14T10:08:18.4087901Z","parent_msg_id":"8b3b5d86-1124-4dd6-95e1-9d8646c353bb"},"text/plain":"StatementMeta(, 85ef6a09-31a9-4c32-8746-9d93bec4ad32, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"8b8f42e4-b628-4b1c-9496-9185a845d59c"},{"cell_type":"markdown","source":["## Imports & Constants"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"148778f5-edf9-4c95-bd34-4d5cce44f4eb"},{"cell_type":"code","source":["import requests\n","import json"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"85ef6a09-31a9-4c32-8746-9d93bec4ad32","normalized_state":"finished","queued_time":"2025-08-14T10:08:08.1920656Z","session_start_time":null,"execution_start_time":"2025-08-14T10:08:18.4105493Z","execution_finish_time":"2025-08-14T10:08:18.8156223Z","parent_msg_id":"58a9ce86-5ac8-49cd-ac3f-dc33858d7c9a"},"text/plain":"StatementMeta(, 85ef6a09-31a9-4c32-8746-9d93bec4ad32, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f9f5a6b4-0abe-4a17-a13d-a7641a811e4a"},{"cell_type":"code","source":["MLV_MODULE_NAME = \"transform.mlv\"\n","ETL_INSERTONLY_MODULE_NAME = \"transform.silver.insertonly\"\n","ETL_SCD2_MODULE_NAME = \"transform.silver.scd2\"\n","\n","ALLOWED_MODULES = {\n","    MLV_MODULE_NAME,\n","    ETL_INSERTONLY_MODULE_NAME,\n","    ETL_SCD2_MODULE_NAME\n","}"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"85ef6a09-31a9-4c32-8746-9d93bec4ad32","normalized_state":"finished","queued_time":"2025-08-14T10:08:10.5165465Z","session_start_time":null,"execution_start_time":"2025-08-14T10:08:18.8180534Z","execution_finish_time":"2025-08-14T10:08:19.2301809Z","parent_msg_id":"b30b3891-4830-4e92-b5b3-c237d50172fe"},"text/plain":"StatementMeta(, 85ef6a09-31a9-4c32-8746-9d93bec4ad32, 5, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"55e195ff-626b-4955-9c59-b89ed1e4b7fb"},{"cell_type":"code","source":["if IS_DEV:\n","    VERSION = \"0.2.0\"\n","    MODULES = \"transform.mlv\"\n","    LAKEHOUSE_CODE_ABFSS = f\"abfss://b50e79c4-1140-479c-b5c1-225982ea6783@onelake.dfs.fabric.microsoft.com/c7ab73ce-7e20-45c4-9bbf-004d37143759\"\n","\n","if \",\" in MODULES:\n","    MODULES = MODULES.split(\",\")\n","else:\n","    MODULES = [MODULES]\n","\n","assert all(module in ALLOWED_MODULES for module in MODULES), f\"All modules should be in {allowed_modules}\"\n","\n","print(MODULES)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":10,"statement_ids":[10],"state":"finished","livy_statement_state":"available","session_id":"85ef6a09-31a9-4c32-8746-9d93bec4ad32","normalized_state":"finished","queued_time":"2025-08-14T10:09:01.6820437Z","session_start_time":null,"execution_start_time":"2025-08-14T10:09:01.6832784Z","execution_finish_time":"2025-08-14T10:09:02.0555043Z","parent_msg_id":"4a537723-fcad-43c9-ac8b-d8ace392356c"},"text/plain":"StatementMeta(, 85ef6a09-31a9-4c32-8746-9d93bec4ad32, 10, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["['transform.mlv']\n"]}],"execution_count":8,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e4b3ec6a-9e8d-4f91-8338-c37c9053d8b2"},{"cell_type":"markdown","source":["## Import Module"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"21e4beaa-faeb-4d77-a738-ec26a17645da"},{"cell_type":"code","source":["def save_module_code(name: str, code: str) -> None:\n","    path = f\"{LAKEHOUSE_CODE_ABFSS}/Files/py-packages/fabricengineer/{VERSION}/{name}.py.txt\"\n","    notebookutils.fs.put(path, code, overwrite=True)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":13,"statement_ids":[13],"state":"finished","livy_statement_state":"available","session_id":"85ef6a09-31a9-4c32-8746-9d93bec4ad32","normalized_state":"finished","queued_time":"2025-08-14T10:11:21.2644239Z","session_start_time":null,"execution_start_time":"2025-08-14T10:11:21.2655529Z","execution_finish_time":"2025-08-14T10:11:21.6372091Z","parent_msg_id":"b0362c27-2b75-4df0-89cd-0e889d555e6e"},"text/plain":"StatementMeta(, 85ef6a09-31a9-4c32-8746-9d93bec4ad32, 13, Finished, Available, Finished)"},"metadata":{}}],"execution_count":11,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"080d134c-53f4-4571-9699-312a82f31ba9"},{"cell_type":"code","source":["url = f\"https://raw.githubusercontent.com/enricogoerlitz/fabricengineer-py/refs/tags/{VERSION}/src/fabricengineer/import_module/import_module.py\"\n","resp = requests.get(url)\n","code = resp.text\n","\n","exec(code, globals())  # This provides the 'import_module' function\n","assert code.startswith(\"import requests\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":14,"statement_ids":[14],"state":"finished","livy_statement_state":"available","session_id":"85ef6a09-31a9-4c32-8746-9d93bec4ad32","normalized_state":"finished","queued_time":"2025-08-14T10:11:23.1831081Z","session_start_time":null,"execution_start_time":"2025-08-14T10:11:23.1840921Z","execution_finish_time":"2025-08-14T10:11:23.5527053Z","parent_msg_id":"6af8bb11-6a26-4536-bd79-09d6386f0f62"},"text/plain":"StatementMeta(, 85ef6a09-31a9-4c32-8746-9d93bec4ad32, 14, Finished, Available, Finished)"},"metadata":{}}],"execution_count":12,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"af5b5c0a-ade5-4b25-aff7-2b7de4a7e0c4"},{"cell_type":"code","source":["mlv_module = None\n","scd2_module = None\n","insertonly_module = None\n","\n","if MLV_MODULE_NAME in MODULES:\n","    mlv_module = import_module(MLV_MODULE_NAME, VERSION)\n","    save_module_code(MLV_MODULE_NAME, mlv_module)\n","\n","if ETL_INSERTONLY_MODULE_NAME in MODULES:\n","    insertonly_module = import_module(ETL_INSERTONLY_MODULE_NAME, VERSION)\n","    save_module_code(ETL_INSERTONLY_MODULE_NAME, insertonly_module)\n","\n","if ETL_SCD2_MODULE_NAME in MODULES:\n","    scd2_module = import_module(ETL_SCD2_MODULE_NAME, VERSION)\n","    save_module_code(ETL_SCD2_MODULE_NAME, scd2_module)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":16,"statement_ids":[16],"state":"finished","livy_statement_state":"available","session_id":"85ef6a09-31a9-4c32-8746-9d93bec4ad32","normalized_state":"finished","queued_time":"2025-08-14T10:12:36.9198791Z","session_start_time":null,"execution_start_time":"2025-08-14T10:12:36.9210096Z","execution_finish_time":"2025-08-14T10:12:39.5127993Z","parent_msg_id":"b8ffff0c-41f8-41ba-bcc4-bc84ed2d7198"},"text/plain":"StatementMeta(, 85ef6a09-31a9-4c32-8746-9d93bec4ad32, 16, Finished, Available, Finished)"},"metadata":{}}],"execution_count":14,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"73fee6a4-403b-4ddb-9d39-576eb8b5afac"},{"cell_type":"markdown","source":["## Testing"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"94548e35-5c46-4839-98c0-0167246ca142"},{"cell_type":"code","source":["if IS_DEV:\n","    pass"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":17,"statement_ids":[17],"state":"finished","livy_statement_state":"available","session_id":"85ef6a09-31a9-4c32-8746-9d93bec4ad32","normalized_state":"finished","queued_time":"2025-08-14T10:12:43.786203Z","session_start_time":null,"execution_start_time":"2025-08-14T10:12:43.7872866Z","execution_finish_time":"2025-08-14T10:12:44.1516587Z","parent_msg_id":"2c21b63c-c495-457a-b66b-abc31045c5c2"},"text/plain":"StatementMeta(, 85ef6a09-31a9-4c32-8746-9d93bec4ad32, 17, Finished, Available, Finished)"},"metadata":{}}],"execution_count":15,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"08c2414f-264e-4db2-9a86-043e4741a447"},{"cell_type":"markdown","source":["## Export payload"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"47631fb4-5551-48cd-859b-0238a5ef90f0"},{"cell_type":"code","source":["payload = {\n","    \"modules\": {\n","        MLV_MODULE_NAME: mlv_module,\n","        ETL_SCD2_MODULE_NAME: scd2_module,\n","        ETL_INSERTONLY_MODULE_NAME: insertonly_module\n","    }\n","}\n","\n","print(payload)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":18,"statement_ids":[18],"state":"finished","livy_statement_state":"available","session_id":"85ef6a09-31a9-4c32-8746-9d93bec4ad32","normalized_state":"finished","queued_time":"2025-08-14T10:12:48.9063994Z","session_start_time":null,"execution_start_time":"2025-08-14T10:12:48.9075444Z","execution_finish_time":"2025-08-14T10:12:49.2449443Z","parent_msg_id":"d5308a0b-25ba-4b0b-a4f7-621ad7f15cad"},"text/plain":"StatementMeta(, 85ef6a09-31a9-4c32-8746-9d93bec4ad32, 18, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{'modules': {'transform.mlv': 'from typing import Any\\nfrom pyspark.sql import DataFrame, SparkSession\\n\\n\\ndef to_spark_sql(sql: str) -> str:\\n    return sql \\\\\\n            .replace(\"[\", \"`\") \\\\\\n            .replace(\"]\", \"`\")\\n\\n\\nclass MaterializedLakeView:\\n    def __init__(\\n        self,\\n        lakehouse: str = None,\\n        schema: str = None,\\n        table: str = None,\\n        table_suffix: str = \"_mlv\",\\n        spark_: SparkSession = None,\\n        notebookutils_: Any = None,\\n        **kwargs\\n    ) -> None:\\n        self.init(\\n            lakehouse=lakehouse,\\n            schema=schema,\\n            table=table,\\n            table_suffix=table_suffix,\\n            spark_=spark_,\\n            notebookutils_=notebookutils_,\\n            is_testing_mock=kwargs.get(\"is_testing_mock\", False)\\n        )\\n\\n    def init(\\n        self,\\n        lakehouse: str,\\n        schema: str,\\n        table: str,\\n        table_suffix: str = \"_mlv\",\\n        spark_: SparkSession = None,\\n        notebookutils_: Any = None,\\n        **kwargs\\n    ) -> \\'MaterializedLakeView\\':\\n        \"\"\"Initializes the MaterializedLakeView instance.\\n\\n        Args:\\n            lakehouse (str): The lakehouse name.\\n            schema (str): The schema name.\\n            table (str): The table name.\\n            table_suffix (str, optional): The table suffix. Defaults to \"_mlv\".\\n            spark_ (SparkSession, optional): The SparkSession instance. Defaults to None.\\n            notebookutils_ (Any, optional): The NotebookUtils instance. Defaults to None.\\n            is_testing_mock (bool, optional): Whether the instance is a testing mock. Defaults to False.\\n\\n        Returns:\\n            MaterializedLakeView: The initialized MaterializedLakeView instance.\\n        \"\"\"\\n        self._lakehouse = lakehouse\\n        self._schema = schema\\n        self._table = table\\n        self._table_suffix = table_suffix\\n        self._is_testing_mock = kwargs.get(\"is_testing_mock\", False)\\n\\n        # \\'spark\\' and \\'notebookutils\\' are available in Fabric notebook\\n        self._spark = self._get_init_spark(spark_)\\n        self._notebookutils = self._get_init_notebookutils(notebookutils_)\\n        return self\\n\\n    def _get_init_spark(self, spark_: SparkSession) -> SparkSession | None:\\n        \"\"\"Initializes the SparkSession instance.\\n        If a SparkSession is provided, it is returned. Otherwise, it tries to use the global \\'spark\\' variable.\\n\\n        Args:\\n            spark_ (SparkSession): The SparkSession instance.\\n\\n        Returns:\\n            SparkSession | None: The initialized SparkSession instance or None.\\n        \"\"\"\\n        if isinstance(spark_, SparkSession):\\n            return spark_\\n        try:\\n            if spark is not None:  # noqa: F821 # type: ignore\\n                return spark  # noqa: F821 # type: ignore\\n            return spark_\\n        except Exception:\\n            return None\\n\\n    def _get_init_notebookutils(self, notebookutils_: Any) -> Any | None:\\n        \"\"\"Initializes the NotebookUtils instance.\\n        If a NotebookUtils instance is provided, it is returned. Otherwise, it tries to use the global \\'notebookutils\\' variable.\\n\\n        Args:\\n            notebookutils_ (Any): The NotebookUtils instance.\\n\\n        Returns:\\n            Any | None: The initialized NotebookUtils instance or None.\\n        \"\"\"\\n        if notebookutils_ is not None:\\n            return notebookutils_\\n        try:\\n            if notebookutils is not None:  # noqa: F821 # type: ignore\\n                return notebookutils  # noqa: F821 # type: ignore\\n            return None\\n        except Exception:\\n            return None\\n\\n    @property\\n    def lakehouse(self) -> str:\\n        if self._lakehouse is None:\\n            raise ValueError(\"Lakehouse is not initialized.\")\\n        return self._lakehouse\\n\\n    @property\\n    def schema(self) -> str:\\n        if self._schema is None:\\n            raise ValueError(\"Schema is not initialized.\")\\n        return self._schema\\n\\n    @property\\n    def table(self) -> str:\\n        if self._table is None:\\n            raise ValueError(\"Table is not initialized.\")\\n        return self._table\\n\\n    @property\\n    def table_suffix(self) -> str:\\n        return self._table_suffix\\n\\n    @property\\n    def spark(self) -> SparkSession:\\n        if self._spark is None:\\n            raise ValueError(\"SparkSession is not initialized.\")\\n        return self._spark\\n\\n    @property\\n    def notebookutils(self) -> Any:\\n        if self._notebookutils is None:\\n            raise ValueError(\"NotebookUtils is not initialized.\")\\n        return self._notebookutils\\n\\n    @property\\n    def table_name(self) -> str:\\n        table_suffix = self.table_suffix or \"\"\\n        return f\"{self.table}{table_suffix}\"\\n\\n    @property\\n    def file_path(self) -> str:\\n        path = f\"Files/mlv/{self.lakehouse}/{self.schema}/{self.table_name}.sql.txt\"\\n        return path\\n\\n    @property\\n    def table_path(self) -> str:\\n        table_path = f\"{self.lakehouse}.{self.schema}.{self.table_name}\"\\n        return table_path\\n\\n    @property\\n    def schema_path(self) -> str:\\n        schema_path = f\"{self.lakehouse}.{self.schema}\"\\n        return schema_path\\n\\n    def read_file(self) -> str | None:\\n        \"\"\"Reads the content of the SQL file from the specified lakehouse.\\n        If the file does not exist, it returns None.\\n\\n        Raises:\\n            RuntimeError: If the file cannot be read.\\n\\n        Returns:\\n            str | None: The content of the file or None if it doesn\\'t exist.\\n        \"\"\"\\n        path = self.file_path\\n        try:\\n            if not self.notebookutils.fs.exists(path):\\n                return None\\n            if self._is_testing_mock:\\n                with open(path, \"r\") as file:\\n                    return file.read()\\n            df = self.spark.read.text(path, wholetext=True)\\n            mlv_code = df.collect()[0][0]\\n            return mlv_code\\n        except Exception as e:\\n            raise RuntimeError(f\"Fehler beim Lesen der Datei: {e}\")\\n\\n    def write_file(self, sql: str) -> bool:\\n        \"\"\"Writes the SQL content to the specified file in a lakehouse.\\n        If the file already exists, it will be overwritten.\\n        If the file cannot be written, it raises a RuntimeError.\\n\\n        Args:\\n            sql (str): The SQL content to write.\\n\\n        Raises:\\n            RuntimeError: If the file cannot be written.\\n\\n        Returns:\\n            bool: True if the file was written successfully, False otherwise.\\n        \"\"\"\\n        try:\\n            result = self.notebookutils.fs.put(\\n                file=self.file_path,\\n                content=sql,\\n                overwrite=True\\n            )\\n            return result\\n        except Exception as e:\\n            raise RuntimeError(f\"Fehler beim Schreiben der Datei: {e}\")\\n\\n    def create_schema(self) -> DataFrame | None:\\n        \"\"\"Creates the schema in the lakehouse if it does not exist.\"\"\"\\n        create_schema = f\"CREATE SCHEMA IF NOT EXISTS {self.schema_path}\"\\n        logger.info(create_schema)\\n\\n        if self._is_testing_mock:\\n            return None\\n\\n        return self.spark.sql(create_schema)\\n\\n    def create(self, sql: str) -> DataFrame | None:\\n        \"\"\"Creates a Materialized Lake View (MLV) in the lakehouse with the given SQL query.\"\"\"\\n        self.create_schema()\\n\\n        create_mlv = f\"CREATE MATERIALIZED LAKE VIEW {self.table_path}\\\\nAS\\\\n{sql}\"\\n        logger.info(f\"CREATE MLV: {self.table_path}\")\\n\\n        if self._is_testing_mock:\\n            return None\\n\\n        return self.spark.sql(create_mlv)\\n\\n    def drop(self) -> str:\\n        \"\"\"Drops the Materialized Lake View (MLV) if it exists.\"\"\"\\n        drop_mlv = f\"DROP MATERIALIZED LAKE VIEW IF EXISTS {self.table_path}\"\\n        logger.info(drop_mlv)\\n\\n        if self._is_testing_mock:\\n            return None\\n\\n        return self.spark.sql(drop_mlv)\\n\\n    def create_or_replace(self, sql: str, mock_is_existing: bool = None) -> DataFrame:\\n        \"\"\"Creates or replaces the Materialized Lake View (MLV) in the lakehouse.\\n\\n        Args:\\n            sql (str): The SQL query to create the MLV.\\n            mock_is_existing (bool, optional): If True, it simulates the existence of the MLV. Defaults to None.\\n\\n        Returns:\\n            DataFrame: The result of the create or replace operation.\\n        \"\"\"\\n        mlv_code_current = self.read_file()\\n        is_existing = (\\n            mock_is_existing\\n            if mock_is_existing is not None\\n            else self.spark.catalog.tableExists(self.table_path)\\n        )\\n\\n        if mlv_code_current is None and not is_existing:\\n            res = self.create(sql)\\n            self.write_file(sql)\\n            return res\\n\\n        elif mlv_code_current is None and is_existing:\\n            logger.warning(\"WARN: file=None, is_existing=True. RECREATE.\")\\n            self.drop()\\n            res = self.create(sql)\\n            self.write_file(sql)\\n            return res\\n\\n        elif sql == mlv_code_current and is_existing:\\n            logger.info(\"Nothing has changed.\")\\n            return None\\n\\n        logger.info(f\"REPLACE MLV: {self.table_path}\")\\n        self.drop()\\n        res = self.create(sql)\\n        self.write_file(sql)\\n        return res\\n\\n    def refresh(self, full_refresh: bool) -> DataFrame:\\n        \"\"\"Refreshes the Materialized Lake View (MLV) in the lakehouse.\"\"\"\\n        full_refresh_str = \"FULL\" if full_refresh else \"\"\\n        refresh_mlv = f\"REFRESH MATERIALIZED LAKE VIEW {self.table_path} {full_refresh_str}\"\\n        logger.info(refresh_mlv)\\n\\n        if self._is_testing_mock:\\n            return None\\n\\n        return self.spark.sql(refresh_mlv)\\n\\n    def to_dict(self) -> None:\\n        \"\"\"Returns a dictionary representation of the Materialized Lake View.\"\"\"\\n        return {\\n            \"lakehouse\": self.lakehouse,\\n            \"schema\": self.schema,\\n            \"table\": self.table,\\n            \"table_path\": self.table_path\\n        }\\n\\n\\nmlv = MaterializedLakeView()', 'transform.silver.scd2': None, 'transform.silver.insertonly': None}}\n"]}],"execution_count":16,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8f5a63c9-8e6b-4d70-b37d-9bedea96514e"},{"cell_type":"code","source":["payload_str = json.dumps(payload)\n","notebookutils.notebook.exit(payload_str)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"a94a6196-327b-4b4d-863a-2bc5c2c39026","normalized_state":"finished","queued_time":"2025-08-08T13:01:57.0827827Z","session_start_time":null,"execution_start_time":"2025-08-08T13:02:11.2519111Z","execution_finish_time":"2025-08-08T13:02:12.7208349Z","parent_msg_id":"7b0d2bb1-ed0c-4d63-bcaf-dec8ac585985"},"text/plain":"StatementMeta(, a94a6196-327b-4b4d-863a-2bc5c2c39026, 11, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["ExitValue: {'modules': {'transform.mlv': 'from typing import Any\\nfrom pyspark.sql import DataFrame, SparkSession\\n\\n\\ndef to_spark_sql(sql: str) -> str:\\n    return sql \\\\\\n            .replace(\"[\", \"`\") \\\\\\n            .replace(\"]\", \"`\")\\n\\n\\nclass MaterializedLakeView:\\n    def __init__(\\n        self,\\n        lakehouse: str = None,\\n        schema: str = None,\\n        table: str = None,\\n        table_suffix: str = \"_mlv\",\\n        spark_: SparkSession = None,\\n        notebookutils_: Any = None,\\n        **kwargs\\n    ) -> None:\\n        self.init(\\n            lakehouse=lakehouse,\\n            schema=schema,\\n            table=table,\\n            table_suffix=table_suffix,\\n            spark_=spark_,\\n            notebookutils_=notebookutils_,\\n            is_testing_mock=kwargs.get(\"is_testing_mock\", False)\\n        )\\n\\n    def init(\\n        self,\\n        lakehouse: str,\\n        schema: str,\\n        table: str,\\n        table_suffix: str = \"_mlv\",\\n        spark_: SparkSession = None,\\n        notebookutils_: Any = None,\\n        **kwargs\\n    ) -> \\'MaterializedLakeView\\':\\n        \"\"\"Initializes the MaterializedLakeView instance.\\n\\n        Args:\\n            lakehouse (str): The lakehouse name.\\n            schema (str): The schema name.\\n            table (str): The table name.\\n            table_suffix (str, optional): The table suffix. Defaults to \"_mlv\".\\n            spark_ (SparkSession, optional): The SparkSession instance. Defaults to None.\\n            notebookutils_ (Any, optional): The NotebookUtils instance. Defaults to None.\\n            is_testing_mock (bool, optional): Whether the instance is a testing mock. Defaults to False.\\n\\n        Returns:\\n            MaterializedLakeView: The initialized MaterializedLakeView instance.\\n        \"\"\"\\n        self._lakehouse = lakehouse\\n        self._schema = schema\\n        self._table = table\\n        self._table_suffix = table_suffix\\n        self._is_testing_mock = kwargs.get(\"is_testing_mock\", False)\\n\\n        # \\'spark\\' and \\'notebookutils\\' are available in Fabric notebook\\n        self._spark = self._get_init_spark(spark_)\\n        self._notebookutils = self._get_init_notebookutils(notebookutils_)\\n        return self\\n\\n    def _get_init_spark(self, spark_: SparkSession) -> SparkSession | None:\\n        \"\"\"Initializes the SparkSession instance.\\n        If a SparkSession is provided, it is returned. Otherwise, it tries to use the global \\'spark\\' variable.\\n\\n        Args:\\n            spark_ (SparkSession): The SparkSession instance.\\n\\n        Returns:\\n            SparkSession | None: The initialized SparkSession instance or None.\\n        \"\"\"\\n        if isinstance(spark_, SparkSession):\\n            return spark_\\n        try:\\n            if spark is not None:  # noqa: F821 # type: ignore\\n                return spark  # noqa: F821 # type: ignore\\n            return spark_\\n        except Exception:\\n            return None\\n\\n    def _get_init_notebookutils(self, notebookutils_: Any) -> Any | None:\\n        \"\"\"Initializes the NotebookUtils instance.\\n        If a NotebookUtils instance is provided, it is returned. Otherwise, it tries to use the global \\'notebookutils\\' variable.\\n\\n        Args:\\n            notebookutils_ (Any): The NotebookUtils instance.\\n\\n        Returns:\\n            Any | None: The initialized NotebookUtils instance or None.\\n        \"\"\"\\n        if notebookutils_ is not None:\\n            return notebookutils_\\n        try:\\n            if notebookutils is not None:  # noqa: F821 # type: ignore\\n                return notebookutils  # noqa: F821 # type: ignore\\n            return None\\n        except Exception:\\n            return None\\n\\n    @property\\n    def lakehouse(self) -> str:\\n        if self._lakehouse is None:\\n            raise ValueError(\"Lakehouse is not initialized.\")\\n        return self._lakehouse\\n\\n    @property\\n    def schema(self) -> str:\\n        if self._schema is None:\\n            raise ValueError(\"Schema is not initialized.\")\\n        return self._schema\\n\\n    @property\\n    def table(self) -> str:\\n        if self._table is None:\\n            raise ValueError(\"Table is not initialized.\")\\n        return self._table\\n\\n    @property\\n    def table_suffix(self) -> str:\\n        return self._table_suffix\\n\\n    @property\\n    def spark(self) -> SparkSession:\\n        if self._spark is None:\\n            raise ValueError(\"SparkSession is not initialized.\")\\n        return self._spark\\n\\n    @property\\n    def notebookutils(self) -> Any:\\n        if self._notebookutils is None:\\n            raise ValueError(\"NotebookUtils is not initialized.\")\\n        return self._notebookutils\\n\\n    @property\\n    def table_name(self) -> str:\\n        table_suffix = self.table_suffix or \"\"\\n        return f\"{self.table}{table_suffix}\"\\n\\n    @property\\n    def file_path(self) -> str:\\n        path = f\"Files/mlv/{self.lakehouse}/{self.schema}/{self.table_name}.sql.txt\"\\n        return path\\n\\n    @property\\n    def table_path(self) -> str:\\n        table_path = f\"{self.lakehouse}.{self.schema}.{self.table_name}\"\\n        return table_path\\n\\n    @property\\n    def schema_path(self) -> str:\\n        schema_path = f\"{self.lakehouse}.{self.schema}\"\\n        return schema_path\\n\\n    def read_file(self) -> str | None:\\n        \"\"\"Reads the content of the SQL file from the specified lakehouse.\\n        If the file does not exist, it returns None.\\n\\n        Raises:\\n            RuntimeError: If the file cannot be read.\\n\\n        Returns:\\n            str | None: The content of the file or None if it doesn\\'t exist.\\n        \"\"\"\\n        path = self.file_path\\n        try:\\n            if not self.notebookutils.fs.exists(path):\\n                return None\\n            if self._is_testing_mock:\\n                with open(path, \"r\") as file:\\n                    return file.read()\\n            df = self.spark.read.text(path, wholetext=True)\\n            mlv_code = df.collect()[0][0]\\n            return mlv_code\\n        except Exception as e:\\n            raise RuntimeError(f\"Fehler beim Lesen der Datei: {e}\")\\n\\n    def write_file(self, sql: str) -> bool:\\n        \"\"\"Writes the SQL content to the specified file in a lakehouse.\\n        If the file already exists, it will be overwritten.\\n        If the file cannot be written, it raises a RuntimeError.\\n\\n        Args:\\n            sql (str): The SQL content to write.\\n\\n        Raises:\\n            RuntimeError: If the file cannot be written.\\n\\n        Returns:\\n            bool: True if the file was written successfully, False otherwise.\\n        \"\"\"\\n        try:\\n            result = self.notebookutils.fs.put(\\n                file=self.file_path,\\n                content=sql,\\n                overwrite=True\\n            )\\n            return result\\n        except Exception as e:\\n            raise RuntimeError(f\"Fehler beim Schreiben der Datei: {e}\")\\n\\n    def create_schema(self) -> DataFrame | None:\\n        \"\"\"Creates the schema in the lakehouse if it does not exist.\"\"\"\\n        create_schema = f\"CREATE SCHEMA IF NOT EXISTS {self.schema_path}\"\\n        logger.info(create_schema)\\n\\n        if self._is_testing_mock:\\n            return None\\n\\n        return self.spark.sql(create_schema)\\n\\n    def create(self, sql: str) -> DataFrame | None:\\n        \"\"\"Creates a Materialized Lake View (MLV) in the lakehouse with the given SQL query.\"\"\"\\n        self.create_schema()\\n\\n        create_mlv = f\"CREATE MATERIALIZED LAKE VIEW {self.table_path}\\\\nAS\\\\n{sql}\"\\n        logger.info(f\"CREATE MLV: {self.table_path}\")\\n\\n        if self._is_testing_mock:\\n            return None\\n\\n        return self.spark.sql(create_mlv)\\n\\n    def drop(self) -> str:\\n        \"\"\"Drops the Materialized Lake View (MLV) if it exists.\"\"\"\\n        drop_mlv = f\"DROP MATERIALIZED LAKE VIEW IF EXISTS {self.table_path}\"\\n        logger.info(drop_mlv)\\n\\n        if self._is_testing_mock:\\n            return None\\n\\n        return self.spark.sql(drop_mlv)\\n\\n    def create_or_replace(self, sql: str, mock_is_existing: bool = None) -> DataFrame:\\n        \"\"\"Creates or replaces the Materialized Lake View (MLV) in the lakehouse.\\n\\n        Args:\\n            sql (str): The SQL query to create the MLV.\\n            mock_is_existing (bool, optional): If True, it simulates the existence of the MLV. Defaults to None.\\n\\n        Returns:\\n            DataFrame: The result of the create or replace operation.\\n        \"\"\"\\n        mlv_code_current = self.read_file()\\n        is_existing = (\\n            mock_is_existing\\n            if mock_is_existing is not None\\n            else self.spark.catalog.tableExists(self.table_path)\\n        )\\n\\n        if mlv_code_current is None and not is_existing:\\n            res = self.create(sql)\\n            self.write_file(sql)\\n            return res\\n\\n        elif mlv_code_current is None and is_existing:\\n            logger.warning(\"WARN: file=None, is_existing=True. RECREATE.\")\\n            self.drop()\\n            res = self.create(sql)\\n            self.write_file(sql)\\n            return res\\n\\n        elif sql == mlv_code_current and is_existing:\\n            logger.info(\"Nothing has changed.\")\\n            return None\\n\\n        logger.info(f\"REPLACE MLV: {self.table_path}\")\\n        self.drop()\\n        res = self.create(sql)\\n        self.write_file(sql)\\n        return res\\n\\n    def refresh(self, full_refresh: bool) -> DataFrame:\\n        \"\"\"Refreshes the Materialized Lake View (MLV) in the lakehouse.\"\"\"\\n        full_refresh_str = \"FULL\" if full_refresh else \"\"\\n        refresh_mlv = f\"REFRESH MATERIALIZED LAKE VIEW {self.table_path} {full_refresh_str}\"\\n        logger.info(refresh_mlv)\\n\\n        if self._is_testing_mock:\\n            return None\\n\\n        return self.spark.sql(refresh_mlv)\\n\\n    def to_dict(self) -> None:\\n        \"\"\"Returns a dictionary representation of the Materialized Lake View.\"\"\"\\n        return {\\n            \"lakehouse\": self.lakehouse,\\n            \"schema\": self.schema,\\n            \"table\": self.table,\\n            \"table_path\": self.table_path\\n        }\\n\\n\\nmlv = MaterializedLakeView()', 'transform.silver.scd2': None, 'transform.silver.insertonly': None}}"]}],"execution_count":9,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f153b863-d7a3-470a-abb3-fe62d9c2906f"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"c7ab73ce-7e20-45c4-9bbf-004d37143759"}],"default_lakehouse":"c7ab73ce-7e20-45c4-9bbf-004d37143759","default_lakehouse_name":"GlobalUtilsLakehouse","default_lakehouse_workspace_id":"b50e79c4-1140-479c-b5c1-225982ea6783"}}},"nbformat":4,"nbformat_minor":5}