output_dir: ./output
do_train: true
do_eval: false
eval_delay: 0
eval_strategy: 'no'



per_device_train_batch_size: 8
per_device_eval_batch_size: 8
gradient_accumulation_steps: 1

learning_rate: 5e-5
weight_decay: 0.0
num_train_epochs: 3.0
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.0
warmup_steps: 0

# 'debug', 'info', 'warning', 'error' and 'critical', plus a 'passive' level which doesn't set anything and lets the application set the level.
log_level: passive
log_on_each_node: true
logging_strategy: steps
logging_first_step: false
logging_steps: 500

save_strategy: steps
save_steps: 500
save_total_limit: null
load_best_model_at_end: false
metric_for_best_model: None
greater_is_better: None
evaluation_strategy: steps





use_liger_kernel: true